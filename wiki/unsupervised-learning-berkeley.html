<!DOCTYPE html>
	<html lang="en">
		<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0">
		<link rel="icon" type="image/svg+xml" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 width=%22256%22 height=%22256%22 viewBox=%220 0 100 100%22><rect width=%22100%22 height=%22100%22 rx=%2220%22 fill=%22%23d8eaeb%22></rect><text x=%2250%%22 y=%2250%%22 dominant-baseline=%22central%22 text-anchor=%22middle%22 font-size=%2293%22>üå≥</text></svg>" />
            <title>Unsupervised Deep Learning -- Berkeley course</title>
			<link rel="canonical" href="https://strikingloo.github.io/wiki/unsupervised-learning-berkeley">
			  <meta name="description" content="My notes from Berkeley's Unsupervised Deep Learning course, plus any papers from the recommended reading I went through -may be linked-.">
  			<meta property="og:site_name" content="Unsupervised Deep Learning -- Berkeley course">

        	
        	<link rel="stylesheet" type="text/css" href="/css/post-min.css">
        	

			<script async src="https://www.googletagmanager.com/gtag/js?id=UA-52642322-5"></script>

			<script>
			  window.dataLayer = window.dataLayer || [];
			  function gtag(){dataLayer.push(arguments);}
			  gtag('js', new Date());

			  gtag('config', 'UA-52642322-5');
			</script>
			
			  <meta property="og:description" content="My notes from Berkeley's Unsupervised Deep Learning course, plus any papers from the recommended reading I went through -may be linked-.">
			
	  		<meta property="og:locale" content="en_US">
	  		
			  <meta property="og:title" content="Unsupervised Deep Learning -- Berkeley course">
			  <meta property="og:type" content="article">
		  	  <meta property="article:published_time" content="2021-10-14T00:00:00+00:00">
		      <meta property="article:author" content="/">
		  	
		  	
  				<meta property="og:url" content="https://strikingloo.github.io/wiki/unsupervised-learning-berkeley">
  			
  			<meta content="index,follow" name="robots"><!-- All Search Engines -->
  			<meta content="index,follow" name="googlebot"><!-- Google Specific -->
  			<meta name="robots" content="max-image-preview:large">

		</head>
		<body>
			<div class="head-banner">
			<p>Strikingloo</p>
			<nav>
	    		<ul>
	        		<li><a href="/">Home</a></li>
		        	<li><a href="/about/">About</a></li>
	        		<li><a href="/wiki/">Wiki</a></li>
	        		<li><a href="/blog/">Blog </a></li>
	    		</ul>
			</nav>
			<div style="clear: both;"></div>
		    </div>

			<div class="container">
			
			<h1>Unsupervised Deep Learning -- Berkeley course</h1>
<p class="meta">14 Oct 2021 - importance: 10 </p>




<div class="post">
  <h2 id="autoregressive-models">Autoregressive models</h2>

<p>Given the first n elements of a sequence, predict the n+1st one. E.g., given the first n tokens, produce the next one in a sentence.</p>

<p>On a general, generic case you may approach this problem with an autoregressive RNN. This can for instance generate plausible samples for MNIST, and improves drastically if you also provide it with metadata on pixel position (so you pass it the RNN state, and the X and Y coordinates).</p>

<p><strong>PixelCNN</strong>: User a 2d-filter over pixels above, and a different 1d one over pixels to the left, condition on both and do softmax.</p>

<p><strong>PixelCNN++</strong>: Uses a whole 6 layers of resnet with diamond-like residual connections, and instead of doing softmax over 256 values has as an output a mixture of sigmoids (typically 5) where only pi_i, mu_i and s_i are fit by the model, and then output is taken as mean of a gaussian, sampled and finally discretized to 1 of 256 values. Dropout used in first residual connection.</p>

<p><strong>WaveNet</strong>: We actually read the paper, but dillated causal convolutions + tanh*sigmoid activation+ residual connections. It‚Äôs interesting that it generalizes to images, but it <em>needs</em> the pixel coordinates as metadata to work.</p>

<h2 id="flow-models">Flow models</h2>
<p>In one dimension, map arbitrary distributions to some better-known ones using a learned function, that maximizes log likelihood and is invertible. E.g., I take an arbitrary distribution, take its CDF and map it into a Gaussian‚Äôs CDF.</p>

<h3 id="autoregressive-flow-vs-inverse-autoregressive-flow">autoregressive flow vs inverse autoregressive flow</h3>
<p>Autoregressive flow: train fast, sample slow (linearly on pixel quantity).
Inverse: train slow (depends on sampled pixels), but sample very fast (just sample N variables from your prior and then run the f(z‚Ä¶) autoregressively.</p>

<p><img src="unsupervised-learning-images/flow-1d-1.png" alt="" loading="lazy" />
<img src="unsupervised-learning-images/flow-1d-2.png" alt="" loading="lazy" />
<img src="unsupervised-learning-images/flow-1d-3.png" alt="" loading="lazy" />
<img src="unsupervised-learning-images/flow-1d-4.png" alt="" loading="lazy" />
<img src="unsupervised-learning-images/flow-1d-5.png" alt="" loading="lazy" />
<img src="unsupervised-learning-images/flow-1d-5.png" alt="" loading="lazy" />
<img src="unsupervised-learning-images/flow-nd.png" alt="" loading="lazy" />
<img src="unsupervised-learning-images/flow-nd-0.png" alt="" loading="lazy" />
<img src="unsupervised-learning-images/flow-nd-1.png" alt="" loading="lazy" /></p>

<h3 id="uniform-dequantization">(Uniform) Dequantization</h3>
<p>We add a random number uniformly sampled from -.5 to .5 to our discrete data every time, so that flow doesn‚Äôt learn to use discrete data and overfit by asigning too much mass to too few points.</p>

<h3 id="realnvp--nice">RealNVP / NICE</h3>
<p>Map from x -&gt; Z such that X0:n/2 stay the same in z, and Xn/2:n = Xn/2:n * s(x0:d/2) + t(x0:d/2).</p>

<p>This means half of x are left the same, the other half are transformed with an affine transformation where linear weights are a function of the first half.
Typically you‚Äôd start by alternating the pixels (checkerboard pattern) but vertical or horizontal partitioning of the image in half has also been tried. This usually gets worse results. Functions s and t are arbitrary as long as they fit, you don‚Äôt need them to be reversible as you already know x0‚Ä¶n/2.</p>

<p><img src="unsupervised-learning-images/real-nvp.png" alt="" loading="lazy" /></p>

<p>Other models that work but are not covered in detail:</p>
<ul>
  <li><a href="https://openai.com/blog/glow/">Glow</a> <a href="/wiki/flow-based-models-glow">my notesüåø</a></li>
  <li><a href="https://arxiv.org/abs/1810.01367">FFJORD</a> üå±</li>
</ul>

<h2 id="latent-variable-models">Latent Variable Models</h2>

<p>In a latent variable model, you have a distribution of data X, and you assume it has an underlying cause distribution Z such that you can estimate P(X|Z) and Z is easy to sample from.</p>

<p>In a way this is similar to flow models, but here we make no strong constraints on the relationship between X and Z, we may not know the intermediate values for Z and the mapping needs not be invertible.</p>

<p><img src="unsupervised-learning-images/exact-likelihood-objective.png" alt="" loading="lazy" /></p>

<h3 id="variational-autoencoders">Variational AutoEncoders</h3>
<p><strong>Principle of Variational Approach</strong>: We can‚Äôt directly use the <em>p</em> we want, so instead we propose a parameterized distribution <em>q</em> we know we can work with (sample from, estimate likelihood) easily, and try to find a parameter setting that makes it as good as possible.</p>

<p>This works because we‚Äôre doing importance sampling: we could be sampling from our random latent space and doing maximum likelihood of X given Z (without fitting Z, we only fit P(X|Z)), but then for Z st P(X|Z) is ~0, we‚Äôre doing the work pointlessly (because the gradient is 0 and there‚Äôs no information to propagate). Whereas if we make sure to only sample from points where X given Z is likely, we get more bang for our buck. However if we already had P(Z|X) to calculate this we wouldn‚Äôt be going through this in the first place (because Bayes) and this is usually intractable (if Z is gaussian or generally not enumerable), so instead we try to get this to be ‚Äúas good as possible‚Äù and that‚Äôs that.</p>

<p><img src="unsupervised-learning-images/variational-equation.png" alt="" loading="lazy" /></p>

<p><img src="unsupervised-learning-images/VLB-derivation.png" alt="" loading="lazy" />
<img src="unsupervised-learning-images/VLB-consequences.png" alt="" loading="lazy" /></p>

<p><img src="unsupervised-learning-images/vae-1-loss.png" alt="" loading="lazy" />
<img src="unsupervised-learning-images/vae-2-loss.png" alt="" loading="lazy" /></p>

<h2 id="gans">GANs</h2>
<p>The course introduces GANs, following Goodfellow et al. closely, so I won‚Äôt reproduce what they say as it overlaps with <a href="/wiki/GAN">GANs</a>.</p>

<p>It then discusses the difficulties in measuring GAN performance, as human judgement over samples is not enough (though it is the original propeller of GANs in a way).</p>

<h3 id="inception-score">Inception Score</h3>
<p>We pick a pretrained classifier on imagenet 1k classes and:</p>
<ul>
  <li>Entropy of labels should be high (the generator should generate a diverse set of classes)</li>
  <li>But entropy of certainty of classifier should be low: the classifier must be quite certain which class each image belongs to.</li>
</ul>

<p>The Inception score ends up being the DKL between p(y|x) and p(y), where p(y) is the marginal probability over a sample set for each class, and p(y|x) is the probability for that label for that specific sample. The p(y) marginal should be a uniform distribution, whereas the p(y|x) should peak around a class. You want KLD to be <em>high</em>.</p>

<p>Equivalent to entropy of labels for generated samples - entropy of labels for each sample.</p>

<p>FID is a more complicated metric based on inception score that uses an embedding space from some classifier, and compares the embedding for generated image vs average embedding. <a href="/wiki/fid"><em>notes on FID üåø</em></a></p>

<h3 id="discriminator-saturation">Discriminator Saturation</h3>
<p>The bayes ideal discriminator is always right (assigns prob 0 to fake and prob 1 to true). However if our discriminator is too good too fast, since the gradient tends to be very close to zero for high confidence predictions (the landscape is flat around the edges, think of a sigmoid), the generator gets little information to work with.</p>

<p><strong>Ways to address Discriminator Saturation</strong>: Use a different, non-zero-sum objective where you switch the log(1 - D(G(z))) with a -log(D(G(z))). This changes the regime of the game so it stops being zero sum, and also makes it so that the gradient goes to 0 when the generator is already winning, which is far less terrible. Otherwise a generator may get stuck on a bad initialization. The other way to prevent this is alternating updates between generator and discriminator.</p>

<p><img src="unsupervised-learning-images/dcgan-archi.png" alt="" loading="lazy" /></p>

<h3 id="other-techniques">Other techniques</h3>

<p><strong>Feature Matching</strong>: Add to the objective that the mean batch value for the vector of <strong>hidden features</strong> for generated images shouldn‚Äôt stray too far from mean vector of data features.</p>

<p><strong>DC-Gan</strong>: Improved performance by using deconv layers and a few other tricks.</p>

<p><strong>Improvements over DCGAN</strong>:</p>
<ul>
  <li>Feature Matching: add a loss that looks at 2-norm divergence between means of hidden reps of generated and true data.</li>
  <li>Minibatch discrimination: I didn‚Äôt quite understand this one [üå±]</li>
  <li>Historical Averaging: add a penalty to distance between theta and mean historical theta</li>
  <li>Virtual batch normalization: instead of calculating mu and sigma for each minibatch, keep a big one and use their weights and keep adding the new batches to it.</li>
  <li>One-sided label smoothing: change label 1 to .9 (less important if you use the good objective)</li>
</ul>

<p><strong>WGAN</strong>: Wassertein GAN: keep the gradient in a magic (Lipschitz-1) space using weight clipping: for each weight, clip it between c and -c. c too big changes nothing and makes convergence hard. c too close to 0 may make you lose too much information. Made models significantly more robust, allowing for instance to train a DCGAN without batch-norm with similar results.</p>

<p><strong>WGAN-GP</strong>: Same idea, achieves the same without weight clipping (which is subpar in many ways and gets ugly results). Just add a term to loss that punishes the gradient of the discriminator from diverging from norm 1. Keeps the gradient in a more docile regime and generally gets better and more robust results. The only code change is adding this term to the loss. Interestingly, the gradient is computed for an interpolation (convex sum) between generated and real samples.</p>

<p><strong>SNGAN</strong>: You normalize each layer of the discriminator by the <a href="/wiki/metnum-final">spectral norm</a> (value of biggest eigenvalue) of its weight matrix W. You do this to guarantee lipschitz-ness of the network, restricting the optimization problem to min G: max ||f||Lip &lt;= K V(G,D).</p>

<p><strong>SAGAN</strong>: Adds self attention to SNGAN. Applies spectral norm in both discriminator and generator. (Instead of just discriminator).</p>

<p><img src="unsupervised-learning-images/sagan-self-attention.png" alt="" loading="lazy" /></p>

<p><strong>BigGAN</strong>: Just makes everything bigger (wider and deeper), Orthonormal regularization -make each W close to I- &amp; Truncation Trick -at test time only, sample from truncated normal instead of standard one-.</p>

<p><strong>StyleGAN</strong>: Very big GAN with a MLP that turns gaussian latent into a different vector, and feeds it into every layer. Also adds noise to inputs 100 times (100 different noise vectors) and uses <em>that</em> to generate the statistics for batch norm, which it does individually for each sample and for each channel (averaging over all WxH). This is called AdaIN.</p>

<p><img src="unsupervised-learning-images/stylegan-architecture.png" alt="" loading="lazy" /></p>

<blockquote>
  <p><strong>Bottom line</strong>: Use BigGan for conditioned image generation, styleGAN for unconditioned or if you have an interest in interpolation. They require at least 16GB of VRAM anyway.</p>
</blockquote>

<p><strong>Pix2Pix</strong>: Train a model G(x, z) where z is your (random) latent and x is an actual black and white image, such that a discriminator that takes D(x, G(x, z)) and D(x, y) can‚Äôt tell which is which in an adversarial setting (where y = actual-colored-of(x) ). I think this approach is beautiful. It kind of reminds me of <a href="/wiki/clip">contrastive learning</a></p>

<p><img src="unsupervised-learning-images/pix2pix-color.png" alt="" loading="lazy" /></p>

<p>In the objective, besides the shared loss function they add a term for L1 reconstruction loss (L1-norm of difference between G(x,z) and y) -between the generated translation of the source image and the expected target image-.</p>

<ul>
  <li>Pix2Pix can be extended to video but I‚Äôm not 100% sure how (see 2018 neurips video synthesis from semantic drawings). [üå±]</li>
  <li>There‚Äôs also GauGAN by Nvidia which makes photorealistic illustrations from segmentation semantic drawings. It looks pretty awesome so I‚Äôd like to read on that. [üå±]</li>
</ul>

<p><strong>InfoGAN</strong>: Instead of class, you provide a vector of ‚Äúlatent codes‚Äù such that each code captures a few aspects of your image. You train the discriminator with two losses: the classic discriminator loss (real or fake?) and also a new term where the discriminator tries to maximize I(c ; x) the mutual information (by maximizing its estimation of the posterior P(c|x)). So basically it‚Äôs trying to predit what C generated a given x. e.g., for MNIST you could have c1 = digit, c2 = rotation etc.</p>

<p>Interestingly, the code can be discrete and fed in a random way (e.g., uniformCateg(1024) for a 1000 classes dataset) and eventually even without a specific loss for this, the models converge in making different classes for different values of the categ code (obtained in a completely unsupervised way). This is only using the loss of a regular BigGAN.</p>

<p><strong>BigBiGAN</strong>: Comes after BigGAN. You train an encoder that generates z‚Äô from image x, and your typical generator G(z)=x‚Äô. You then train the discriminator to receive both z‚Äô and x‚Äô or z and x, and decide whether x/‚Äô is real or fake. 
You could then take any image and map it into the representational space z‚Äô. This reaches state of the art in linear probe for top 1 accuracy in some datasets at the time.
Also you can do a weird autoencoder (which was never in the objective!) x‚Äô = G(E(x)), where x‚Äô is the optimal confounder for a given discriminator. Pretty neat! (and pure nightmare fuel for human faces apparently).</p>

<p><img src="unsupervised-learning-images/bigbigan.png" alt="" loading="lazy" /></p>

<p><strong>IMLE (Implicit Maximum Likelihood Estimation)</strong>: initialize theta on random. for k = 1 to K: sample X1‚Äô‚Ä¶Xn‚Äô from P‚Äô(x), pick a random batch S of X (real), L times repeat: update theta with gradient where cost is distance between a random sample of size n from the batch S, and the generated samples, but you compare generated vector to the one closest to it in the real batch (for some definition of distance).
This evenually converges preventing mode collapse.</p>

<h2 id="self-supervised-learning">Self-Supervised Learning</h2>

<p>We wish to learn rich and useful features from raw unlabeled data that can be useful for several downstream tasks. For this we use pretext tasks that require no labeling on a big dataset, improving data efficiency for downstream tasks.</p>

<h3 id="pretext-tasks">Pretext Tasks</h3>
<p>Used to learn feature representations.</p>
<ul>
  <li>Autoencoders (typically denoising)</li>
  <li>Relative Position of Image Patches</li>
  <li>Predicting one view from another (e.g., color from B&amp;W)</li>
  <li>Predict rotation (rotating just 4 angles works best)</li>
  <li>Word2Vec and contrastive learning are also Self-supervised learning</li>
</ul>

<p>CPC (Contrastive predicting coding) can also be used to predict next audio chunk after X audio (using a RNN as encoder) or next patch of an image. They just use softmax over k_i embedding * W * c where c is code for X.</p>

<p>MoCo and SimCLR do the same, but they keep a stash of all previous instances in memory using a weighted average of the weights over time to predict (so noisy output of model for contrastive loss). This is useful for batchnorm and biggest ‚Äúno‚Äù for contrastive step.</p>

<p><strong>CycleGAN</strong>: Image to Image GAN that uses a triple loss for its generator: given two domains X and Y, you have your typical adversarial loss (how well the discriminator is fooled by genY(x) ) + cycle loss (how well GenX(GenY(x)) matches x) + identity loss (error reconstruction of GenX(x) vs x) plus their analogs with y as input. The example in the paper turns pictures of zebras into pictures of horses with moderate success, and it can also be used for summer to winter or night to day.</p>

<p>Excellent <a href="https://keras.io/examples/generative/cyclegan/">Keras Documentation on CycleGAN with code üå±</a>.</p>

<h2 id="semi-supervised-learning">Semi-Supervised Learning</h2>

<p>Train a supervised model on labeled data and then leverage unlabeled (typically much more numerous) data to improve it.</p>

<p>Some techniques include:</p>
<ul>
  <li>Label Consistency: minimizing entropy for unlabeled data</li>
  <li>unsupervised data augmentation</li>
  <li>Pseudo Labeling: adding the most confident predictions as 100% labeled</li>
  <li>VAT Virtual Adversarial Training: where you take derivative of prediction wrt <em>the input</em> and move the input in the opposite direction keeping the label</li>
</ul>

<p><strong>MixMatch</strong>: Take unlabeled data, augment it many times, average predictions and sharpen them with temperature, then make that the label for all augmentations. This term in loss is weighted with a hyperparam. This algorithm <em>almost</em> matches supervised performance in CIFAR10 and others.</p>

<p><strong>Noisy Student</strong> : Train a supervised model on your labeled data, that‚Äôs your teacher. Create a student, which trains on labeled data but also in unlabeled such that the original teacher has high confidence in the label. Add noise (stochastic skip connections, dropout, data augmentation) to the inputs that go into the noisy student (hence the name). Then the student becomes the teacher and you keep repeating.</p>

<p>This uses the logits instead of the one-hot encoded predictions for the teacher classifications. It got a 10x data efficiency compared with sota (10x smaller unlabelled dataset) combining ImageNet and JFT. Sota in imageNet. Very robust in Imagenet-A, an adversarial dataset [üå±]. Scales well to little labeled data.</p>

<hr />
<p>A small aside: a matrix W is orthogonalized by solving the following constrained quadratic problem:</p>

<p>\( \min_{\bar{W}} \lVert W - \bar{W} \rVert s.t. \bar{W}^T \bar{W} = I \)</p>

<p>One can show that this problem can be solved by taking the singular value decomposition (SVD) of W and replacing the singular values to ones.</p>

<h3 id="notes-from-normalized-word-embedding-and-orthogonal-transform-for-bilingual-word-translation">Notes from ‚ÄúNormalized Word Embedding and Orthogonal Transform for Bilingual Word Translation‚Äù</h3>

<p><a href="https://aclanthology.org/N15-1104.pdf">Paper</a></p>

<p>The authors try training an orthogonal matrix W to align (i.e., minimize cosine distance of normalized vectors) two word embedding spaces (EN-ES). They sample 6k frequent words, translate with google, and find W orthogonal that minimizes cosine distance (after normalizing embedding vectors to norm 1).</p>

<p>They compare against Mikolov who didn‚Äôt normalize and used arbitrary linear transform, and conclude that ‚Äúbilingual translation can be largely improved by the normalized embedding and the accompanied orthogonal transform‚Äù. They reach about 40% P@1, 60% P@5.</p>

</div>
<a href='https://ko-fi.com/R6R3F4NIO' target='_blank' rel="noopener noreferrer nofollow">
  <img style='border:0px;height:4em;width:auto;' src='https://cdn.ko-fi.com/cdn/kofi5.png?v=3' border='0' alt='Buy Me a Coffee at ko-fi.com' loading='lazy'/></a>
  <p style='text-align: center;'>
  <a href="https://twitter.com/intent/tweet?text=Unsupervised Deep Learning -- Berkeley course&url=https://strikingloo.github.io/wiki/unsupervised-learning-berkeley%2F%3Futm_source%3Dtwitter%26utm_medium%3Dsocial&via=strikingLoo" title="Share on Twitter!">[<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 310 310" height="1em" width="1em" rel="noopener noreferrer nofollow"><path fill="#1DA1F2" d="M302.973 57.388a117.512 117.512 0 01-14.993 5.463 66.276 66.276 0 0013.494-23.73 5 5 0 00-7.313-5.824 117.994 117.994 0 01-34.878 13.783c-12.381-12.098-29.197-18.983-46.581-18.983-36.695 0-66.549 29.853-66.549 66.547 0 2.89.183 5.764.545 8.598C101.163 99.244 58.83 76.863 29.76 41.204a5.001 5.001 0 00-8.196.642c-5.896 10.117-9.013 21.688-9.013 33.461 0 16.035 5.725 31.249 15.838 43.137a56.37 56.37 0 01-8.907-3.977 5 5 0 00-7.427 4.257c-.007.295-.007.59-.007.889 0 23.935 12.882 45.484 32.577 57.229a57.372 57.372 0 01-5.063-.735 4.998 4.998 0 00-5.699 6.437c7.29 22.76 26.059 39.501 48.749 44.605-18.819 11.787-40.34 17.961-62.932 17.961a120.4 120.4 0 01-14.095-.826 5 5 0 00-3.286 9.174c29.023 18.609 62.582 28.445 97.047 28.445 67.754 0 110.139-31.95 133.764-58.753 29.46-33.421 46.356-77.658 46.356-121.367 0-1.826-.028-3.67-.084-5.508 11.623-8.757 21.63-19.355 29.773-31.536a5 5 0 00-6.182-7.351z"></path></svg>Share on twitter]</a></p>

<template id="post-delayed-content">

<div class="backButton">
<a href="https://twitter.com/intent/tweet?text=Unsupervised Deep Learning -- Berkeley course&url=https://strikingloo.github.io/wiki/unsupervised-learning-berkeley%2F%3Futm_source%3Dtwitter%26utm_medium%3Dsocial&via=strikingLoo" id='tweetThis' title="Share on Twitter!" rel="noopener noreferrer nofollow">[<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 310 310" height="1em" width="1em"><path fill="#1DA1F2" d="M302.973 57.388a117.512 117.512 0 01-14.993 5.463 66.276 66.276 0 0013.494-23.73 5 5 0 00-7.313-5.824 117.994 117.994 0 01-34.878 13.783c-12.381-12.098-29.197-18.983-46.581-18.983-36.695 0-66.549 29.853-66.549 66.547 0 2.89.183 5.764.545 8.598C101.163 99.244 58.83 76.863 29.76 41.204a5.001 5.001 0 00-8.196.642c-5.896 10.117-9.013 21.688-9.013 33.461 0 16.035 5.725 31.249 15.838 43.137a56.37 56.37 0 01-8.907-3.977 5 5 0 00-7.427 4.257c-.007.295-.007.59-.007.889 0 23.935 12.882 45.484 32.577 57.229a57.372 57.372 0 01-5.063-.735 4.998 4.998 0 00-5.699 6.437c7.29 22.76 26.059 39.501 48.749 44.605-18.819 11.787-40.34 17.961-62.932 17.961a120.4 120.4 0 01-14.095-.826 5 5 0 00-3.286 9.174c29.023 18.609 62.582 28.445 97.047 28.445 67.754 0 110.139-31.95 133.764-58.753 29.46-33.421 46.356-77.658 46.356-121.367 0-1.826-.028-3.67-.084-5.508 11.623-8.757 21.63-19.355 29.773-31.536a5 5 0 00-6.182-7.351z"></path></svg>]</a>
<br/>
<a href="/blog/" id='backToBlog' title="Back to blog" rel="noopener noreferrer">[‚Üê]</a>
</div>
</template>
<script>
const headings = document.querySelectorAll('h2[id],h3[id]');
for (var heading of headings) {
    heading.innerHTML = `<a href=#${heading.id}>${heading.innerHTML}</a>`;
}
function externalLinks() { for(var c = document.getElementsByTagName("a"), a = 0;a < c.length;a++) { var b = c[a]; b.getAttribute("href") && b.hostname !== location.hostname && (b.target = "_blank") } }
externalLinks();

function renderBottomButtons(){
  const templateNode = document.getElementById('post-delayed-content')
  const templateContentClone = templateNode.content.cloneNode(true) // perform a deep copy
  document.body.appendChild(templateContentClone)
}

function scrollEventHandler(){
 const scrollOffset = window.pageYOffset
 const browserViewHeight = window.innerHeight
 if (scrollOffset > browserViewHeight/3) {
    console.log('done')
    renderBottomButtons();
    window.removeEventListener('scroll', scrollEventHandler)
 }
}
window.addEventListener('scroll', scrollEventHandler)
</script>

<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:site" content="@StrikingLoo" />
<meta name="twitter:title" content="Unsupervised Deep Learning -- Berkeley course" />
<meta name="twitter:description" content="My notes from Berkeley's Unsupervised Deep Learning course, plus any papers from the recommended reading I went through -may be linked-." />

<meta name="twitter:image:src" content="https://strikingloo.github.io/resources/book-tw.jpg"/>
<meta property="og:image" content="https://strikingloo.github.io/resources/preview-image-terrarium.png"/>

			
			</div>
			<footer>
	    		<ul>
	        		<li><a href="mailto:lucianostrika44@gmail.com" rel="me" title="email me">‚úâÔ∏è</a></li>
	        		<li><a href="https://github.com/strikingloo" rel="me noopener noreferrer nofollow" title="GitHub"><svg viewBox="0 0 438.549 438.549" xmlns="http://www.w3.org/2000/svg" height="1em" width="1em"><path fill="#0F3D3E" d="M409.132 114.573c-19.608-33.596-46.205-60.194-79.798-79.8-33.598-19.607-70.277-29.408-110.063-29.408-39.781 0-76.472 9.804-110.063 29.408-33.596 19.605-60.192 46.204-79.8 79.8C9.803 148.168 0 184.854 0 224.63c0 47.78 13.94 90.745 41.827 128.906 27.884 38.164 63.906 64.572 108.063 79.227 5.14.954 8.945.283 11.419-1.996 2.475-2.282 3.711-5.14 3.711-8.562 0-.571-.049-5.708-.144-15.417a2549.81 2549.81 0 01-.144-25.406l-6.567 1.136c-4.187.767-9.469 1.092-15.846 1-6.374-.089-12.991-.757-19.842-1.999-6.854-1.231-13.229-4.086-19.13-8.559-5.898-4.473-10.085-10.328-12.56-17.556l-2.855-6.57c-1.903-4.374-4.899-9.233-8.992-14.559-4.093-5.331-8.232-8.945-12.419-10.848l-1.999-1.431c-1.332-.951-2.568-2.098-3.711-3.429-1.142-1.331-1.997-2.663-2.568-3.997-.572-1.335-.098-2.43 1.427-3.289s4.281-1.276 8.28-1.276l5.708.853c3.807.763 8.516 3.042 14.133 6.851 5.614 3.806 10.229 8.754 13.846 14.842 4.38 7.806 9.657 13.754 15.846 17.847 6.184 4.093 12.419 6.136 18.699 6.136s11.704-.476 16.274-1.423c4.565-.952 8.848-2.383 12.847-4.285 1.713-12.758 6.377-22.559 13.988-29.41-10.848-1.14-20.601-2.857-29.264-5.14-8.658-2.286-17.605-5.996-26.835-11.14-9.235-5.137-16.896-11.516-22.985-19.126-6.09-7.614-11.088-17.61-14.987-29.979-3.901-12.374-5.852-26.648-5.852-42.826 0-23.035 7.52-42.637 22.557-58.817-7.044-17.318-6.379-36.732 1.997-58.24 5.52-1.715 13.706-.428 24.554 3.853 10.85 4.283 18.794 7.952 23.84 10.994 5.046 3.041 9.089 5.618 12.135 7.708 17.705-4.947 35.976-7.421 54.818-7.421s37.117 2.474 54.823 7.421l10.849-6.849c7.419-4.57 16.18-8.758 26.262-12.565 10.088-3.805 17.802-4.853 23.134-3.138 8.562 21.509 9.325 40.922 2.279 58.24 15.036 16.18 22.559 35.787 22.559 58.817 0 16.178-1.958 30.497-5.853 42.966-3.9 12.471-8.941 22.457-15.125 29.979-6.191 7.521-13.901 13.85-23.131 18.986-9.232 5.14-18.182 8.85-26.84 11.136-8.662 2.286-18.415 4.004-29.263 5.146 9.894 8.562 14.842 22.077 14.842 40.539v60.237c0 3.422 1.19 6.279 3.572 8.562 2.379 2.279 6.136 2.95 11.276 1.995 44.163-14.653 80.185-41.062 108.068-79.226 27.88-38.161 41.825-81.126 41.825-128.906-.01-39.771-9.818-76.454-29.414-110.049z"></path></svg></a></li>
			        <li><a href="https://twitter.com/intent/follow?screen_name=strikingloo" rel="me noopener noreferrer nofollow" title="twitter"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 310 310" height="1em" width="1em"><path fill="#0F3D3E" d="M302.973 57.388a117.512 117.512 0 01-14.993 5.463 66.276 66.276 0 0013.494-23.73 5 5 0 00-7.313-5.824 117.994 117.994 0 01-34.878 13.783c-12.381-12.098-29.197-18.983-46.581-18.983-36.695 0-66.549 29.853-66.549 66.547 0 2.89.183 5.764.545 8.598C101.163 99.244 58.83 76.863 29.76 41.204a5.001 5.001 0 00-8.196.642c-5.896 10.117-9.013 21.688-9.013 33.461 0 16.035 5.725 31.249 15.838 43.137a56.37 56.37 0 01-8.907-3.977 5 5 0 00-7.427 4.257c-.007.295-.007.59-.007.889 0 23.935 12.882 45.484 32.577 57.229a57.372 57.372 0 01-5.063-.735 4.998 4.998 0 00-5.699 6.437c7.29 22.76 26.059 39.501 48.749 44.605-18.819 11.787-40.34 17.961-62.932 17.961a120.4 120.4 0 01-14.095-.826 5 5 0 00-3.286 9.174c29.023 18.609 62.582 28.445 97.047 28.445 67.754 0 110.139-31.95 133.764-58.753 29.46-33.421 46.356-77.658 46.356-121.367 0-1.826-.028-3.67-.084-5.508 11.623-8.757 21.63-19.355 29.773-31.536a5 5 0 00-6.182-7.351z"></path></svg></a></li>
			        <li><a href="http://www.linkedin.com/in/luciano-strika" rel="me noopener noreferrer nofollow" title="linkedin"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 310 310" height="1em" width="1em"><path fill="#0F3D3E" d="M72.16 99.73H9.927a5 5 0 00-5 5v199.928a5 5 0 005 5H72.16a5 5 0 005-5V104.73a5 5 0 00-5-5zM41.066.341C18.422.341 0 18.743 0 41.362 0 63.991 18.422 82.4 41.066 82.4c22.626 0 41.033-18.41 41.033-41.038C82.1 18.743 63.692.341 41.066.341zM230.454 94.761c-24.995 0-43.472 10.745-54.679 22.954V104.73a5 5 0 00-5-5h-59.599a5 5 0 00-5 5v199.928a5 5 0 005 5h62.097a5 5 0 005-5V205.74c0-33.333 9.054-46.319 32.29-46.319 25.306 0 27.317 20.818 27.317 48.034v97.204a5 5 0 005 5H305a5 5 0 005-5V194.995c0-49.565-9.451-100.234-79.546-100.234z"></path></svg></a></li>
			        <li><a href="/resources/Luciano_Strika.pdf">CV</a></li>
				</ul>
				<p><i>Built with ‚ù§Ô∏è by <a href="https://strikingloo.github.io/">Strikingloo</a>.</i></p>
			</footer>
			

			
			<script async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
			

			
			<link rel="preload" href="/css/non-critical-post-min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
			<noscript><link rel="stylesheet" href="/css/non-critical-post-min.css"></noscript>
        	
		</body>
	</html>
