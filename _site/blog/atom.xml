<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

	<title>StrikingLoo's Blog</title>
	<link href="http://strikingloo.github.io/blog/atom.xml" rel="self"/>
	<link href="http://strikingloo.github.io/blog"/>
	<updated>2022-10-07T23:51:48-03:00</updated>
	<id>http://strikingloo.github.io/blog</id>
	<author>
		<name>Luciano Strika</name>
		<email>lucianostrika44@gmail.com</email>
	</author>

	
		<entry>
			<title>Reinforcement Learning for Beginners: Coding a Maze-solving Agent from Scratch</title>
			<link href="http://strikingloo.github.io/reinforcement-learning-beginners"/>
			<updated>2022-09-21T00:00:00-03:00</updated>
			<id>http://strikingloo.github.io/reinforcement-learning-beginners</id>
			<content type="html">&lt;p&gt;What could we do if, instead of coding programs from the ground up, we could just specify the rules for a task, the success criteria, and make AI learn to complete it?&lt;/p&gt;

&lt;p&gt;Imagine the blessings humanity could unlock if we could automate all the tasks nobody wants to do, all the unsafe, unhealthy or uninspiring jobs. Or the ways Scientific progress could be sped up if big parts of the research process were accelerated.&lt;/p&gt;

&lt;p&gt;That powerful question motivates Reinforcement Learning. Instead of programs that classify data or attempt to solve narrow tasks (like next-token prediction), Reinforcement Learning is concerned with creating &lt;strong&gt;agents&lt;/strong&gt;, autonomous programs that run in an &lt;strong&gt;environment&lt;/strong&gt; and execute tasks.&lt;/p&gt;

&lt;p&gt;The algorithms that train a Reinforcement Learning agent are very hands-off compared to other branches of Machine Learning: just provide the agent with features describing the environment (like the graphics for a game, or other users’ recent posts for social media content), and give it rewards according to its actions.&lt;/p&gt;

&lt;p&gt;Following the algorithms, the agent will work to maximize total reward over time, like a dog that’s given treats until it learns to do a cartwheel.&lt;/p&gt;

&lt;p&gt;This may not always work. At each time step (however we define our time steps), the agent will have to pick one of all possible actions, which can potentially form a huge universe of possibilities. There could also be delays between the time the action is chosen and when we learn if it was a good choice, or the environment could be hard or expensive to simulate –imagine simulating trades in the stock market, or Robotics tasks that deal with expensive equipment.&lt;/p&gt;

&lt;p&gt;Moreover, Reinforcement Learning algorithms are data-hungry: it took &lt;a href=&quot;https://www.quora.com/How-many-games-did-Alpha-Zero-played-against-itself-during-its-four-hours-training&quot;&gt;years of playing time&lt;/a&gt; for Alpha Zero to learn to play go at human level, but when it did it surpassed our greatest players. In terms of data efficiency, RL ranks lowest among the Machine Learning family.&lt;/p&gt;

&lt;p&gt;However, just the fact that it is already possible to train an AI to beat all humans in most intellectual games, begs the question of what the next generation of Reinforcement Learning models will be able to do once they incorporate Large Language Models and Vision Models for feature extraction.&lt;/p&gt;

&lt;p&gt;For my speculations on that and a few interesting recent results, keep posted for the conclusions.&lt;/p&gt;

&lt;h2 id=&quot;how-reinforcement-learning-works&quot;&gt;How Reinforcement Learning Works&lt;/h2&gt;

&lt;p&gt;The current paradigm of Reinforcement Learning looks like this.&lt;/p&gt;

&lt;p&gt;We have an &lt;strong&gt;agent&lt;/strong&gt;, which takes &lt;strong&gt;actions&lt;/strong&gt; in an &lt;strong&gt;environment&lt;/strong&gt; it does not control directly. Time is discretized into timesteps, either naturally (if the environment is a turn-based game, for instance) or artificially (by using sampling rates, like observing 30 frames per second).&lt;/p&gt;

&lt;p&gt;At each timestep, the agent will choose an action according to a decision process we call a &lt;strong&gt;policy&lt;/strong&gt;, and that action will make the environment change state. The agent is given rewards every time it takes an action, which can be positive or negative, typically real numbers.&lt;/p&gt;

&lt;p&gt;In this article and most of the literature, we model the environment as a Markov Discrete Process: this means all past history is ignored, or assumed to be contained in the current state. Imagine a chessboard: if I gave you one with all the pieces set in a certain position, which move is optimal would not depend on in which order the pieces arrived at their current places. You only care about the current state, not past states. This &lt;em&gt;memory loss&lt;/em&gt; property characterizes Markov Processes, which is where MDPs take their name from.&lt;/p&gt;

&lt;p&gt;Tasks can be &lt;em&gt;episodic&lt;/em&gt; or &lt;em&gt;continuing&lt;/em&gt;, where episodic tasks have a beginning and an end, like playing chess matches, and continuing ones are ongoing (like managing a business).&lt;/p&gt;

&lt;p&gt;The whole training process will be centered around &lt;strong&gt;modifying the agent’s policy&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;If we can make it choose actions better in each situation, it will perform better at the given task. To achieve this, we will have a value function that assigns an expected value to each state, or state-pair action. If we know what state each action leads to, and how much reward we should expect from a given state, then the best policy will be the one that always picks the actions that lead to the highest rewards over time.&lt;/p&gt;

&lt;p&gt;The state (or state-action) value is defined as the expected total reward from a given state (or state-action). That is, if we start on this state, and keep following the current policy, what will be the sum of all our rewards, averaged over many repetitions? This takes into account the possible non-determinism of the environment, where taking the same action in the same state may not always lead to the same next state, or yield the same fixed reward.&lt;/p&gt;

&lt;p&gt;Choosing the best policy, then, will be about estimating as correctly as possible what the value of each state and action is, and then always choosing the state with the highest value -what we call following a greedy policy-.&lt;/p&gt;

&lt;p&gt;There are still further considerations we might look into, which I will skip in this article, like whether the environment itself can change behavior over time (imagine finding the optimal move in poker, but then all the other players realize you always make the same bluff and start not buying into it) or how to deal with the expected reward in a continuing task (if it was just the sum, it would diverge, so we normally use exponential discounting).&lt;/p&gt;

&lt;p&gt;If you want to learn Reinforcement Learning in more detail, I recommend you read &lt;a href=&quot;http://incompleteideas.net/book/RLbook2020.pdf&quot;&gt;Introduction to Reinforcement Learning by Richard Sutton&lt;/a&gt; -the book is free-, of which I wrote a &lt;a href=&quot;/wiki/reinforcement-learning-sutton&quot;&gt;book summary here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;sarsa-and-tabular-methods&quot;&gt;Sarsa and Tabular Methods&lt;/h2&gt;

&lt;p&gt;For this article, we are going to focus on tabular methods for Reinforcement Learning.&lt;/p&gt;

&lt;p&gt;In tabular methods, the environment is modeled as a set of discrete states, where each possible state is assigned a unique identifier. If we go back to our chess example, each state would be a possible piece arrangement in the board. Depending on how we define things, we could include or exclude arrangements which are not really attainable in-game, like one player having 17 pieces, or 10 queens.&lt;/p&gt;

&lt;p&gt;As the name implies, our value function will be very straightforward: a lookup table that maps each possible state into a (real numbered) value.&lt;/p&gt;

&lt;p&gt;Remember that a state’s value is the expected sum of all subsequent rewards. Alternatively, we can add a discounting factor (a number between 0 and 1) that multiplies each subsequent term in the sum, to give more weight to short-term rewards and avoid divergence of the sum. This is called exponential discounting, as we will estimate the value of transitioning to a state as the reward from that transition, plus the discounted (multiplied by this factor) value of the next state, which was estimated by taking into account the discounted value of the next one, and so on.&lt;/p&gt;

&lt;p&gt;Thus, when the agent is standing in a certain state, it will pick the action that makes it transition into the highest-valued state next, or the one that maximizes expected next-state value if transitions are non-deterministic. As an example, imagine you can either take action A, to go to a state of value 5, or action B where you flip a coin and land on either one of two states with values 10 and -5. In this case taking action A would be better in terms of maximizing value, so an optimal policy would pick A every time.&lt;/p&gt;

&lt;p&gt;Thus emerge the Bellman equations, which predict the value of each state given the values of all other states under a defined policy.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/wiki/image_rl/Screen_Shot_2022-06-16_at_11-04-16.png&quot; alt=&quot;Bellman equation for state value&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;If we can make the Bellman equations converge for a system, then we’ve solved our Reinforcement Learning problem. The good news is, these converge if we use iterative linear equation solvers like Gauss-Seidel. The bad news is, for most interesting problems the state-space is so big, we can’t just plug the matrix into a linear solver and call it a day, as it would take ages for it to converge.&lt;/p&gt;

&lt;p&gt;Instead, we resort to focusing our estimations into the values of states we tread on more often, like the first turns of a game, or the most likely moves an opponent would make. We will update a state’s value as often as we land on it, thus naturally prioritizing the most likely states.&lt;/p&gt;

&lt;p&gt;In this context, &lt;strong&gt;Temporal-Difference (TD) methods&lt;/strong&gt; emerge.&lt;/p&gt;

&lt;p&gt;TD methods update their value estimates based in part on other value estimates, a practice called bootstrapping. Instead of running a whole episode and then updating all value estimates by taking them closer to the sum of the discounted rewards, they take each action based on the policy, and then immediately update the previous state’s value using the newly obtained information.&lt;/p&gt;

&lt;p&gt;Thus in Sarsa, the TD method we will use in this article, on each time step after taking an action the estimate for the action’s value on the current state will be nudged towards the sum of&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The obtained reward, plus&lt;/li&gt;
  &lt;li&gt;The discounted value of the next state-action we will encounter if we keep following the policy.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In math terms, with &lt;em&gt;γ&lt;/em&gt; the discount factor, &lt;em&gt;Q(s,a)&lt;/em&gt; the value function for taking action &lt;em&gt;a&lt;/em&gt; in state &lt;em&gt;s&lt;/em&gt;, and &lt;em&gt;r&lt;/em&gt; the reward at time &lt;em&gt;t&lt;/em&gt;, our update rule will be:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;resources/post_image/sarsaupdate.svg&quot; alt=&quot;sarsa update rule&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In this case, &lt;em&gt;α&lt;/em&gt; corresponds to the update size/learning rate, similar to the one used in neural networks, normally a value between 0 and 1 that determines how much the current instance influences the value estimation (usually, we initialize it to a higher value and make it closer to zero over time).&lt;/p&gt;

&lt;p&gt;Note that the bigger &lt;em&gt;γ&lt;/em&gt; is, the more we will care about subsequent rewards (as estimated by the value for the next &lt;em&gt;state-action&lt;/em&gt; pair). If &lt;em&gt;γ&lt;/em&gt; was 0, then we would only estimate the value as the expected reward (and not take the long-term into account), whereas if &lt;em&gt;γ&lt;/em&gt; is 1, then there is no incentive to rush: any reward, no matter how delayed, is as valuable as that which we could have obtained now.&lt;/p&gt;

&lt;p&gt;In that case, if for instance we have a bird fetching a grape, it would have no incentive to fly in a straight line instead of going in lots of circles, as delaying the grape would not change the state value. If instead we add discounting, suddenly it is more valuable to grab the grape sooner rather than later. Nonetheless in many episodic tasks, &lt;em&gt;γ&lt;/em&gt; is kept at 1 and the reward is only given at the end of the task if successfully completed, for instance when winning a game of chess -since no intermediate steps are certain to lead to victory-.&lt;/p&gt;

&lt;p&gt;Note also that this algorithm does not require any explicit estimation of the transition probabilities between states: we never need to estimate how likely it was that taking action &lt;em&gt;a&lt;/em&gt; in state &lt;em&gt;s&lt;/em&gt; led to state &lt;em&gt;s’&lt;/em&gt;, which could be intractable (like in card games with lots of possibilities) or outright impossible (in environments based in Physics, for instance). Instead, we just sample the next state over and over, and trust that the law of great numbers will lead us to eventual convergence.&lt;/p&gt;

&lt;p&gt;I hope by now you have an intuition of how Tabular Methods work. Now I will show you how we can implement them from scratch in Python, so you too can teach your computer to play games.&lt;/p&gt;

&lt;h2 id=&quot;maze-solver-a-python-sarsa-implementation&quot;&gt;Maze Solver: A Python Sarsa Implementation&lt;/h2&gt;

&lt;p&gt;For this article I wanted to build something fun. So, I made this: a Sarsa-based algorithm that receives an arbitrary labyrinth and finds a solution.&lt;/p&gt;

&lt;p&gt;To do this, I implemented the maze as a separate class. It only knows which actions can be taken on each state, what reward to give for each one, how big the maze is and what its shape is. It also knows where the player starts from and where it needs to go, and allows for transitions. Finally, it can make a player’s trip into a cute gif so I can share with you guys.&lt;/p&gt;

&lt;p&gt;The reward scheme is very simple: The maze hands out a reward of 100 if the maze is solved, -1 if the agent tries to bump into an internal maze wall, and 0 otherwise.&lt;/p&gt;

&lt;p&gt;As for Sarsa, I coded it from scratch so it:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Stores each state-action’s value in a dictionary (where the lookup is first by state, then by action).&lt;/li&gt;
  &lt;li&gt;Follows an ε-greedy policy (epsilon greedy), which means the agent chooses the best value action with probability 1-ε, or a random one with probability ε. However, I made it so it couldn’t choose to bump into an external boundary -so it can’t try to go off-limits-, though that behavior could have been learned.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To initiate learning, a loop starts that runs through &lt;em&gt;N&lt;/em&gt; episodes of the labyrinth, with the player always starting in the same position and needing to get to the same place, avoiding the walls of the maze. After each action is taken, the previous state-action’s value is updated in the map (as Sarsa learns online), and the agent always picks its action using the ε-greedy policy.&lt;/p&gt;

&lt;p&gt;After a number of episodes, ε is reduced to 0, so the agent can fine-tune the optimal policy. I made it so a gif is made in the first and last episodes, so we can see the difference between an agent that moves at random (luckily, even a random agent will reach the exit eventually, as diffusion is divergent, a property that even &lt;a href=&quot;/wiki/the-machinery-of-life&quot;&gt;proteins exploit in cells&lt;/a&gt;) and one that knows where it is going.&lt;/p&gt;

&lt;p&gt;Here are the relevant snippets, but you can also check the &lt;a href=&quot;https://github.com/StrikingLoo/maze-solver&quot;&gt;GitHub Project&lt;/a&gt; for the full code.&lt;/p&gt;

&lt;p&gt;The lookup table:
 &lt;script src=&quot;https://gist.github.com/StrikingLoo/1a7abd14725455e9200ae2fa256e8a83.js&quot;&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;The policy:
 &lt;script src=&quot;https://gist.github.com/StrikingLoo/6055bf081061b74c3d71adaaf3155cdc.js&quot;&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;The main loop (minus the prints and after cleaning). I used an &lt;em&gt;α&lt;/em&gt; value of 0.5, and a &lt;em&gt;γ&lt;/em&gt; of 0.9.
 &lt;script src=&quot;https://gist.github.com/StrikingLoo/a73489773ca8c11028047d142b8024f2.js&quot;&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;I polished them a bit compared with the ones on GitHub, for clarity’s sake.&lt;/p&gt;

&lt;p&gt;I ran the code for multiple mazes, and was happy to see all of the results were positive.&lt;/p&gt;

&lt;p&gt;Here is our agent solving a very simple maze: a wall running across the middle. The agent is the blue square, the goal -an apple- is the red one.&lt;/p&gt;

&lt;p&gt;Before training:
&lt;img src=&quot;resources/post_image/first_iter_1.gif&quot; alt=&quot;&quot; loading=&quot;lazy&quot; style=&quot;width:30%;height:30%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;After training:
&lt;img src=&quot;resources/post_image/last_iter_1.gif&quot; alt=&quot;&quot; loading=&quot;lazy&quot; style=&quot;width:30%;height:30%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;For a more advanced challenge, I tried a hockey-stick shape, where it needs to go through a narrow passage. It actually took it less time to learn this pattern, I guess because it was more constrained in the possible movements it could make.&lt;/p&gt;

&lt;p&gt;Before training:
&lt;img src=&quot;resources/post_image/first_iter_2.gif&quot; alt=&quot;&quot; loading=&quot;lazy&quot; style=&quot;width:30%;height:30%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;After training:
&lt;img src=&quot;resources/post_image/last_iter_2.gif&quot; alt=&quot;&quot; loading=&quot;lazy&quot; style=&quot;width:30%;height:30%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It performed similarly with a cross, even though in this case it had to back-pedal a bit.&lt;/p&gt;

&lt;p&gt;Before training:
&lt;img src=&quot;resources/post_image/first_iter_3.gif&quot; alt=&quot;&quot; loading=&quot;lazy&quot; style=&quot;width:30%;height:30%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;After training:
&lt;img src=&quot;resources/post_image/last_iter_3.gif&quot; alt=&quot;&quot; loading=&quot;lazy&quot; style=&quot;width:30%;height:30%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Then I tried making it go through narrow passages, one way and the other. This one took a long time for the random agent to crack.&lt;/p&gt;

&lt;p&gt;Before training:
&lt;img src=&quot;resources/post_image/first_iter_4.gif&quot; alt=&quot;&quot; loading=&quot;lazy&quot; style=&quot;width:30%;height:30%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;After training:
&lt;img src=&quot;resources/post_image/last_iter_4.gif&quot; alt=&quot;&quot; loading=&quot;lazy&quot; style=&quot;width:30%;height:30%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;And finally, just to see it could learn anything: what if it had to go through a wall that divided the whole map in half, and then follow it closely back in the other direction?&lt;/p&gt;

&lt;p&gt;Before training:
&lt;img src=&quot;resources/post_image/first_iter_5.gif&quot; alt=&quot;&quot; loading=&quot;lazy&quot; style=&quot;width:30%;height:30%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;After training:
&lt;img src=&quot;resources/post_image/last_iter_5.gif&quot; alt=&quot;&quot; loading=&quot;lazy&quot; style=&quot;width:30%;height:30%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In conclusion, this maze solver is a-mazing!&lt;/p&gt;

&lt;p&gt;As next steps, I think it could be fun to add moving enemies, like in Super Mario, and the agent needs to avoid them (or it is sent back to square one), or maybe add more than one goal but they need to be fetched in a certain order. At any rate, I think this example has been enough to showcase Reinforcement Learning’s capabilities, and it should be very easy to edit the Maze class in the GitHub project to add different mini games.&lt;/p&gt;

&lt;p&gt;Feel free to do it and, if you do, make a Pull Request. You’ll get credit and a link from me. I just want to see if anyone designs anything fun.&lt;/p&gt;

&lt;h2 id=&quot;conclusions&quot;&gt;Conclusions&lt;/h2&gt;

&lt;p&gt;We showed that Sarsa, and Temporal Difference methods in general, can solve complex problems like an arbitrary labyrinth. As long as we have enough time to train it, and a big enough memory, we could even solve problems where the state space is huge, like chess, using this same approach (and it has been done).&lt;/p&gt;

&lt;p&gt;For more complex problems, or ones where partial knowledge of the state suffices, state representation can be reutilized and we could aim for Approximate Learning, doing away with discrete methods as in Policy Gradient (where the state is mapped into a vector, not necessarily bijectively).&lt;/p&gt;

&lt;p&gt;I may write a follow-up showing how to use a convolutional neural network with Policy Gradient for this problem (plus moving obstacles or fog of war) if you guys are interested.&lt;/p&gt;

&lt;p&gt;One personal conclusion I arrived to is that bugs are a big headache. I spent about 2 hours fighting with the code because all the value estimates converged to the right values, but the agent was still moving in a very suboptimal way, until I discovered the policy code was correctly assessing the best action, and then choosing at random every time anyway, which was obviously a bug. I had misnamed a variable. Systematically debugging the code proved the best solution.&lt;/p&gt;

&lt;p&gt;Having added a test for every function in the Maze class was also liberating, as each new change could easily be verified to not break existing features.&lt;/p&gt;

&lt;p&gt;I know these conclusions are not Machine Learning oriented, but I guess my biggest challenges with this project were more design- than algorithms- heavy.&lt;/p&gt;

&lt;p&gt;As for speculation: I think one area that has been researched lately and has a lot of promise is the combination of Genetic Algorithms, Language Models that code, and Reinforcement Learning. In a recent &lt;a href=&quot;https://arxiv.org/pdf/2206.08896.pdf&quot;&gt;OpenAI paper&lt;/a&gt; (&lt;a href=&quot;/wiki/evolution-through-large-models&quot;&gt;my notes&lt;/a&gt;), a pipeline was used where:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;There was a task that could be solved with code, trying to optimize a transparent metric.&lt;/li&gt;
  &lt;li&gt;Multiple code solutions were taken, mutated with text modifying operators based on GPT-3 and, if they still worked, tested to see how far they went. The best candidates were kept and mutated repeatedly until very good candidates were found (with a loss that also encouraged diversity of niches).&lt;/li&gt;
  &lt;li&gt;The new candidates generated through these genetic algorithms were used to train a new autoregressive language model, which now could create plausible candidates.&lt;/li&gt;
  &lt;li&gt;That last model was trained through Reinforcement Learning to make new candidates &lt;strong&gt;conditioned on novel environments not seen by the previous models&lt;/strong&gt;, and rewarded with that same metric.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Now in this particular paper, the task was sodaracing, a videogame where a simple Physics engine is used to create robots that move as far as possible in the X axis. But imagine applying this to other things: automatically A/B testing the design of a blog, the layout of products in different branches of supermarkets to increase volume sold, or even military applications. Let alone making the jump from sodaracing to real world robots.&lt;/p&gt;

&lt;p&gt;I think the capabilities Reinforcement Learning is about to unlock are enormous, and not enough attention is being put into this field. Additionally, Richard Sutton’s team recently released a new &lt;a href=&quot;https://arxiv.org/pdf/2208.11173.pdf&quot;&gt;long-term strategy for AI&lt;/a&gt; with a focus on Reinforcement Learning, which proposes a roadmap towards AGI -human-level general AI-. I am not sure how far we are from that, but I think it deserves a big chunk of our attention.&lt;/p&gt;

&lt;p&gt;What other big things do you think could be automated by RL and how soon do you think they will impact the market? Let me know.&lt;/p&gt;

&lt;p&gt;If you liked this post, consider reading my summary of &lt;a href=&quot;/wiki/reinforcement-learning-sutton&quot;&gt;Introduction to Reinforcement Learning&lt;/a&gt;, and if you prefer to see more Python code or Optimization discussion, see my post on &lt;a href=&quot;/ant-colony-optimization-tsp&quot;&gt;Ant Colony Optimization&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;related-articles&quot;&gt;Related Articles&lt;/h2&gt;

&lt;p&gt;From the wiki:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;/wiki/reinforcement-learning-sutton&quot;&gt;Introduction to Reinforcement Learning (book notes)&lt;/a&gt; for a thorough review of the subject as I learned it.&lt;/li&gt;
  &lt;li&gt;A quick intro and roadmap in &lt;a href=&quot;/wiki/reinforcement-learning-base&quot;&gt;Reinforcement Learning&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;My article on &lt;a href=&quot;/wiki/proximal-policy-optimization&quot;&gt;Proximal Policy Optimization&lt;/a&gt; if you are looking for something more advanced. It covers a recent paper by OpenAI describing the current state of the art (as of 2022).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;If this post was useful or interesting, please share it on social media.&lt;/em&gt;&lt;/p&gt;
</content>
		</entry>
	
		<entry>
			<title>What Are the Most Common Onomatopoeia in Japanese?</title>
			<link href="http://strikingloo.github.io/japanese-most-frequent-onomatopoeia"/>
			<updated>2022-09-14T00:00:00-03:00</updated>
			<id>http://strikingloo.github.io/japanese-most-frequent-onomatopoeia</id>
			<content type="html">&lt;p&gt;The Japanese language has many quirks that make it alluring for students. After studying it for almost 8 years, one of the things that fascinates me is its rich pool of onomatopoeic expressions.&lt;/p&gt;

&lt;p&gt;English is not lacking in this respect: we have words like bang, clang, clatter and sputter which sound like the verbs they describe. Riffraff, whippersnapper and so on also have the property of alliteration. But Japanese, in my opinion, takes this to a whole new level.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&lt;strong&gt;Note:&lt;/strong&gt; This post is aimed at people who are learning Japanese, but not advanced, or people who are interested in knowing more about Japanese culture and language. Due to this, I will use romaji -western characters for Japanese words- a lot, even though it is aesthetically quite unsatisfying to look at.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;In the Japanese language there are basically &lt;strong&gt;two kinds of onomatopoeia&lt;/strong&gt;.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;擬音語 (&lt;em&gt;giongo&lt;/em&gt;) are words that mimic sounds. These could be the sound of barking (wan-wan), whistling (hyou-hyou), and so on.&lt;/li&gt;
  &lt;li&gt;擬態語 (&lt;em&gt;gitaigo&lt;/em&gt;), or mimetic words, are words that are supposed to mimic actions, states or emotions. They describe sensorial experiences outside of sound, like how a thing feels to the touch (e.g., &lt;em&gt;bisshori&lt;/em&gt;: soaking wet, or &lt;em&gt;gotsugotsu&lt;/em&gt;, coarse, gnarly).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Onomatopoeia in Japanese often come in the form of two syllables repeating twice (&lt;em&gt;gata-gata&lt;/em&gt;, &lt;em&gt;nuku-nuku&lt;/em&gt;), or two syllables joined by a pause and the syllable &lt;em&gt;‘ri’&lt;/em&gt; (e.g., &lt;em&gt;gussuri&lt;/em&gt;, where the double consonant is read gus-&lt;em&gt;pause&lt;/em&gt;-su-ri).&lt;/p&gt;

&lt;p&gt;I like the first kind especially, as it lends the language a playful tone. Gitaigo can serve to emphasize emotions or sensations that are sometimes harder to put into words (especially one single word) in other languages like my native Spanish.&lt;/p&gt;

&lt;p&gt;For this article, instead of picking my own favorite onomatopoeic or mimetic words, I chose to do a little analysis. Borrowing from an online web corpus, I looked into what the most common onomatopoeia in Japanese are. I filtered to limit the analysis to those words that follow the pattern I explained above.&lt;/p&gt;

&lt;p&gt;Here are the most commonly used mimetic words in Japanese:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;resources/post_image/gitaigo.svg&quot; alt=&quot;A chart showing the 12 most common onomatopoeia in Japanese with reduplication&quot; /&gt;&lt;/p&gt;

&lt;p&gt;If you are learning Japanese, just by memorizing these 12 words you will have made a decent progress into learning mimetic words. These also appear often if you’re watching anime. Especially &lt;em&gt;sorosoro&lt;/em&gt; (which can mean ‘for the moment…’), and &lt;em&gt;girigiri&lt;/em&gt; (which means ‘By a slim margin’, like ‘I was late for only 30 seconds!’ or ‘That almost got me!’),&lt;/p&gt;

&lt;p&gt;In more violent shows, &lt;em&gt;barabara&lt;/em&gt; also features prominently, as making someone go &lt;em&gt;barabara&lt;/em&gt; can mean dismembering them. And &lt;em&gt;dokidoki&lt;/em&gt; appears in 8 out of 10 anime openings.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Wakuwaku&lt;/em&gt; may need no introduction, since it is SpyxFamily’s Anya’s signature catchphrase. &lt;em&gt;Mechakucha&lt;/em&gt; means messy, both messy hair or a messy room.&lt;/p&gt;

&lt;p&gt;As for the adverbs ending in ri, here are the 9 most common (I stopped at 9 as they are significantly less common than the other category).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;resources/post_image/gitaigo-9-words.svg&quot; alt=&quot;A chart showing the 9 most common onomatopoeia in Japanese with ri-pattern&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Interestingly, I had never seen &lt;em&gt;sarari&lt;/em&gt; or &lt;em&gt;garari&lt;/em&gt; before. The others are all pretty common, with &lt;em&gt;niyari&lt;/em&gt; used a lot for bully characters, &lt;em&gt;pittari&lt;/em&gt; for clothes (e.g., &lt;em&gt;pittari au&lt;/em&gt; : it fits you nicely).&lt;/p&gt;

&lt;p&gt;If you are just here to see fun words, that’s it for today. I hope this entertained you! And maybe if you’re into anime, you’ll start hearing these sometimes from now on.&lt;/p&gt;

&lt;p&gt;If you are a student and think any of these words are going to be useful to you, I recommend you add them to your &lt;a href=&quot;/wiki/anki&quot;&gt;anki deck&lt;/a&gt; or any other SRS system you use. I actually thought of making an anki deck with the 21 words to accompany this post, but I feel the effort may be wasted depending on how many people are interested, so let me know if this would be a thing you would want to use (it would be free, obviously).&lt;/p&gt;

&lt;p&gt;Also feel free to use either image any way you want, as a consultation chart or just adding it to your blog. Just don’t erase the watermark, and consider linking to this post so I know it was useful to someone!&lt;/p&gt;

&lt;p&gt;Corpus Source: &lt;a href=&quot;https://nlb.ninjal.ac.jp/search/&quot;&gt;Ninjal LWP for BCCWJ&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;further-reading&quot;&gt;Further Reading&lt;/h3&gt;

&lt;p&gt;For a more thorough list of onomatopoeia and mimetic words in Japanese, refer to &lt;a href=&quot;https://www.tofugu.com/japanese/japanese-onomatopoeia/&quot;&gt;Tofugu’s Japanese Onomatopoeia Guide&lt;/a&gt;. I think the list here is so big it becomes sort of overwhelming, and I wish it were presented in a more pallatable way (maybe an Anki deck?). But I think this is a great resource to study at your leisure, perhaps in chunks.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;This is a slight departure from my usual programming related posts, so if you liked it please share it on Twitter or with your friends, so I know if I should keep making these too!&lt;/em&gt;&lt;/p&gt;
</content>
		</entry>
	
		<entry>
			<title>Ant Colony Optimization for the Travelling Salesman Problem</title>
			<link href="http://strikingloo.github.io/ant-colony-optimization-tsp"/>
			<updated>2022-09-06T00:00:00-03:00</updated>
			<id>http://strikingloo.github.io/ant-colony-optimization-tsp</id>
			<content type="html">&lt;p&gt;Ant Colony Optimization algorithms always intrigued me. They are loosely based in biology and the real protocols ants use to communicate and plan routes. They do this by coordinating through small pheromone messages: chemical trails they leave as they move forward, signaling for other ants to follow them. Even though each ant is not especially smart, and they follow simple rules individually, collectively they can converge to complex behaviors as a system, and amazing properties emerge.&lt;/p&gt;

&lt;p&gt;In the computational sense, Ant Colony Optimization algorithms solve complex optimization problems for which a closed-form or polynomial solution does not exist, by trying different “routes” across some relevant space or graph, and trying to find the most efficient one (typically the shortest) from two points that satisfies some constraints.&lt;/p&gt;

&lt;p&gt;Personally, I had a debt with myself from 5 years ago from an &lt;em&gt;Algorithms III&lt;/em&gt; class where Ant Colony Optimization was mentioned as an alternative to simulated annealing and Genetic Algorithms, but not expanded on and left as an exercise for future study. I remember back then the concept sounded interesting, but since I was busy with other matters I decided to postpone studying it. Now I find myself having more free time, so I finally decided to give it a try. What better way to verify I learned than coding an Ant Colony Optimization algorithm from scratch and showing it here?&lt;/p&gt;

&lt;p&gt;First, let’s start with some motivation: why would you want to learn about Ant Colony Optimization?&lt;/p&gt;

&lt;h2 id=&quot;the-travelling-salesman-problem&quot;&gt;The Travelling Salesman Problem&lt;/h2&gt;

&lt;p&gt;One especially important use-case for Ant Colony Optimization (ACO from now on) algorithms is solving the Travelling Salesman Problem (TSP).&lt;/p&gt;

&lt;p&gt;This problem is defined as follows: &lt;em&gt;Given a complete graph G with weighted edges, find the minimum weight Hamiltonian cycle. That is, a cycle that passes through each node exactly once and minimizes the total weight sum.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Note that the graph needs to be &lt;em&gt;complete&lt;/em&gt;: there needs to exist an edge connecting each possible pair of nodes. For graphs based in real places, this makes sense: you can just connect two places with an edge with a weight equal to their distance, or their estimated travel time.&lt;/p&gt;

&lt;p&gt;For a concrete example, look at the following graph.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;resources/post_image/TSP-graph-example.png&quot; alt=&quot;an image of a graph for travelling salesman problem&quot; loading=&quot;lazy&quot; style=&quot;height:40%; width:40%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In this case, the salesman wants to visit every home once and get back to where it started. Each edge joining two houses has a numeric label, representing the travel time between them in minutes. The salesman is a busy man, and would prefer to take as little time as possible in visiting all the houses. What would be the most efficient route?&lt;/p&gt;

&lt;p&gt;As an example, if we started from the house on the top left, we would want to go bottom, right, center, left again for a total of 80 minutes of travel. You can take a little time to convince yourself that is the right answer by hand, since this is a small case. Try to find a different route that would take less time to visit the four houses.&lt;/p&gt;

&lt;p&gt;Why is the Travelling Salesman Problem important? Many reasons.&lt;/p&gt;

&lt;p&gt;First, &lt;strong&gt;TSP appears everywhere in logistics&lt;/strong&gt;. Imagine you need to make multiple deliveries with a truck. You have packages, each of which has to go to a different place. What is the most time-efficient order to deliver them in and then go back to the warehouse? You just found the Travelling Salesman Problem.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;TSP is also important because it is an NP-Complete problem&lt;/strong&gt;. That means in the family of NP (nondeterministic polynomial time) problems -those problems for which verification of a solution takes polynomial time, even if finding that solution is harder-, it is in the hardest category: if we found a polynomial time solution for it, then since any other NP problem can be transformed into a TSP problem (sometimes through esoteric means, but still) in polynomial time too, we would have found a polynomial solution for all NP problems.&lt;/p&gt;

&lt;p&gt;Finding TSP can be solved in polynomial time would prove P=NP. This would be huge. To the point of being considered one of this century’s biggest questions. Suddenly swathes of hard problems would become easier to solve, and many new applications would open up, with multiple kinds of software becoming vastly more efficient. What it would do for logistics would probably contribute significantly to the world’s GDP and global trade.&lt;/p&gt;

&lt;p&gt;Before I digress further however, now that we know what TSP is, let’s see how to solve it. For more information, I recommend the &lt;a href=&quot;https://en.wikipedia.org/wiki/Travelling_salesman_problem&quot;&gt;Wikipedia article on TSP&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;ant-colony-optimization-solving-tsp&quot;&gt;Ant Colony Optimization: Solving TSP&lt;/h2&gt;

&lt;p&gt;There are many possible ways to solve the Travelling Salesman Problem for a given graph. As discussed above, there is no fast way to get the best solution for an arbitrary graph for certain, at least not without it taking a long time.&lt;/p&gt;

&lt;p&gt;The trivial way to solve TSP would be to look at all the possible Hamiltonian Cycles and keep the best one. This would imply looking at all possible orderings of nodes, which grow factorially -O(N!)- with the number N of nodes. Growing factorially is much worse than growing exponentially, for any base. It is so bad that even parallelism would not help: since adding a single node makes the problem N times harder, each extra node in the graph would require we grow the infrastructure superexponentially just to keep up. This would be extremely inefficient.&lt;/p&gt;

&lt;p&gt;Due to this, instead of looking for the exact solution for a graph, what most frameworks and solvers do is finding approximate solutions: can we find a way of connecting all nodes in a cycle that is “good enough”? To achieve this, multiple optimization algorithms exist. the &lt;em&gt;Networkx&lt;/em&gt; framework for graphs in Python solves TSP with &lt;em&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Christofides_algorithm&quot;&gt;Christofides&lt;/a&gt;&lt;/em&gt; or &lt;em&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Simulated_annealing&quot;&gt;Simulated Annealing&lt;/a&gt;&lt;/em&gt;, for example, of which the latter is quite similar to Ant Colony Optimization. Christofides has the nice property of never being wrong by more than 50% (so if the best cycle has a weight of 100, Christofides is guaranteed to find a cycle of weight at most 150).&lt;/p&gt;

&lt;p&gt;The algorithm we will see today is one such way of approximating a solution.&lt;/p&gt;

&lt;p&gt;In &lt;strong&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Ant_colony_optimization_algorithms&quot;&gt;Ant Colony Optimization Algorithms&lt;/a&gt;&lt;/strong&gt;, we will run a simulation of “ants” traversing the graph, constrained to only move in cycles, visiting each node exactly once. Each ant will leave, after finishing its traversal, a trail of pheromones that is proportional to the inverse weight of the discovered cycle (that is, if the cycle the ant encountered is twice as big, it will leave half the pheromones on each edge of the graph it went through, and so on).&lt;/p&gt;

&lt;p&gt;Finally, though we will make ants choose which edge to go through on each step of their traversal randomly, they will assign more preference to edges with more pheromones on them, and less preference to those with less pheromones. Additionally, if an edge is longer, it will receive less preference, since it implies higher travel times.&lt;/p&gt;

&lt;p&gt;These two preference adjustments could be linear, or any other polynomial (in my case, I tried many different coefficients and found the optimum to be sublinear for the pheromones, and quadratic or **1.5 for the distance).&lt;/p&gt;

&lt;p&gt;The pseudocode Wikipedia gives is:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;procedure ACO_MetaHeuristic is
    while not terminated do
        generateSolutions()
        daemonActions()
        pheromoneUpdate()
    repeat
end procedure
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;For this post, I coded Ant Colony Optimization (initially proposed by Marco Dorigo in 1992 in his PhD thesis) from scratch in Python using the Wikipedia article as a reference. I then ran a few experiments with it and benchmarked it against other algorithms for different problem instances.&lt;/p&gt;

&lt;p&gt;I used numpy for the traversals and other numerical operations, and pytest for testing. The whole code is &lt;a href=&quot;https://github.com/StrikingLoo/ant-colony-optimization&quot;&gt;available on GitHub&lt;/a&gt;, but I will show you the main parts step-by-step now. If you’re not interested in how the Ant Colony Optimization algorithm works in detail, you can skip straight to the &lt;a href=&quot;#tests-and-results&quot;&gt;results and benchmarks&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;First, I designed a minimal Graph class, whose code I will not include here since it is quite simple. Suffice it to say that the &lt;em&gt;.distance&lt;/em&gt; property holds an adjacency matrix with the weight -distance- for each edge.&lt;/p&gt;

&lt;p&gt;Then I coded the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;traverse_graph&lt;/code&gt; function, which represents a single ant going through the graph one node at a time, constrained to move in a cycle.&lt;/p&gt;

&lt;p&gt;The ant starts from a given node, and will at each step choose from among every node it has not stepped on yet, with a weighted distribution that assigns preference proportional to an edge’s pheromone load and to the inverse of its distance, each raised to a power that is a hyperparameter coefficient (&lt;em&gt;alpha&lt;/em&gt; and &lt;em&gt;beta&lt;/em&gt; respectively).&lt;/p&gt;

&lt;p&gt;That is, the probability of choosing a certain edge will be proportional to:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;resources/post_image/weight.png&quot; alt=&quot;weight equation for ant colony optimization&quot; loading=&quot;lazy&quot; style=&quot;height:25%; width:25%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Where P is the level of pheromones in that edge, and D the distance the edge covers. To get the distribution we sample from at each random jump, we normalize these weight coefficients so they add up to one.&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/StrikingLoo/432302f114822d24504cf6bab0ab3964.js&quot;&gt;&lt;/script&gt;

&lt;p&gt;After that, the optimization procedure itself consists of:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Initialize the graph with a constant (typically initially high, to encourage exploration) amount of pheromones on each edge.&lt;/li&gt;
  &lt;li&gt;Make &lt;em&gt;k&lt;/em&gt; ants start from random nodes and traverse the graph using the procedure defined above.&lt;/li&gt;
  &lt;li&gt;For each traversal, update the level of pheromones in its edges according to the function &lt;em&gt;Q/total_weight&lt;/em&gt;, where Q is a hyperparameter (a constant) and &lt;em&gt;total_weight&lt;/em&gt; is the sum of the distances of all the edges in the cycle. If using &lt;em&gt;elitism&lt;/em&gt;, add to the list of traversals the best one we have encountered so far, to incentivize the ants not to deviate too far from it.&lt;/li&gt;
  &lt;li&gt;If a cycle was found that beats the best one so far, update it.&lt;/li&gt;
  &lt;li&gt;All pheromone levels are multiplied by a &lt;em&gt;degradation constant&lt;/em&gt;, another hyperparameter between 0 and 1 that represents the passage of time and prevents bad past solutions to influence good recent ones too much.&lt;/li&gt;
  &lt;li&gt;Repeat for a certain number of iterations, or until convergence.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Intuitively, this converges to short cycles because &lt;strong&gt;each ant is leaving more pheromones in the edges on its cycle the shorter it is&lt;/strong&gt; and, as old pheromones fade over time, and new ants favor edges with more pheromones in them, &lt;strong&gt;new cycles will tend to be ever shorter&lt;/strong&gt;. Crucially, as each ant is choosing its next step at random, even though they will &lt;em&gt;tend&lt;/em&gt; to pick the candidates with the most pheromone every time, they will also have a non-negligible probability of picking a different edge and going off exploring. Should that lead to a better cycle overall, then that ant will tell future ants about it by leaving even more pheromones, as the cycle is shorter.&lt;/p&gt;

&lt;p&gt;Over time, we would expect the average ant traversal to get shorter and shorter.&lt;/p&gt;

&lt;p&gt;Additionally, I tried a few more modifications to the algorithm: the ‘elite’ or best candidate can be specified manually at the start (as that allows for reusing of the best solution from other runs) and I designed a protocol for increasing the amount of pheromones everywhere by a constant if progress stagnated -no new best cycle found after &lt;em&gt;patience&lt;/em&gt; iterations-, though I did not achieve better results through that. Also, after running &lt;em&gt;k&lt;/em&gt; ants, I only updated the pheromone trails with the best &lt;em&gt;k/2&lt;/em&gt; ants’ traversals instead of using them all. This did improve results quite significantly, as did using elite candidates –not keeping them made the algorithm more unstable and it converged a lot more slowly.&lt;/p&gt;

&lt;p&gt;Here is the whole function in all its glory (with comments for sanity).&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/StrikingLoo/778db2438b18d38f126082c046b19acd.js&quot;&gt;&lt;/script&gt;

&lt;p&gt;Some possible improvements for this algorithm that I didn’t have the time for:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Traversals could be trivially paralllelized since each ant is independent. This can be done easily using the &lt;em&gt;multiprocessing&lt;/em&gt; Python module, but it doesn’t work on Mac by default. In this tradeoff, I chose portability over speed.&lt;/li&gt;
  &lt;li&gt;Choosing the next jump in a traversal can be done in parallel with numpy vector multiplication, which resulted in everything running about 5x faster. However due to numerical instability, a jump could be performed to the same node over and over, even though I was multiplying by zero, and solving this bug would have taken more time than I thought worth it. If you find a way to make this work for all cases, then feel free to make a pull request and you will get the credit and a link.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;tests-and-results&quot;&gt;Tests and Results&lt;/h2&gt;

&lt;p&gt;After coding the algorithm and testing it in toy cases, I was happy to find that the internet had provided me with a wealth of different graphs and TSP problems to try it on.&lt;/p&gt;

&lt;p&gt;I got my first small but real test case from this &lt;a href=&quot;https://towardsdatascience.com/solving-the-travelling-salesman-problem-for-germany-using-networkx-in-python-2b181efd7b07&quot;&gt;Medium Article&lt;/a&gt; using real Germany cities data. I was happy to see ACO found the optimal solution in seconds.&lt;/p&gt;

&lt;p&gt;Then I found the huge &lt;a href=&quot;http://cs.uef.fi/sipu/santa/data.html&quot;&gt;Santa Claus Challenge&lt;/a&gt; with coordinates data representing millions of houses in Finland (for Santa to visit). The entire dataset did not fit in memory, so I could not verify how close my solution got to the best ones in the challenge, but taking ever bigger samples let me see how fast or slow each part of the program was for profiling. Go to the &lt;a href=&quot;https://www.frontiersin.org/articles/10.3389/frobt.2021.689908/full&quot;&gt;challenge’s article&lt;/a&gt; for a fun read.&lt;/p&gt;

&lt;p&gt;Finally, my favorite resource for finding TSP problems, often with their optimal cycle’s weight, was &lt;a href=&quot;http://comopt.ifi.uni-heidelberg.de/software/TSPLIB95/XML-TSPLIB/instances/&quot;&gt;Heidelberg University’s site&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I used that site’s Berlin dataset for most of my benchmarking and hyperparameter optimization, from which I found the best &lt;em&gt;alpha&lt;/em&gt; and &lt;em&gt;beta&lt;/em&gt; values to be around &lt;em&gt;0.9&lt;/em&gt; and &lt;em&gt;1.5&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;I was happy to see that, while Networkx’s &lt;em&gt;TSP solve&lt;/em&gt; took 2 seconds and this program took a couple minutes, my solution for that dataset had a weight of ~44000 whereas Networkx’s was around 46k. This proves for some cases, even though slower, ACO algorithms could be a good approach for solving TSP problems.&lt;/p&gt;

&lt;h2 id=&quot;experiments&quot;&gt;Experiments&lt;/h2&gt;

&lt;p&gt;Encouraged by the comments in Reddit, I decided to experiment further and see how the optimization behaved in different situations.&lt;/p&gt;

&lt;p&gt;Particularly, since ACO can be updated online, it is supposed to perform well in dynamic network or logistics problems where the graph is shifting in real time, in comparison with other algorithms which need to be re-run from scratch.&lt;/p&gt;

&lt;p&gt;Since the ants update their pheromone trails in real time, whenever there is a shift in the edge’s distances they should eventually notice it and change their path to reflect it. For instance if two nodes got closer (the distance value in the edge joining them was reduced) then more ants should want to cross between them, and its pheromone load should grow larger. Alternatively if two nodes grow farther apart, the ants should shun them more.&lt;/p&gt;

&lt;p&gt;To test whether this was the case, I tried two experiments. In both of them I started with the Berlin graph I had looked at earlier, which I knew the algorithm converged in after about 500 iterations of 50 ants each.&lt;/p&gt;

&lt;p&gt;For the first experiment, after the 500th iteration I selected the edge with the highest amount of pheromones, and made its weight 10 times bigger. That is, if the edge was joining nodes i and j, then the distance between them grew 10 times larger.&lt;/p&gt;

&lt;p&gt;I wanted to see how quickly the swarm would respond to this change, so I plotted the pheromone load for that edge from iteration 500 onwards for 500 more iterations.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;resources/post_image/ant_trail_smaller.png&quot; alt=&quot;An image depicting a graph of decreasing pheromone trails after an edge&apos;s weight grew bigger&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;As you can see, the ants don’t respond instantaneously to the changes, but after 30 iterations they have adapted to them and do not visit that edge nearly as often as before. Its pheromone level remains close to zero afterward, with occasional peaks probably due to some of the exploration incentives I set.&lt;/p&gt;

&lt;p&gt;For the second experiment, I took a random Hamiltonian cycle and divided all of its edges by 10. This way, this cycle suddenly became tempting for the ants, as it was a cheap way of traversing the whole graph, smaller by an order of magnitude. Again this change took place in the 500th iteration, so I wanted to see how the ants reacted.&lt;/p&gt;

&lt;p&gt;I looked at the mean pheromone load for edges in the diminished cycle, and this is what it looked like.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;resources/post_image/ant_trail_mean.png&quot; alt=&quot;An image depicting a graph of increasing pheromone trails after a cycle grew shorter, incentivizing ants to explore it&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;As expected, the ants were highly incentivized to deviate from their known paths and explore this cycle (it had a third of the weight of the next smallest cycle that the colony had found so far). After a single iteration, the average pheromone levels for that cycle had increased dramatically.&lt;/p&gt;

&lt;p&gt;This shows that, as long as the algorithm contemplates the possibility of change by always encouraging a minimum level of exploration, new opportunities can be exploited as they arise.&lt;/p&gt;

&lt;p&gt;Interestingly, if the minimum level of pheromones was plotted instead of the mean, it would not rise by a lot. I think this is because even after dividing by ten, a few of the edges in the best solution were still not included in this cycle. This can further be attested by the dip in average pheromone levels by the end of the graph above. I believe in the last 50 iterations a cycle was found that contained an edge that had not been diminished, but was nonetheless small enough to present an improvement.&lt;/p&gt;

&lt;h2 id=&quot;conclusions&quot;&gt;Conclusions&lt;/h2&gt;

&lt;p&gt;We showed that Ant Colony Optimization can be implemented quite easily in Python, and since many of its operations can be vectorized or parallelized it should not be too slow, though not it is not as fast as Christofides’s algorithm or others.&lt;/p&gt;

&lt;p&gt;More importantly, we showed that in many datasets, ACO can converge to the optimal solution, and in many others its flexibility allows it to find better solutions (shorter traversals) than simpler algorithms.&lt;/p&gt;

&lt;p&gt;Additionally, it could be seen that one of the best properties of Ant Colony Optimization over other algorithms is its capability for online adaptation to changes in the system. In certain situations this could prove critical for performance, especially if rapid response is encouraged.&lt;/p&gt;

&lt;p&gt;On a more philosophical level, I think it is beautiful how by specifying a large set of simple agents that each follow a few rules, we could solve a problem that is known to be hard.&lt;/p&gt;

&lt;p&gt;I would like to try Ant Colony Optimization for problems other than TSP in the future, so if you know of any other applications where ACO shines, let me know.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;If you enjoyed this article, please share it on Twitter or with a friend.&lt;/strong&gt; I write these for you and would be happy if more people can read them and share my love for algorithms.&lt;/p&gt;

&lt;h2 id=&quot;suggested-further-reading&quot;&gt;Suggested Further Reading&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://kazimuth.github.io/blog/post/shake-and-pull-gently/&quot;&gt;&lt;em&gt;Shake and Pull Gently&lt;/em&gt;, Kazimuth&lt;/a&gt;: This post reminded me of my love for search and optimization algorithms, and I recommend it full-heartedly.&lt;/li&gt;
  &lt;li&gt;Reddit User &lt;em&gt;/u/git&lt;/em&gt;’s comments on &lt;a href=&quot;https://www.reddit.com/r/programming/comments/wx69fs/comment/ilplkgs/&quot;&gt;Ant Behavior&lt;/a&gt; and &lt;a href=&quot;https://www.reddit.com/r/funny/comments/wt1fcr/comment/il1w9u2/&quot;&gt;Ant Trails&lt;/a&gt;, which originally inspired me to write this post.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.frontiersin.org/articles/10.3389/frobt.2021.689908/full&quot;&gt;Solving the Large-Scale TSP Problem in 1 h: Santa Claus Challenge 2020&lt;/a&gt;: A fun challenge and a good explanation of TSP.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/2205.15678v1.pdf&quot;&gt;Automatic Relation-aware Graph Network Proliferation&lt;/a&gt;: Using Graph Neural Networks to solve, among other things, TSP.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/maoaiz/tsp-genetic-python&quot;&gt;TSP Genetic Python&lt;/a&gt;: A genetic algorithm for solving TSP.&lt;/li&gt;
&lt;/ul&gt;
</content>
		</entry>
	
		<entry>
			<title>Stable Diffusion: Prompt Guide and Examples</title>
			<link href="http://strikingloo.github.io/stable-diffusion-vs-dalle-2"/>
			<updated>2022-08-30T00:00:00-03:00</updated>
			<id>http://strikingloo.github.io/stable-diffusion-vs-dalle-2</id>
			<content type="html">&lt;p&gt;What a week, huh? A few days ago, &lt;a href=&quot;https://stability.ai&quot;&gt;Stability.ai&lt;/a&gt; released the new AI art model Stable Diffusion. It is similarly powerful to &lt;a href=&quot;/DALL-E-2-prompt-guide&quot;&gt;DALL-E 2&lt;/a&gt;, but open source, and open to the public through &lt;a href=&quot;https://beta.dreamstudio.ai/dream&quot;&gt;Dream Studio&lt;/a&gt;, where anyone gets 50 free uses just by signing up with an email address.&lt;/p&gt;

&lt;p&gt;Since it is open source and anyone who has 5GB of GPU VRAM can download it (and Emad Mostaque, Stability.ai’s founder has come out and said more efficient models are coming) to get unlimited uses, expect to keep seeing headlines about AI art for a while.&lt;/p&gt;

&lt;p&gt;I am tired of repeating the same old speech, but &lt;em&gt;thinking back to how primitive models were just a year and a half ago with DALL-E and other VQVAE, this is completely insane&lt;/em&gt;. I can only imagine what applications artists and other users will come up with in the near future by leveraging StableDiffusion’s embeddings and its text-to-image capabilities, let alone whatever the next generation of models will be able to do.&lt;/p&gt;

&lt;p&gt;Extrapolating from how much this field has grown in the last 18 months, I wouldn’t be surprised if in 2 more years you can write a script for a comic book, feed it to some large language encoder and a text-to-image model like this, and get a fully illustrated, style-coherent graphic novel. This would also apply for frames for an animated movie or a storyboard.&lt;/p&gt;

&lt;p&gt;Are we really that close to something so big? I feel like the technology is there if enough compute and budget were allocated, but I am not sure whether someone will do it. I don’t see any obvious blockers or barriers to the next generation of models being even bigger or understanding style better.&lt;/p&gt;

&lt;p&gt;Given this context, many people are concerned some artists may lose their jobs. After lots of discussion in Reddit and at parties, I will try to summarize my current opinion on that topic.&lt;/p&gt;

&lt;p&gt;For use-cases where having a human artist brings the least value, I think text-to-image models will dominate the market. However, for those cases I think we already had stock images. For instance if I am adorning a random blog post, I’d rather get a free stock image in the header than pay an artist for a new professional photo, as I don’t think my readers care that much (see &lt;a href=&quot;https://deephaven.io/blog/2022/08/08/AI-generated-blog-thumbnails/&quot;&gt;“I replaced all our blog thumbnails using DALL·E 2”&lt;/a&gt; for an example).&lt;/p&gt;

&lt;p&gt;Especially if this site was monetized and the big picture was just there to make you scroll further down and get more engagement and ad views.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;resources/ai-generated-images/stable-diffusion/selected/2835514280_A_mossy_fallen_tree_with_butterflies_and_azaleas_and_lavender__by_Josef_Thoma__matte_painting_trending_on_artstation_HQ__concept_art-add.jpg&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;For cases where the artist’s vision matters, like original paintings for decorating my home, or the panels for a graphic novel, I think StableDiffusion or DALL-E 2 for that matter are far away from beating humans. So far.&lt;/p&gt;

&lt;p&gt;However, I guess many freelance artists who work for commissions may find less demand for their work as bloggers or random people get their art itch scratched by AI art. I would love to hear the opinion of artists from that sort of market on this, as I am quite ignorant of how the whole process works (for instance, what kind of people commission art in the first place).&lt;/p&gt;

&lt;p&gt;I think models like this can also enhance artists’ work. Say you are asked to draw 5 illustrations of a character doing different things. You could use dall-e to get 5 relevant background scenes in 5 minutes, then use your time to add in the characters and some details on top of them. AI art models are significantly better at drawing background scenes than action and characters, so this is a combination of the best capabilities of both human and machine.&lt;/p&gt;

&lt;p&gt;Compare against drawing the whole thing from scratch.&lt;/p&gt;

&lt;p&gt;Obviously style matching would not be easy, but when we reach a point where you can supply your own mini dataset of backgrounds to condition on and style transfer, this may make artists able to work much faster.&lt;/p&gt;

&lt;p&gt;Now is this a good thing? I guess it is a double edged sword: will artists who leverage AI art models to become more productive drive down the general price of art? Or will the new supply generate new demand? I am not an economist, nor an artist, so speculating further would be futile and I will leave the rest to my dear readers.&lt;/p&gt;

&lt;p&gt;Speculate away, please.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&lt;strong&gt;Extra reflection on the topic&lt;/strong&gt;: As I said on Reddit, I would be worried that eventually you may remove the artist from some parts of the design loop if user feedback or artist feedback could be used as a reward for a reinforcement learning agent.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Something like asking “did you like this city’s design / this building? 1 to 10” to users, and using this for a policy gradient or similar algorithm.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;I am reminded of &lt;a href=&quot;/wiki/evolution-through-large-models&quot;&gt;Evolution through Large Models&lt;/a&gt; where this approach is used for autogenerated instances made through genetic mutation algorithms. Imagine the same but for text-to-image.&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;stable-diffusion-art&quot;&gt;Stable Diffusion Art&lt;/h2&gt;

&lt;p&gt;As in my &lt;a href=&quot;/DALL-E-2-prompt-guide&quot;&gt;DALL-E 2 article&lt;/a&gt;, I tried the same prompts I had already tried in &lt;a href=&quot;/art-prompts&quot;&gt;Craiyon or Guided diffusion&lt;/a&gt; (I know, that’s &lt;em&gt;so&lt;/em&gt; early 2022!) to show just how much these systems have improved.&lt;/p&gt;

&lt;p&gt;My focus was on fantasy, science fiction, and steampunk illustrations, because that is what I like, but I also experimented with more complex scenes and descriptions, to see how well StableDiffusion understands things like scene composition, prepositions and element interactions.&lt;/p&gt;

&lt;p&gt;As with DALL-E 2, I found the model yields much better results for prompts that do not ask for a certain action or verb to be performed, but rather static scenes, especially if there are no humanoids or moving characters in them.&lt;/p&gt;

&lt;p&gt;For everything else though, the results were astounding. I feel like StableDiffusion beats DALL-E 2 in character design and realism, but loses in the beauty of its landscapes and backgrounds, but take this with a 10% certainty as it is only my intuition and is likely be biased by my prompt choices.&lt;/p&gt;

&lt;p&gt;One thing I’ve found helps a lot in getting more beautiful results is thinking of which exact visual effect would add to the picture, and specifying it. For instance, for fantasy illustrations usually adding fireflies or sparkles helps. For landscapes, I like naming specific flowers like azaleas, and for buildings naming features like a column, a fountain, or anything else to ground the picture in a certain time and place, even if the detail is only tangential to the whole picture.&lt;/p&gt;

&lt;p&gt;In my opinion these details can steer the model even better than many vague cues like “4k”, in this generation of models -unlike in &lt;a href=&quot;/wiki/clip&quot;&gt;CLIP&lt;/a&gt; or &lt;a href=&quot;/wiki/glide&quot;&gt;GLIDE&lt;/a&gt;-. See &lt;a href=&quot;#appendix-a-stable-diffusion-prompt-guide&quot;&gt;Appendix A: StableDiffusion Prompt Guide&lt;/a&gt; to see how I choose most of my prompts, and some advice.&lt;/p&gt;

&lt;h3 id=&quot;prompt-examples-and-experiments&quot;&gt;Prompt Examples and Experiments&lt;/h3&gt;

&lt;p&gt;I will begin with some scenes that I already tried with other models. These are some of the stable diffusion prompts that I liked best.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“A digital illustration of a steampunk library with clockwork machines, 4k, detailed, trending in artstation, fantasy vivid colors”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;resources/ai-generated-images/stable-diffusion/selected/1397887120__A_digital_illustration_of_a_steampunk_library_with_clockwork_machines__4k__detailed__trending_in_artstation__fantasy_vivid_colors_.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;resources/ai-generated-images/stable-diffusion/selected/18524077__A_digital_illustration_of_a_steampunk_library_with_clockwork_machines__4k__detailed__trending_in_artstation__fantasy_vivid_colors_.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;resources/ai-generated-images/stable-diffusion/selected/2528743898__A_digital_illustration_of_a_steampunk_library_with_clockwork_machines__4k__detailed__trending_in_artstation__fantasy_vivid_colors_.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I like the last one especially.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“A digital illustration of a steampunk flying machine in the sky with cogs and mechanisms, 4k, detailed, trending in artstation, fantasy vivid colors”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;resources/ai-generated-images/stable-diffusion/selected/1113348597__A_digital_illustration_of_a_steampunk_flying_machine_in_the_sky_with_cogs_and_mechanisms__4k__detailed__trending_in_artstation__fantasy_vivid_colors_.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;resources/ai-generated-images/stable-diffusion/selected/1839163504__A_digital_illustration_of_a_steampunk_flying_machine_in_the_sky_with_cogs_and_mechanisms__4k__detailed__trending_in_artstation__fantasy_vivid_colors_.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;resources/ai-generated-images/stable-diffusion/selected/1870656804_A_clockwork_flying_machine_from_the_renaissance__oil_painting_on_canvas__trending_on_artstation_HQ.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;resources/ai-generated-images/stable-diffusion/selected/2850435724__A_digital_illustration_of_a_steampunk_flying_machine_in_the_sky_with_cogs_and_mechanisms__4k__detailed__trending_in_artstation__fantasy_vivid_colors_.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;resources/ai-generated-images/stable-diffusion/selected/79036762_A_digital_illustration_of_a_steampunk_flying_machine__4k__detailed__trending_in_artstation__fantasy_vivid_colors.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;For scene composition, it is still struggling. Here are a ferret and a badger (which the model turned into another ferret) fencing with swords.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;resources/ai-generated-images/stable-diffusion/selected/169820319_A_ferret_and_a_badger_fighting_with_swords__by_Josef_Thoma__matte_painting_trending_on_artstation_HQ.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;As expected, some of the best prompts that had worked for DALL-E 2 and Craiyon worked great with stable diffusion too.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;A digital Illustration of the Babel tower, 4k, detailed, trending in artstation, fantasy vivid colors&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;resources/ai-generated-images/stable-diffusion/selected/200385555_A_digital_illustration_of_the_Babel_tower__4k__detailed__trending_in_artstation__fantasy_vivid_colors.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;resources/ai-generated-images/stable-diffusion/selected/3059386456_A_digital_illustration_of_the_Babel_tower__4k__detailed__trending_in_artstation__fantasy_vivid_colors.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;resources/ai-generated-images/stable-diffusion/selected/3565221494_A_digital_illustration_of_the_Babel_tower__4k__detailed__trending_in_artstation__fantasy_vivid_colors.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Cluttered house in the woods, anime, oil painting, high resolution, cottagecore, ghibli inspired, 4k&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;resources/ai-generated-images/stable-diffusion/selected/2159513123__Cluttered_house_in_the_woods__anime_oil_painting_high_resolution_cottagecore_ghibli_inspired_4k_.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;resources/ai-generated-images/stable-diffusion/selected/232013801__Cluttered_house_in_the_woods__anime_oil_painting_high_resolution_cottagecore_ghibli_inspired_4k.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;resources/ai-generated-images/stable-diffusion/selected/2391510358__Cluttered_house_in_the_woods__anime_oil_painting_high_resolution_cottagecore_ghibli_inspired_4k_.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;resources/ai-generated-images/stable-diffusion/selected/499419696__Cluttered_house_in_the_woods__anime_oil_painting_high_resolution_cottagecore_ghibli_inspired_4k_.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In my opinion, both prompts yielded better images here than they did for DALLE-2, but you can be the judge of that.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;A digital illustration of a medieval town, 4k, detailed, trending in artstation, fantasy&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;resources/ai-generated-images/stable-diffusion/selected/1099736972__A_digital_illustration_of_a_medieval_town__4k__detailed__trending_in_artstation__fantasy_.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;resources/ai-generated-images/stable-diffusion/selected/1381417243__A_digital_illustration_of_a_medieval_town__4k__detailed__trending_in_artstation__fantasy_.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;resources/ai-generated-images/stable-diffusion/selected/3707234254__A_digital_illustration_of_a_medieval_town__4k__detailed__trending_in_artstation__fantasy_.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;A medieval town with disco lights and a fountain, by Josef Thoma, matte painting trending on artstation HQ, concept art&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;resources/ai-generated-images/stable-diffusion/selected/1607022320_A_medieval_town_with_disco_lights_and_a_fountain__by_Josef_Thoma__matte_painting_trending_on_artstation_HQ__concept_art.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;resources/ai-generated-images/stable-diffusion/selected/1723824377_A_medieval_town_with_disco_lights_and_a_fountain__by_Josef_Thoma__matte_painting_trending_on_artstation_HQ__concept_art.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;resources/ai-generated-images/stable-diffusion/selected/2405556329_A_medieval_town_with_disco_lights_and_a_fountain__by_Josef_Thoma__matte_painting_trending_on_artstation_HQ__concept_art.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;A digital illustration of a treetop house with fireflies, vivid colors, 4k, fantasy [,/organic]&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;resources/ai-generated-images/stable-diffusion/selected/1176496391_A_digital_illustration_of_a_treetop_house_with_fireflies__vivid_colors__4k__fantasy.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;resources/ai-generated-images/stable-diffusion/selected/1795930148_A_digital_illustration_of_a_treetop_house_with_fireflies__vivid_colors__4k__fantasy__organic.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;resources/ai-generated-images/stable-diffusion/selected/1864580786_A_digital_illustration_of_a_treetop_house_with_fireflies__vivid_colors__4k__fantasy__organic.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;resources/ai-generated-images/stable-diffusion/selected/3103402568_A_digital_illustration_of_a_treetop_house_with_fireflies__vivid_colors__4k__fantasy.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;resources/ai-generated-images/stable-diffusion/selected/3653447808_A_digital_illustration_of_a_treetop_house_with_fireflies__vivid_colors__4k__fantasy.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;paintings-of-landscapes&quot;&gt;Paintings of Landscapes&lt;/h3&gt;

&lt;p&gt;Again back to comparing with DALL-E 2 and Craiyon, I tried asking for landscape paintings of mansions, castles and general garden scenes. I expected a more classical feel as opposed to the 
more digital illustration like air of the images from before.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;A beautiful castle beside a waterfall in the woods, by Josef Thoma, matte painting, trending on artstation HQ&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;resources/ai-generated-images/stable-diffusion/selected/1495253853_A_beautiful_castle_beside_a_waterfall_in_the_woods__by_Josef_Thoma__matte_painting_trending_on_artstation_HQ.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;resources/ai-generated-images/stable-diffusion/selected/538903565_A_beautiful_castle_beside_a_waterfall_in_the_woods__by_Josef_Thoma__matte_painting_trending_on_artstation_HQ.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;resources/ai-generated-images/stable-diffusion/selected/900548929_A_beautiful_castle_beside_a_waterfall_in_the_woods__by_Josef_Thoma__matte_painting_trending_on_artstation_HQ.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;A beautiful mansion with flowered gardens and a fountain, digital illustration, 4k, detailed, bokeh&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;resources/ai-generated-images/stable-diffusion/selected/1190172094_A_beautiful_mansion_with_flowered_gardens_and_a_fountain__digital_illustration__4k__detailed__bokeh.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;resources/ai-generated-images/stable-diffusion/selected/1214903560_A_beautiful_mansion_with_flowered_gardens_and_a_fountain__painting__oil_on_canvas__4k__detailed__bokeh.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;resources/ai-generated-images/stable-diffusion/selected/1460966951_A_beautiful_mansion_beside_a_waterfall_in_the_woods__by_Josef_Thoma__matte_painting_trending_on_artstation_HQ.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;A beautiful mansion beside a waterfall in the woods, by josef thoma, matte painting, trending on artstation HQ&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;resources/ai-generated-images/stable-diffusion/selected/1963473302_A_beautiful_mansion_beside_a_waterfall_in_the_woods__by_Josef_Thoma__matte_painting_trending_on_artstation_HQ.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;resources/ai-generated-images/stable-diffusion/selected/258823595_A_beautiful_mansion_beside_a_waterfall_in_the_woods__by_Josef_Thoma__matte_painting_trending_on_artstation_HQ.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;A beautiful mansion with flowered gardens and a fountain, painting, oil on canvas, 4k, detailed, thomas cole&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;resources/ai-generated-images/stable-diffusion/selected/2766143347_A_beautiful_mansion_with_flowered_gardens_and_a_fountain__painting__oil_on_canvas__4k__detailed__thomas_cole.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;resources/ai-generated-images/stable-diffusion/selected/768722554_A_beautiful_mansion_with_flowered_gardens_and_a_fountain__painting__oil_on_canvas__4k__detailed__thomas_cole.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;resources/ai-generated-images/stable-diffusion/selected/829486673_A_beautiful_mansion_with_flowered_gardens_and_a_fountain__painting__oil_on_canvas__4k__detailed__thomas_cole.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;A beautiful mansion with flowered gardens and a fountain, painting, oil on canvas, 4k, detailed, bokeh&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;resources/ai-generated-images/stable-diffusion/selected/3425994411_A_beautiful_mansion_with_flowered_gardens_and_a_fountain__digital_illustration__4k__detailed__bokeh.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;resources/ai-generated-images/stable-diffusion/selected/3746536288_A_beautiful_mansion_with_flowered_gardens_and_a_fountain__digital_illustration__4k__detailed__bokeh.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;resources/ai-generated-images/stable-diffusion/selected/3853609176_A_beautiful_mansion_with_flowered_gardens_and_a_fountain__painting__oil_on_canvas__4k__detailed__bokeh.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;resources/ai-generated-images/stable-diffusion/selected/474726138_A_beautiful_mansion_with_flowered_gardens_and_a_fountain__painting__oil_on_canvas__4k__detailed__bokeh.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;style-cue-steampunk--clockpunk&quot;&gt;Style cue: Steampunk / Clockpunk&lt;/h3&gt;

&lt;blockquote&gt;
  &lt;p&gt;A digital illustration of a steampunk robot [/with cogs and clockwork y Josef Thoma], 4k, deatiled, trending in artstation, fantasy vivid colors&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;resources/ai-generated-images/stable-diffusion/selected/1173280232_A_digital_illustration_of_a_steampunk_robot__4k__detailed__trending_in_artstation__fantasy_vivid_colors.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;resources/ai-generated-images/stable-diffusion/selected/733543897_A_digital_illustration_of_a_steampunk_robot__4k__detailed__trending_in_artstation__fantasy_vivid_colors.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;resources/ai-generated-images/stable-diffusion/selected/2920284979_A_digital_illustration_of_a_steampunk_robot__4k__detailed__trending_in_artstation__fantasy_vivid_colors.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;resources/ai-generated-images/stable-diffusion/selected/1534798598_A_steampunk_robot_with_cogs_and_clockwork__by_Josef_Thoma__matte_painting_trending_on_artstation_HQ.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;resources/ai-generated-images/stable-diffusion/selected/2876423299_A_steampunk_robot_with_cogs_and_clockwork__by_Josef_Thoma__matte_painting_trending_on_artstation_HQ.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;resources/ai-generated-images/stable-diffusion/selected/2878664672_A_steampunk_robot_with_cogs_and_clockwork__by_Josef_Thoma__matte_painting_trending_on_artstation_HQ.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;recommended-style-cue-beksinski&quot;&gt;Recommended Style Cue: Beksinski&lt;/h3&gt;

&lt;p&gt;I tried adding the name of the painter &lt;a href=&quot;https://es.wikipedia.org/wiki/Zdzis%C5%82aw_Beksi%C5%84ski&quot;&gt;Beksinski&lt;/a&gt; as a style cue, and the results were mixed: many of them blocked by StableDiffusion’s content policy, which I guess means there was something awful in them, but the survivors looked amazing.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;resources/ai-generated-images/stable-diffusion/selected/2118950198_A_pile_of_computers_and_TVs_with_raining_sulfur__beksinski__4k__detailed__oil_on_canvas.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;resources/ai-generated-images/stable-diffusion/selected/2558887622_A_pile_of_computers_and_TVs__beksinski__4k__detailed__oil_on_canvas.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;resources/ai-generated-images/stable-diffusion/selected/3319505391_A_pile_of_computers_and_TVs_with_raining_sulfur__beksinski__4k__detailed__oil_on_canvas.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;resources/ai-generated-images/stable-diffusion/selected/3472481486_A_dark_gentleman_surrounded_by_piles_of_skulls_in_fog_hell__beksinski__4k__detailed__oil_on_canvas.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;resources/ai-generated-images/stable-diffusion/selected/37591142_A_pile_of_computers_and_TVs_with_raining_sulfur__beksinski__4k__detailed__oil_on_canvas.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;resources/ai-generated-images/stable-diffusion/selected/3790635847_An_astronaut_being_surrounded_by_eldritch_tentacles_in_the_void_of_space_with_stars__beksinski__4k__detailed__oil_on_canvas.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;anthropomorphic-animals-mostly-dressed-as-adventurers&quot;&gt;Anthropomorphic Animals (Mostly dressed as adventurers)&lt;/h3&gt;

&lt;p&gt;One thing I struggled to get right with other models was anthropomorphic animals, especially if I also asked for medieval, steampunk or fantasy clothes. My dream of drawing a Mouseguard party with DALL-E would never come to fruition. With StableDiffusion one trick that worked for me was, instead of, say, prompting “ferret with pirate clothes/dressed as a pirate”, using the prompt “ferret wearing a pirate costume”.&lt;/p&gt;

&lt;p&gt;Then I also got a prompt from twitter and iterated it, which was “cute and adorable [animal], wearing [clothes], steampunk/clockpunk/fantasy…” plus style prompts.&lt;/p&gt;

&lt;p&gt;This one worked like a charm. Rather than telling you the prompt I used for each individual picture, I will just show you the ones I liked best, so you can see what possibilities exist by tweaking a prompt like that one (I guess you can deduce the animal, etc. from the images themselves).&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Example prompt: “Cute and adorable ferret wizard, wearing coat and suit, steampunk, lantern, anthromorphic, Jean paptiste monge, oil painting”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In general, removing Jean Paptiste Monge didn’t change results that much, switching coat and suit by tailcoat gave me the results I liked best, and adding portrait made all the paintings into humans. Also, the lantern keyword was copied from twitter but it didn’t really bring much to the table (And most of these animals aren’t carrying a lantern because of that).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;resources/ai-generated-images/stable-diffusion/selected/1080959312_Cute_and_adorable_mouse_adventurer__wearing_warm_jacket__survival_gear__steampunk__lantern__anthromorphic__Jean_paptiste_monge__oil_painting-yy.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; style=&quot;display:inline;height:30%; width:30%&quot; /&gt;&lt;img src=&quot;resources/ai-generated-images/stable-diffusion/selected/1087059005_A_cute_and_adorable_red_panda_looking_at_reflection_on_sparkling_lake_under_milky_way__digital_illustration__trending_in_artstation__4k__detailed__vivid_colors-yy.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; style=&quot;display:inline;height:30%; width:30%&quot; /&gt;&lt;img src=&quot;resources/ai-generated-images/stable-diffusion/selected/1152829999_Tiny_cute_and_adorable_mouse_adventurer__wearing_warm_jacket__survival_gear__steam_punk__lantern__anthromorphic__Jean_paptiste_monge-yy.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; style=&quot;display:inline;height:30%; width:30%&quot; /&gt;&lt;img src=&quot;resources/ai-generated-images/stable-diffusion/selected/1470289104_Cute_and_adorable_ferret_adventurer__wearing_warm_jacket__survival_gear__steam_punk__lantern__anthromorphic__Jean_paptiste_monge-yy.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; style=&quot;display:inline;height:30%; width:30%&quot; /&gt;&lt;img src=&quot;resources/ai-generated-images/stable-diffusion/selected/1710425125_Fierce_bear_adventurer__wearing_Victorian_tailcoat__steampunk__anthromorphic__Jean_paptiste_monge__oil_on_canvas_portrait-yy.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; style=&quot;display:inline;height:30%; width:30%&quot; /&gt;&lt;img src=&quot;resources/ai-generated-images/stable-diffusion/selected/1974449469_Cute_and_adorable_ferret_adventurer__wearing_Victorian_tailcoat__musket__steampunk__anthromorphic__Jean_paptiste_monge__oil_painting-yy.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; style=&quot;display:inline;height:30%; width:30%&quot; /&gt;&lt;img src=&quot;resources/ai-generated-images/stable-diffusion/selected/2439754937_A_ferret_wearing_a_pirate_costume__by_Josef_Thoma__matte_painting_trending_on_artstation_HQ-yy.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; style=&quot;display:inline;height:30%; width:30%&quot; /&gt;&lt;img src=&quot;resources/ai-generated-images/stable-diffusion/selected/2441072927_Cute_and_adorable_chinchilla_adventurer__wearing_Victorian_tailcoat__steampunk__anthromorphic__Jean_paptiste_monge__oil_on_canvas_portrait-yy.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; style=&quot;display:inline;height:30%; width:30%&quot; /&gt;&lt;img src=&quot;resources/ai-generated-images/stable-diffusion/selected/2567093150_Cute_and_adorable_red_panda_adventurer__wearing_warm_jacket__survival_gear__steampunk__lantern__anthromorphic__Jean_paptiste_monge__oil_painting-yy.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; style=&quot;display:inline;height:30%; width:30%&quot; /&gt;&lt;img src=&quot;resources/ai-generated-images/stable-diffusion/selected/2757110376_A_ferret_wearing_a_sherlock_holmes_costume__bokeh__4k__detailed__oil_on_canvas-yy.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; style=&quot;display:inline;height:30%; width:30%&quot; /&gt;&lt;img src=&quot;resources/ai-generated-images/stable-diffusion/selected/2886990977_Cute_and_adorable_ferret_adventurer__wearing_warm_jacket__survival_gear__steam_punk__lantern__anthromorphic__Jean_paptiste_monge-yy.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; style=&quot;display:inline;height:30%; width:30%&quot; /&gt;&lt;img src=&quot;resources/ai-generated-images/stable-diffusion/selected/2974914158_Cute_and_adorable_ferret_adventurer__wearing_Victorian_tailcoat__musket__steampunk__anthromorphic__Jean_paptiste_monge__oil_painting-yy.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; style=&quot;display:inline;height:30%; width:30%&quot; /&gt;&lt;img src=&quot;resources/ai-generated-images/stable-diffusion/selected/3135780918_Cute_and_adorable_squirrel_adventurer__wearing_Victorian_tailcoat__sword__steampunk__anthromorphic__Jean_paptiste_monge__oil_painting-yy.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; style=&quot;display:inline;height:30%; width:30%&quot; /&gt;&lt;img src=&quot;resources/ai-generated-images/stable-diffusion/selected/3431930152_Cute_and_adorable_red_panda_adventurer__wearing_Victorian_tailcoat__steampunk__anthromorphic__Jean_paptiste_monge__oil_painting-yy.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; style=&quot;display:inline;height:30%; width:30%&quot; /&gt;&lt;img src=&quot;resources/ai-generated-images/stable-diffusion/selected/3700760151_Cute_and_adorable_red_panda_adventurer__wearing_Victorian_tailcoat__steampunk__anthromorphic__Jean_paptiste_monge__oil_painting-yy.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; style=&quot;display:inline;height:30%; width:30%&quot; /&gt;&lt;img src=&quot;resources/ai-generated-images/stable-diffusion/selected/3705614363_Cute_and_adorable_mouse_adventurer__wearing_warm_jacket__survival_gear__steampunk__lantern__anthromorphic__Jean_paptiste_monge__oil_painting-yy.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; style=&quot;display:inline;height:30%; width:30%&quot; /&gt;&lt;img src=&quot;resources/ai-generated-images/stable-diffusion/selected/3819606400_Cute_and_adorable_ferret_wizard__wearing_coat_and_suit__steampunk__lantern__anthromorphic__Jean_paptiste_monge__oil_painting-yy.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; style=&quot;display:inline;height:30%; width:30%&quot; /&gt;&lt;img src=&quot;resources/ai-generated-images/stable-diffusion/selected/3979092551_Cute_and_adorable_red_panda_explorer__wearing_coat_and_suit__steampunk__anthromorphic__Jean_paptiste_monge__oil_painting-yy.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; style=&quot;display:inline;height:30%; width:30%&quot; /&gt;&lt;img src=&quot;resources/ai-generated-images/stable-diffusion/selected/4142452916_Cute_and_adorable_ferret_wizard__wearing_coat_and_suit__steampunk__lantern__anthromorphic__Jean_paptiste_monge__oil_painting-yy.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; style=&quot;display:inline;height:30%; width:30%&quot; /&gt;&lt;img src=&quot;resources/ai-generated-images/stable-diffusion/selected/849011389_A_ferret_wearing_a_pirate_costume__bokeh__4k__detailed__oil_on_canvas-yy.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; style=&quot;display:inline;height:30%; width:30%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I really went crazy with these, and these are my selection, so you can imagine how many I tried.&lt;/p&gt;

&lt;h3 id=&quot;forest-scenes&quot;&gt;Forest Scenes&lt;/h3&gt;

&lt;p&gt;Another thing that I love about StableDiffusion (Which DALL-E 2 also gets right) is how well it renders textures. I can imagine how a 3d artist may use one of these models to enhance their own 3d objects by creating many different textures fast and combining them with domain knowledge.&lt;/p&gt;

&lt;p&gt;Because of that, and my love for moss, I made many forest scenes with abundant growth and moss. Many of these look like a Magic: the Gathering illustration. I will not post the prompts as each one was different, but they mostly followed my digital fantasy illustration template.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;resources/ai-generated-images/stable-diffusion/selected/1394321457_A_giant_boar_made_of_moss_and_azaleas_in_the_woods__by_Josef_Thoma__matte_painting_trending_on_artstation_HQ__concept_art-xx.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;
&lt;img src=&quot;resources/ai-generated-images/stable-diffusion/selected/1537772634_A_Centaur_in_a_glade_under_the_stars_by_Zdzis_aw_Beksi_ski__Gorgeous_digital_painting_with_sober_colours_amazing_art_mesmerizing__captivating__artstation_3-centaur-xx.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;
&lt;img src=&quot;resources/ai-generated-images/stable-diffusion/selected/3401078383_A_mossy_fallen_tree_with_butterflies_and_azaleas_and_lavender__by_Josef_Thoma__matte_painting_trending_on_artstation_HQ__concept_art-xx.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;
&lt;img src=&quot;resources/ai-generated-images/stable-diffusion/selected/3650187890_A_giant_fire_bear_with_moss_and_azaleas_in_forest__fantasy__matte_painting_trending_on_artstation_HQ__concept_art-xx.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;
&lt;img src=&quot;resources/ai-generated-images/stable-diffusion/selected/2835514280_A_mossy_fallen_tree_with_butterflies_and_azaleas_and_lavender__by_Josef_Thoma__matte_painting_trending_on_artstation_HQ__concept_art-add.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;
&lt;img src=&quot;resources/ai-generated-images/stable-diffusion/selected/738606398_A_magnificent_elk_made_of_azaleas_and_moss_in_the_woods__fantasy__matte_painting_trending_on_artstation_HQ__concept_art-xx.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;
&lt;img src=&quot;resources/ai-generated-images/stable-diffusion/selected/975594345_A_giant_boar_made_of_moss_and_azaleas_in_the_woods__by_Josef_Thoma__matte_painting_trending_on_artstation_HQ__concept_art-xx.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;StableDiffusion was the first AI art model where I have successfully got a centaur. Not a deformed monstrosity, not a horse, not a weird human. A real centaur! So that made me happy and I had to share it.&lt;/p&gt;

&lt;p&gt;I honestly made a lot more illustrations I loved (like 400 total, I think?) but I guess most readers will get bored long before they finish scrolling this post, so I will not keep you any longer.&lt;/p&gt;

&lt;p&gt;These are the last ones, I swear.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;resources/ai-generated-images/stable-diffusion/selected/1621667580_A_digital_illustration_of_a_steampunk_inventor_s_desk__4k__detailed__trending_in_artstation__fantasy_vivid_colors-add.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; style=&quot;display:inline;height:40%; width:40%&quot; /&gt;&lt;img src=&quot;resources/ai-generated-images/stable-diffusion/selected/3418018944_A_treasure_chest_with_glowing_jeewels_and_gems__by_Josef_Thoma__matte_painting_trending_on_artstation_HQ-add.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; style=&quot;display:inline;height:40%; width:40%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Before we reach the end, I want to raise a concern and propose a challenge. No matter what I tried, I could &lt;em&gt;not&lt;/em&gt; make either DALL-E 2, or StableDiffusion make characters in the style of Jojo’s Bizarre Adventure (or Araki, in general). I tried the obvious style cues and others, and none worked. So if any of you manages to make one of these models draw Spongebob Squarepants in the style of Jojo’s, or any other recognizable character, you will get a thousand internet points from me.&lt;/p&gt;

&lt;h3 id=&quot;appendix-a-stable-diffusion-prompt-guide&quot;&gt;Appendix A: Stable Diffusion Prompt Guide&lt;/h3&gt;
&lt;p&gt;In general, a the best stable diffusion prompts will have this form:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“A &lt;em&gt;[type of picture]&lt;/em&gt; of a &lt;em&gt;[main subject, mostly composed of adjectives and nouns -avoid verbs-]&lt;/em&gt;, &lt;em&gt;[style cues]*&lt;/em&gt;”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Some &lt;em&gt;types of picture&lt;/em&gt; include &lt;em&gt;digital illustration, oil painting (usually good results), matte painting, 3d render, medieval map&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;The &lt;em&gt;main subject&lt;/em&gt; can be anything you’re thinking of, but StableDiffusion still struggles with compositionality, so it shouldn’t be more than one or two main things (say, &lt;em&gt;a beaver wearing a suit&lt;/em&gt;, or &lt;em&gt;a cat samurai with a pet pug&lt;/em&gt;).&lt;/p&gt;

&lt;p&gt;Style cues can be anything you want to condition the image on. I wouldn’t add too many, maybe only 1 to 3. These can really vary a lot but some good ones are: &lt;em&gt;concept art, steampunk, trending in artstation, good composition, hyper realistic, oil on canvas, vivid colors&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Additionally, adding the name of an artist as a cue will make the picture look like something that artist made, though it may condition the image’s contents, especially if that artist had narrow themes (Beatrix Potter gets you spurious rabbits, for instance).&lt;/p&gt;

&lt;p&gt;For a detailed guide to crafting the best stable diffusion prompts, see &lt;a href=&quot;https://docs.google.com/document/d/17VPu3U2qXthOpt2zWczFvf-AH6z37hxUbvEe1rJTsEc/edit&quot;&gt;A Guide to Writing Prompts for Text-to-image AI&lt;/a&gt; if you feel like you want to read more.&lt;/p&gt;

&lt;p&gt;You can also find many great prompts if you use the &lt;a href=&quot;https://lexica.art/&quot;&gt;Lexica.art&lt;/a&gt; prompt search engine to find images you like and make tweaks to their prompts.&lt;/p&gt;

&lt;h2 id=&quot;appendix-b-resources-and-links&quot;&gt;Appendix B: Resources and Links&lt;/h2&gt;

&lt;p&gt;Given how much has happened lately (even John Oliver is talking about DALL-E!), here are some other articles you may want to read.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://thealgorithmicbridge.substack.com/p/stable-diffusion-is-the-most-important&quot;&gt;Stable Diffusion: The Most Important AI Art Model Ever&lt;/a&gt; covering the more social/economic side of this.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://sweet-hall-e72.notion.site/A-Traveler-s-Guide-to-the-Latent-Space-85efba7e5e6a40e5bd3cae980f30235f&quot;&gt;A traveler’s guide to the latent space&lt;/a&gt;: a guide on prompt engineering that goes &lt;em&gt;really&lt;/em&gt; in depth. I haven’t actually read the whole thing.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://docs.google.com/document/d/17VPu3U2qXthOpt2zWczFvf-AH6z37hxUbvEe1rJTsEc/&quot;&gt;A guide to Writing Prompts for Text-to-Image AI&lt;/a&gt;: The best quick primer I’ve found on prompt engineering and writing prompts for DALL-E 2/StableDiffusion or any other text-to-image AI.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/art-prompts&quot;&gt;Art Prompts: My Experiments with Mini DALL-E&lt;/a&gt;: My first post on text-to-image AI, where I included my own AI art prompt guide. Here you can see how far we’ve come and how fast.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/DALL-E-2-prompt-guide&quot;&gt;DALL-E 2 Experiments&lt;/a&gt;: The post I wrote two weeks ago when DALL-E 2 beta release was news and StableDiffusion hadn’t come out yet. See if you can spot the same prompts’ different results.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://andys.page/posts/how-to-draw/&quot;&gt;How to Draw&lt;/a&gt;: Where a user uses StableDiffusion’s img2img version to convert an MSPaint drawing into a realistic sci-fi image.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://replicate.com/stability-ai/stable-diffusion&quot;&gt;&lt;strong&gt;Image2Image StableDiffusion&lt;/strong&gt;&lt;/a&gt;, available on Replicate for free. You can draw a rough sketch of what you want in jspaint (the browser copy of MSPaint), then upload it to Stable Diffusion img2img and use that as a starting point for your AI art. See also &lt;a href=&quot;https://simonwillison.net/2022/Aug/29/stable-diffusion/&quot;&gt;Stable Diffusion is a really big deal, Simon Willison&lt;/a&gt;: This came out a little after I wrote this post, when Stability.ai released the img2img StableDiffusion model. It is amazing! You can make a sketch in MSPaint (Or JsPaint) and make the AI turn it into a painting or illustration in the style you want.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://keras.io/guides/keras_cv/generate_images_with_stable_diffusion/&quot;&gt;&lt;strong&gt;High-performance image generation using Stable Diffusion in KerasCV&lt;/strong&gt;&lt;/a&gt;: Stable Diffusion was finally ported to keras. It runs smoothly both on GPU or CPU if you have keras installed, and this is the version I’ve been using to make AI art in my local computer.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://lexica.art/&quot;&gt;&lt;strong&gt;Lexica.art&lt;/strong&gt;&lt;/a&gt; a great search engine for prompts and artworks. It is an amazing resource to find good images and see which prompts generated them, which you can then copy and tune to your needs. An easy way to build on the best stable diffusion prompts other people has already found.&lt;/li&gt;
  &lt;li&gt;If you like anime, &lt;a href=&quot;https://huggingface.co/hakurei/waifu-diffusion&quot;&gt;&lt;strong&gt;Waifu Diffusion&lt;/strong&gt;&lt;/a&gt; is a text-to-image diffusion model that was conditioned on high-quality anime images through fine-tuning, using Stable Diffusion as a starting point. It generates anime illustrations and it’s awesome.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://jalammar.github.io/illustrated-stable-diffusion/&quot;&gt;The Illustrated Stable Diffusion&lt;/a&gt; explains how Stable Diffusion works, step by step and through different levels of abstraction, and has great illustrations.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;If you liked this article, please share it with someone you think will like reading it too. I wrote this for you guys.&lt;/em&gt;&lt;/p&gt;
</content>
		</entry>
	
		<entry>
			<title>How to Set Up a Personal Wiki (with Jekyll)</title>
			<link href="http://strikingloo.github.io/personal-wiki-set-up"/>
			<updated>2022-08-24T00:00:00-03:00</updated>
			<id>http://strikingloo.github.io/personal-wiki-set-up</id>
			<content type="html">&lt;p&gt;I have been using a personal wiki for note-taking and studying for 2 years. In that time, it has helped me a lot when preparing exams or job interviews, and generally made studying from books or papers feel more productive and enjoyable.&lt;/p&gt;

&lt;p&gt;A part of that is just emotional: it feels good to take notes and capture what you learn, knowing you will be able to find it again later. Being more deliberate about what you will remember in the long term, and what resources you have close to hand when writing an essay or programming, frees up a lot of cognitive load from day-to-day studying or processing, and lets you focus on the tasks themselves.&lt;/p&gt;

&lt;p&gt;Reviewing a whole topic from a set of notes instead of having to reopen an old book (or books, plural) is also a liberating feature.&lt;/p&gt;

&lt;p&gt;After I wrote my previous article on &lt;a href=&quot;/reflections-digital-gardening&quot;&gt;how I use my Personal Wiki for studying&lt;/a&gt; and what advantages it has afforded me, a few people told me it had made them curious about trying one out for themselves.&lt;/p&gt;

&lt;p&gt;I am going out on a limb here and assuming some of my online readers also felt that way, so I am writing this guide for you.&lt;/p&gt;

&lt;p&gt;Want to try your hands at a personal wiki but don’t know how to start? Follow along.&lt;/p&gt;

&lt;p&gt;You can read this on your mobile phone and come back later if you want to try it out for yourself, but I think the most fun way of consuming this article will be from a computer, following along step by step and in the end having your very own personal site and wiki.&lt;/p&gt;

&lt;p&gt;In this post, I will show you&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;How to fork my personal site / wiki template, to create your own.&lt;/li&gt;
  &lt;li&gt;How to customize it so it fits your own profile.&lt;/li&gt;
  &lt;li&gt;How you can start capturing insights and facts from the content you consume, editing them and linking them so you end up with your own garden of networked thought.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The project you end up with will be a personal site in its own right, which you can host for free on &lt;a href=&quot;https://pages.github.com/&quot;&gt;Github Pages&lt;/a&gt; if you are so inclined. Then you can even use your wiki as a way to showcase your skills and a part of what you’ve learned and read to future employers. Plus it’s a great sign of status, and I’m sure it will woo your potential significant others &lt;em&gt;[citation needed]&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Before we start, you will need: a Github account, 20 minutes (I estimate, but tell me how long it took for you.) and a text editor.&lt;/p&gt;

&lt;p&gt;What we are about to do is: fork a personal wiki template repository, using jekyll to make a static site where each wiki article is a page, and host it on Github pages. Don’t worry if that didn’t make sense, you’ll understand by the end.&lt;/p&gt;

&lt;h2 id=&quot;step-1-forking-the-github-project&quot;&gt;Step 1: Forking the Github Project&lt;/h2&gt;

&lt;p&gt;I built a template for a &lt;a href=&quot;https://github.com/StrikingLoo/Personal-Wiki-Site-Setup&quot;&gt;personal site with a Digital Garden&lt;/a&gt; by stripping away all the content from this site and changing the design.&lt;/p&gt;

&lt;p&gt;You can fork it in Github, which allows you to basically copy paste all of my code and then edit on top of it. You are free to edit away any part of it or just leave it all as-is and start using the wiki right away.&lt;/p&gt;

&lt;p&gt;To fork, simply go to the project’s URL and click on &lt;em&gt;fork&lt;/em&gt; if you have a GitHub account, or create one first. This will create your own copy of the project from which you can begin working.&lt;/p&gt;

&lt;p&gt;To edit your own copy go to its URL (you can also find it in your profile) and &lt;em&gt;clone&lt;/em&gt; the project to your computer. Alternatively you could edit each file from the browser directly through Github’s UI, though you will probably be less happy doing it that way.&lt;/p&gt;

&lt;p&gt;This is what it will look like in the beginning:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;resources/post_image/pw_screen1.png&quot; alt=&quot;screenshot of personal wiki&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It’s a stripped out version of this same website, except I changed the palette to something greener (because it’s supposed to be a garden, get it?). I made it responsive so it looks well on mobile.&lt;/p&gt;

&lt;p&gt;All of the style and design is packed into a file called &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;css/main.css&lt;/code&gt;, which you are free to edit and customize as you prefer, until it suits your aesthetic sensibilities. Right now it has a simple design, but you can do whatever you want to it like changing the font or color palette, background design, etc.&lt;/p&gt;

&lt;p&gt;Here is some inspiration for &lt;a href=&quot;https://jekyllthemes.io/free&quot;&gt;web design templates&lt;/a&gt;, and &lt;a href=&quot;https://colorhunt.co/&quot;&gt;color palettes&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;step-2-making-the-site-your-own&quot;&gt;Step 2: Making the Site Your Own&lt;/h2&gt;

&lt;p&gt;There are a few parts of this template you will need to edit to add in your personal touch.&lt;/p&gt;

&lt;p&gt;In order, these are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;index.html&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;about/index.md&lt;/code&gt;, which contain the Homepage and About page respectively. In index.html you could write in a little bit about you in the profile, and replace &lt;em&gt;Username&lt;/em&gt; with your handle. The About page is typically where you will make a more detailed text about who you are, what you are working on, etc. Adding more sections to the site is as easy as adding more folders with an &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;index.md&lt;/code&gt; file, which you can just copy from &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;about/index.md&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;The file &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;_config.yml&lt;/code&gt; has a few configuration fields: one for your twitter username and another for your google analytics ID if you want to use GA to count your site’s visitors. If you don’t want to use Google Analytics, leave it blank.&lt;/li&gt;
  &lt;li&gt;This is probably the most involved step. Go to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;_layouts/default.html&lt;/code&gt;. The footer has a few links to social media with their corresponding FontAwesome icons. You can either replace mine with yours, like putting in your own LinkedIn etc., or delete the line to remove that link from your site. You can obviously add whatever other social media you use, or links to other sites you have, etc. On that same file, in the header, you could also add links to any new sections you create down the line, like a &lt;em&gt;Now&lt;/em&gt; page or a link to a portfolio with images.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Finally in that footer is a little credit to the template’s humble creator, me. There is nothing stopping you from deleting that, but it would make me smile if you leave it there, especially if I am one day browsing someone’s site and see it in the credits.&lt;/p&gt;

&lt;p&gt;Also, if you make this site public and ever share it on Twitter, whatever image you put on &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;resources/placeholder.png&lt;/code&gt; will be your link’s image on twitter, but each individual markdown has a &lt;em&gt;twitter_image&lt;/em&gt; property that overrides that path with any URL you write into it.&lt;/p&gt;

&lt;h2 id=&quot;step-3-using-the-wiki&quot;&gt;Step 3: Using the Wiki&lt;/h2&gt;

&lt;p&gt;Now this is where it gets fun.&lt;/p&gt;

&lt;p&gt;Under the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;wiki&lt;/code&gt; folder, you will find a default, template article. You’ll notice it is on markdown, an extension of plaintext that will let you format files nicely without having to learn how to code. You can do headings, subheadings, lists and links quite easily, and this example file has a little bit of each piece of syntax so you don’t have to learn from scratch. You can visit &lt;a href=&quot;https://www.markdownguide.org/basic-syntax/&quot;&gt;this Markdown guide&lt;/a&gt; to learn more about the syntax, but there isn’t that much more you are going to need.&lt;/p&gt;

&lt;p&gt;Whenever you want to add a new article to your wiki, all you have to do is create a new &lt;em&gt;.md&lt;/em&gt; file in that folder, and give it a title and description like in the example file (between ‘—’s). You can also add tags, separated by commas, which allow for searching (in the wiki’s searchbox, or using a link to &lt;em&gt;/tagged/?q=keyword&lt;/em&gt;) and an abstract that will appear in the beginning of the article in a different colored box.&lt;/p&gt;

&lt;p&gt;The &lt;em&gt;importance&lt;/em&gt; and &lt;em&gt;date&lt;/em&gt; fields serve mostly as metadata: the former ranks articles in the &lt;em&gt;/wiki&lt;/em&gt; base page, the latter just lets you know when you created an article first.&lt;/p&gt;

&lt;p&gt;If you want examples of how to use a wiki in general, I will direct you to my previous &lt;a href=&quot;/reflections-digital-gardening&quot;&gt;article on my note-taking workflow and Digital Gardens&lt;/a&gt;, or my &lt;a href=&quot;wiki/archiving&quot;&gt;wiki article on note-taking&lt;/a&gt;, but a short summary would be: when you come up with an idea that you like, or read something that surprises you or you think you will want to use or review later, add it to your wiki. Ideally use your own words, but there is no shame in copying and pasting (though future-you will probably be happier if you link or cite your source). Finally, link the article to other articles in your wiki that you think will be relevant or related, so in future traversals you will see how it fits in the bigger picture (this step is optional and each person manages it in their own way).&lt;/p&gt;

&lt;p&gt;To read more about this, consider perusing all the resources I cite at the bottom of my previous links. There is a whole philosophy to planting notes in a digital garden, along with the idea of maintaining them so they are ‘evergreen’ (as opposed to a blog post which is often a temporary thing that loses relevance over time).&lt;/p&gt;

&lt;p&gt;Additionally, this personal site comes with support for a blog, in case you want to start writing one. Again each post will be a markdown file, with the only condition that their filenames should start with the date you wrote them in. If you are on the fence about starting a blog, you should try it, it can be fun.&lt;/p&gt;

&lt;p&gt;Many people have written about &lt;a href=&quot;https://guzey.com/personal/why-have-a-blog/&quot;&gt;why you should start blogging&lt;/a&gt;. I don’t particularly endorse the idea that &lt;a href=&quot;https://tomcritchlow.com/2015/09/03/nycblogclub/&quot;&gt;everyone should blog&lt;/a&gt;, but I do think everyone should try it for a while. If you like it and it helps you, keep at it. If you feel you are wasting your time, quit it and nobody will be harmed.&lt;/p&gt;

&lt;h2 id=&quot;step-4-optionally-host-it-on-github-pages&quot;&gt;Step 4: Optionally, Host it on Github Pages.&lt;/h2&gt;

&lt;p&gt;If you want to make your wiki public, you can host it for free in many places like &lt;a href=&quot;https://pages.github.com/&quot;&gt;Github Pages&lt;/a&gt; or &lt;a href=&quot;https://www.netlify.com/blog/2016/09/29/a-step-by-step-guide-deploying-on-netlify/&quot;&gt;Netlify&lt;/a&gt;. I think Github has the friendliest workflow for this, but they are both free providers and you can customize the subdomain in either of them, so I don’t have a big preference for one of them. You can also buy your own domain and host it there, if you want to and can afford it.&lt;/p&gt;

&lt;p&gt;Here is a &lt;a href=&quot;https://idratherbewriting.com/documentation-theme-jekyll/mydoc_publishing_github_pages.html&quot;&gt;tutorial on how to host a jekyll site like this on github pages&lt;/a&gt;, though the whole process is quite intuitive.&lt;/p&gt;

&lt;p&gt;Additionally, if you &lt;em&gt;do&lt;/em&gt; decide to host your wiki online, please let me know. You can send me the link on HackerNews, reddit or twitter, or do a Pull Request, and I will link to it in the project’s README so other people can see how many beautiful personal wikis were added to the internet.&lt;/p&gt;

&lt;p&gt;Actually this whole thing is my evil plan because I like browsing personal wikis in my free time and I want to add new ones to the web so that I can keep finding new places to procrastinate in.&lt;/p&gt;

&lt;h2 id=&quot;conclusions&quot;&gt;Conclusions&lt;/h2&gt;

&lt;p&gt;I think many people can benefit from having their own place in the web, and their own system for note taking. Being able to take notes in public and link them, leveraging the web’s capability for connecting thoughts, can make you a more effective writer and, indirectly, thinker.&lt;/p&gt;

&lt;p&gt;More directly, it has helped me a lot with studying and it may do the same for you. So if you were on the fence on creating a Digital Garden or personal wiki and didn’t know where to start, I hope this tutorial will have given you the push you needed.&lt;/p&gt;

&lt;p&gt;If there are any steps that are unclear to you or you couldn’t make the site or hosting work, don’t be afraid to reach out to me. I may be a bit slow to respond, but I will eventually help you if I can.&lt;/p&gt;

&lt;p&gt;And if you found this post entertaining or useful, share it on Twitter. Let’s pave the web with wikis everywhere.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Follow me on &lt;a href=&quot;https://twitter.com/strikingloo&quot;&gt;Twitter&lt;/a&gt; to know when I write a new article.&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;further-reading&quot;&gt;Further Reading&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://nesslabs.com/tiddlywiki-beginner-tutorial&quot;&gt;TiddlyWiki Beginner Tutorial&lt;/a&gt;: If you don’t want to have a whole personal website and don’t feel that the level of customization afforded by a static website beats the ease of using a tool, I recommend you try TiddlyWiki instead. It is pretty plug and play and will give you most of the basic features you want for a personal wiki. As inspiration, this is one &lt;a href=&quot;https://wiki.waifu.haus/#Root&quot;&gt;Digital Garden made with TiddlyWiki&lt;/a&gt; that I find fascinating.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://sadgrl.online/webmastery/layouts/&quot;&gt;SadGrl’s Layout Directory&lt;/a&gt; is a great resource to get new HTML templates if you want to change the layout of some of the pages (or add new ones) after forking. These designs are great, and they have that old web, indie feel I like so much, like you would see in a blogspot from 2008.&lt;/li&gt;
&lt;/ul&gt;
</content>
		</entry>
	
		<entry>
			<title>DALL-E 2 Art: Experiments with Prompts or How I Got My New Wallpaper</title>
			<link href="http://strikingloo.github.io/DALL-E-2-prompt-guide"/>
			<updated>2022-08-16T00:00:00-03:00</updated>
			<id>http://strikingloo.github.io/DALL-E-2-prompt-guide</id>
			<content type="html">&lt;p&gt;OpenAI’s &lt;a href=&quot;https://openai.com/blog/dall-e-now-available-in-beta/&quot;&gt;Dall-E 2 became available in beta&lt;/a&gt; a month ago, and as a big fan of generative text-to-image models I instantly joined the waitlist.&lt;/p&gt;

&lt;p&gt;After a little less than a month, the email arrived and I was notified I was finally allowed to use DALL-E 2! OpenAI gave me 50 free credits, with a monthly refill of 15 more from September onwards.&lt;/p&gt;

&lt;p&gt;I won’t go into how text-to-image models work here, and for how to write prompts for DALL-E I recommend you see my &lt;a href=&quot;/art-prompts#how-to-write-prompts-for-dall-e--craiyon-a-crash-course&quot;&gt;prompt guide for AI art&lt;/a&gt; (or my &lt;a href=&quot;/stable-diffusion-vs-dalle-2&quot;&gt;StableDiffusion prompt guide&lt;/a&gt;, where I also keep adding new ‘tried and tested’ prompts as I discover them.&lt;/p&gt;

&lt;p&gt;I’ve tried multiple different prompts, style cues, etc., and I’ll keep adding the ones that generate the coolest images there.&lt;/p&gt;

&lt;p&gt;To anyone on the fence about trying DALL-E 2, I can genuinely recommend it. Prompting the model and seeing what it comes up with is almost addictive, and creates this beautiful dopamine feedback loop that just keeps you glued to your keyboard until you’ve spent all your credits. Or at least that’s how it went for me.&lt;/p&gt;

&lt;p&gt;Personally, I like asking these models for fantasy illustrations, landscapes and that kind of stuff, as I find that area less explored than, say, hyper realistic renders or popular media characters. However given &lt;strong&gt;how good DALL-E 2 is at photorealism&lt;/strong&gt;, I did try my hand at some photograph prompts.&lt;/p&gt;

&lt;p&gt;As a summary, here are some of the &lt;strong&gt;prompt templates that have yielded the best results&lt;/strong&gt; for me:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Pixar style 3D render of X&lt;/li&gt;
  &lt;li&gt;A photograph of X, 4k, detailed, [vivid colors]&lt;/li&gt;
  &lt;li&gt;Low-poly render of X; high resolution, 4k&lt;/li&gt;
  &lt;li&gt;A digital illustration of X, 4k, detailed, trending in artstation, [fantasy vivid colors/painter name for style/ghibli/cottagecore/steampunk]&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Generally, remember to describe the thing you want as specifically as you can. For instance if you want a red panda knight, instead of red panda knight you may want to write “A digital illustration of a red panda wearing a knight armor, with sword”. Otherwise in the worst case scenario, you may get a red panda behind a helmet that covers its furry face and nobody will know its secret!&lt;/p&gt;

&lt;p&gt;The first thing that surprised me with this new model is how, unlike older ones like DALL-E mini, it generally understands prepositions and simple scene composition.&lt;/p&gt;

&lt;p&gt;The hat goes in the head, these are two characters and each has its own adjectives, blue sphere over green cube, etc.&lt;/p&gt;

&lt;p&gt;Because of that, many of my first experiments were about composing scenes with several modifiers and seeing how the model handled each piece of the input. I also played around a lot with style cues, usually keeping the ones that yielded the most endearing pictures.&lt;/p&gt;

&lt;p&gt;One thing I do is trying many prompts out on Craiyon or other free models first, and then sending the ones that perform better (to my taste) to DALL-E 2, as I had a limited budget in comparison. I expect this will be less necessary as models get cheaper and better &lt;em&gt;[Update: this prediction realized a week later after StableDiffusion’s release]&lt;/em&gt;. In fact DALL-E 2 gave me satisfying images for all prompts except one &lt;strong&gt;on the first try&lt;/strong&gt;, though they not always fit the description well if it is complex enough (more on that later). This was &lt;strong&gt;not possible 6 months ago&lt;/strong&gt; and I was not expecting it to happen so quick.&lt;/p&gt;

&lt;p&gt;Given how much better DALL-E 2 is, you won’t need to do that much prompt engineering as in previous models, so this is why my guide is a lot simpler this time around.&lt;/p&gt;

&lt;p&gt;With that said, in general I found DALL-E 2 to be eerily effective, a lot better than I would have predicted a neural network to be even a year ago, and I really can’t imagine what we will have in 2 or 3 years, available for free download and consumer use.&lt;/p&gt;

&lt;p&gt;I hear stable diffusion, which is roughly equivalent in performance to DALL-E 2, already runs on 5GB of VRAM, and even though that is still above my budget (I think my GPU has 4GB), I find it extremely impressive.&lt;/p&gt;

&lt;p&gt;However, I won’t try to predict the capabilities or effects future models will have, I’ll leave that to more experienced people on twitter.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;[Edit: I wrote a separate &lt;a href=&quot;/stable-diffusion-vs-dalle-2&quot;&gt;post about StableDiffusion&lt;/a&gt;, how it compares against DALL-E 2 and my rough reflections and predictions for this space. It also contains most of the interesting resources and links I found about AI art.]&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Without further ado, here are some cool images I drew with DALL-E 2! I hope if you needed some inspiration for prompts, you will be able to copy the ones you like the most and tune them to your own needs.&lt;/p&gt;

&lt;h2 id=&quot;dall-e-2-experiments-3d-animation&quot;&gt;DALL-E 2 Experiments: 3D Animation&lt;/h2&gt;

&lt;p&gt;For my first set of prompts, I tried doing some 3D renders. I had seen somewhere that DALL-E 2 was capable of responding well to the prompt “Pixar style 3D render of…” and it really did not disappoint me.&lt;/p&gt;

&lt;p&gt;I also tried a low-poly prompt, but it didn’t have a satisfying result so I shelved those for next time.&lt;/p&gt;

&lt;p&gt;Note that all images are actually in high resolution, I am just choosing to render them small so they fit in the screen better. If you like one, you can open it in a new tab to see its full, high resolution glory.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“Pixar style 3D render of a baby hippo, 4k, high resolution, trending in artstation”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;resources/ai-generated-images/dalle_2/DALL_E_2022-08-16_17.49.06_-_Pixar_style_3D_render_of_a_baby_hippo,_4k,_high_resolution,_trending_in_artstation.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt; &lt;img src=&quot;resources/ai-generated-images/dalle_2/DALL_E_2022-08-16_17.49.35_-_Pixar_style_3D_render_of_a_baby_hippo,_4k,_high_resolution,_trending_in_artstation.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt; 
&lt;img src=&quot;resources/ai-generated-images/dalle_2/DALL_E_2022-08-16_17.49.39_-_Pixar_style_3D_render_of_a_baby_hippo,_4k,_high_resolution,_trending_in_artstation.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt; &lt;img src=&quot;resources/ai-generated-images/dalle_2/DALL_E_2022-08-16_17.49.41_-_Pixar_style_3D_render_of_a_baby_hippo,_4k,_high_resolution,_trending_in_artstation.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Right away, I was blown away by the quality of the render, and the picture’s resolution. I was not expecting, nor was I used to my first prompt already working so well. It’s like suddenly I didn’t have to make an effort!&lt;/p&gt;

&lt;p&gt;So I jumped to compositions right away.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“Pixar style 3D render of a fox wearing a tophat, 4k, high resolution, trending in artstation”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;resources/ai-generated-images/dalle_2/DALL_E_2022-08-16_17.49.44_-_Pixar_style_3D_render_of_a_fox_wearing_a_tophat,_4k,_high_resolution,_trending_in_artstation.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt; &lt;img src=&quot;resources/ai-generated-images/dalle_2/DALL_E_2022-08-16_17.49.49_-_Pixar_style_3D_render_of_a_fox_wearing_a_tophat,_4k,_high_resolution,_trending_in_artstation.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;
&lt;img src=&quot;resources/ai-generated-images/dalle_2/DALL_E_2022-08-16_17.49.51_-_Pixar_style_3D_render_of_a_fox_wearing_a_tophat,_4k,_high_resolution,_trending_in_artstation.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt; &lt;img src=&quot;resources/ai-generated-images/dalle_2/DALL_E_2022-08-16_18.24.46_-_Pixar_style_3D_render_of_a_fox_wearing_a_tophat,_4k,_high_resolution,_trending_in_artstation.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;wallpaper-material-scenes-and-landscapes&quot;&gt;Wallpaper Material: Scenes and Landscapes&lt;/h2&gt;

&lt;p&gt;As I said before, I’ve previously found that landscapes and static scenes are usually better rendered than characters performing actions, so I tend to prompt for that a lot.&lt;/p&gt;

&lt;p&gt;With DALL-E 2, characters usually turned out fine too (as in the examples before) but I still have this bias towards nice paintings of landscapes, cities and so on. Something that would belong on &lt;a href=&quot;http://www.reddit.com/r/ImaginaryLandscapes&quot;&gt;r/FatasyLandscapes&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Here are some of the most beaufitul images I got using these sorts of prompts, and the prompts that made them. Most of them are even wallpaper material!&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“An oil painting of a mechanical clockwork flying machine from the renaissance, Gorgeous digital painting, amazing art, artstation 3, realistic”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;resources/ai-generated-images/dalle_2/DALL_E_2022-08-16_17.50.46_-_An_oil_painting_of_a_mechanical_clockwork_flying_machine_from_the_renaissance,_Gorgeous_digital_painting,_amazing_art,_artstation_3,_realistic.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt; &lt;img src=&quot;resources/ai-generated-images/dalle_2/DALL_E_2022-08-16_17.50.50_-_An_oil_painting_of_a_mechanical_clockwork_flying_machine_from_the_renaissance,_Gorgeous_digital_painting,_amazing_art,_artstation_3,_realistic.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;
&lt;img src=&quot;resources/ai-generated-images/dalle_2/DALL_E_2022-08-16_17.50.53_-_An_oil_painting_of_a_mechanical_clockwork_flying_machine_from_the_renaissance,_Gorgeous_digital_painting,_amazing_art,_artstation_3,_realistic.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt; &lt;img src=&quot;resources/ai-generated-images/dalle_2/DALL_E_2022-08-16_17.50.58_-_An_oil_painting_of_a_mechanical_clockwork_flying_machine_from_the_renaissance,_Gorgeous_digital_painting,_amazing_art,_artstation_3,_realistic.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I reproduced this prompt almost verbatim from one I tried on eleutherAI’s .imagine model (and also on craiyon/DALL-E mini). We’ve come a long way since then!&lt;/p&gt;

&lt;p&gt;Here is what that prompt yielded on the older models.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/resources/ai-generated-images/image-2.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I really think a good metric for one of these results is “would I want to have this as a wallpaper?” or even “would I hang this on a wall?” and for this result, I am happy answering yes.&lt;/p&gt;

&lt;p&gt;I tried a few other scene descriptions. Both of them had given me pretty neat images in the older models, so I wanted to see how much they improved on DALL-E 2. Once again, I was not disappointed.&lt;/p&gt;

&lt;p&gt;First, I tried drawing some interior scenes.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“fantasy tavern interior | Breath-taking digital painting with warm colours amazing art mesmerizing, captivating, artstation 3”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;resources/ai-generated-images/dalle_2/DALL_E_2022-08-16_17.51.22_-_fantasy_tavern_interior___Breath-taking_digital_painting_with_warm_colours_amazing_art_mesmerizing,_captivating,_artstation_3.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt; &lt;img src=&quot;resources/ai-generated-images/dalle_2/DALL_E_2022-08-16_17.51.25_-_fantasy_tavern_interior___Breath-taking_digital_painting_with_warm_colours_amazing_art_mesmerizing,_captivating,_artstation_3.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;
&lt;img src=&quot;resources/ai-generated-images/dalle_2/DALL_E_2022-08-16_17.51.28_-_fantasy_tavern_interior___Breath-taking_digital_painting_with_warm_colours_amazing_art_mesmerizing,_captivating,_artstation_3.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt; &lt;img src=&quot;resources/ai-generated-images/dalle_2/DALL_E_2022-08-16_17.51.31_-_fantasy_tavern_interior___Breath-taking_digital_painting_with_warm_colours_amazing_art_mesmerizing,_captivating,_artstation_3.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I’m happy it got the tables and rafters right, it looks amazing!&lt;/p&gt;

&lt;p&gt;And then a classic, our cluttered ghibli style cabin in the woods.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“Cluttered house in the woods | anime oil painting high resolution cottagecore ghibli inspired 4k”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;resources/ai-generated-images/dalle_2/DALL_E_2022-08-16_17.51.42_-_Cluttered_house_in_the_woods___anime_oil_painting_high_resolution_cottagecore_ghibli_inspired_4k.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt; &lt;img src=&quot;resources/ai-generated-images/dalle_2/DALL_E_2022-08-16_17.51.39_-_Cluttered_house_in_the_woods___anime_oil_painting_high_resolution_cottagecore_ghibli_inspired_4k.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;
&lt;img src=&quot;resources/ai-generated-images/dalle_2/DALL_E_2022-08-16_17.51.37_-_Cluttered_house_in_the_woods___anime_oil_painting_high_resolution_cottagecore_ghibli_inspired_4k.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt; &lt;img src=&quot;resources/ai-generated-images/dalle_2/DALL_E_2022-08-16_17.51.36_-_Cluttered_house_in_the_woods___anime_oil_painting_high_resolution_cottagecore_ghibli_inspired_4k.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;As before, I can’t help but find a big improvement versus the old ones.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/resources/ai-generated-images/ghibli-house-craiyon.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“German palace gardens and towers | oil painting, trending in artstation, anime”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;resources/ai-generated-images/dalle_2/german1.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt; &lt;img src=&quot;resources/ai-generated-images/dalle_2/german2.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“The green idyllic Arcadian prairie with sheep by Thomas Cole, Breath-taking digital painting with placid colours, amazing art, artstation 3, cottagecore”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;resources/ai-generated-images/dalle_2/DALL_E_2022-08-16_18.45.50_-_The_green_idyllic_Arcadian_prairie_with_sheep_by_Thomas_Cole,_Breath-taking_digital_painting_with_placid_colours,_amazing_art,_artstation_3,_cottageco.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt; &lt;img src=&quot;resources/ai-generated-images/dalle_2/DALL_E_2022-08-16_18.45.39_-_The_green_idyllic_Arcadian_prairie_with_sheep_by_Thomas_Cole,_Breath-taking_digital_painting_with_placid_colours,_amazing_art,_artstation_3,_cottageco.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Loving what it did with the sky on the first one. It’s a different style from the second one, too.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“A digital illustration of glowing toadstools beside a pond with lilypads, 4k, detailed, trending in artstation”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;resources/ai-generated-images/dalle_2/DALL_E_2022-08-16_18.46.01_-_A_digital_illustration_of_glowing_toadstools_beside_a_pond_with_lilypads,_4k,_detailed,_trending_in_artstation.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt; &lt;img src=&quot;resources/ai-generated-images/dalle_2/DALL_E_2022-08-16_18.46.03_-_A_digital_illustration_of_glowing_toadstools_beside_a_pond_with_lilypads,_4k,_detailed,_trending_in_artstation.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;
&lt;img src=&quot;resources/ai-generated-images/dalle_2/DALL_E_2022-08-16_18.46.05_-_A_digital_illustration_of_glowing_toadstools_beside_a_pond_with_lilypads,_4k,_detailed,_trending_in_artstation.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt; &lt;img src=&quot;resources/ai-generated-images/dalle_2/DALL_E_2022-08-16_18.46.08_-_A_digital_illustration_of_glowing_toadstools_beside_a_pond_with_lilypads,_4k,_detailed,_trending_in_artstation.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;These reminded me of Magic: The Gathering illustrations (maybe for a Swamp) and the first one looks amazing with the fungi that is actually reflecting on the pond.&lt;/p&gt;

&lt;h2 id=&quot;fantasy-illustrations&quot;&gt;Fantasy Illustrations&lt;/h2&gt;

&lt;p&gt;After a lot of tinkering, I arrived at the prompt that so far has given me 100% satisfying results (I also added it to the older post).&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“A digital illustration of X, 4k, detailed, trending in artstation, fantasy”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Here come my results for different values of X.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“A digital illustration of a medieval town, 4k, detailed, trending in artstation, fantasy”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;resources/ai-generated-images/dalle_2/DALL_E_2022-08-16_18.53.24_-_A_digital_illustration_of_a_medieval_town,_4k,_detailed,_trending_in_artstation,_fantasy_vivid_colors.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt; &lt;img src=&quot;resources/ai-generated-images/dalle_2/DALL_E_2022-08-16_18.53.26_-_A_digital_illustration_of_a_medieval_town,_4k,_detailed,_trending_in_artstation,_fantasy_vivid_colors.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;
&lt;img src=&quot;resources/ai-generated-images/dalle_2/DALL_E_2022-08-16_18.53.30_-_A_digital_illustration_of_a_medieval_town,_4k,_detailed,_trending_in_artstation,_fantasy_vivid_colors.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt; &lt;img src=&quot;resources/ai-generated-images/dalle_2/DALL_E_2022-08-16_18.53.32_-_A_digital_illustration_of_a_medieval_town,_4k,_detailed,_trending_in_artstation,_fantasy_vivid_colors.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;These look almost like concept art for a game, and I would be happy using them as imagination fodder for a D&amp;amp;D campaign.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“A digital illustration of the Babel tower, 4k, detailed, trending in artstation, fantasy vivid colors”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;resources/ai-generated-images/dalle_2/DALL_E_2022-08-16_18.53.34_-_A_digital_illustration_of_the_Babel_tower,_4k,_detailed,_trending_in_artstation,_fantasy_vivid_colors.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt; &lt;img src=&quot;resources/ai-generated-images/dalle_2/DALL_E_2022-08-16_18.53.42_-_A_digital_illustration_of_the_Babel_tower,_4k,_detailed,_trending_in_artstation,_fantasy_vivid_colors.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“A digital illustration of a steampunk library with clockwork machines, 4k, detailed, trending in artstation, fantasy vivid colors”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;resources/ai-generated-images/dalle_2/DALL_E_2022-08-16_18.53.45_-_A_digital_illustration_of_a_steampunk_library_with_clockwork_machines,_4k,_detailed,_trending_in_artstation,_fantasy_vivid_colors.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt; &lt;img src=&quot;resources/ai-generated-images/dalle_2/DALL_E_2022-08-16_18.53.47_-_A_digital_illustration_of_a_steampunk_library_with_clockwork_machines,_4k,_detailed,_trending_in_artstation,_fantasy_vivid_colors.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;
&lt;img src=&quot;resources/ai-generated-images/dalle_2/DALL_E_2022-08-16_18.53.49_-_A_digital_illustration_of_a_steampunk_library_with_clockwork_machines,_4k,_detailed,_trending_in_artstation,_fantasy_vivid_colors.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt; &lt;img src=&quot;resources/ai-generated-images/dalle_2/DALL_E_2022-08-16_18.53.52_-_A_digital_illustration_of_a_steampunk_library_with_clockwork_machines,_4k,_detailed,_trending_in_artstation,_fantasy_vivid_colors.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“A digital illustration of Lothlorien houses on treetops with fireflies, 4k, detailed, trending in artstation, fantasy vivid colors, organic and bent”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;resources/ai-generated-images/dalle_2/DALL_E_2022-08-16_18.54.08_-_A_digital_illustration_of_Lothlorien_houses_on_treetops_with_fireflies,_4k,_detailed,_trending_in_artstation,_fantasy_vivid_colors.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;
&lt;img src=&quot;resources/ai-generated-images/dalle_2/DALL_E_2022-08-16_18.54.12_-_A_digital_illustration_of_Lothlorien_houses_on_treetops_with_fireflies,_4k,_detailed,_trending_in_artstation,_fantasy_vivid_colors.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt; &lt;img src=&quot;resources/ai-generated-images/dalle_2/DALL_E_2022-08-16_18.54.14_-_A_digital_illustration_of_Lothlorien_houses_on_treetops_with_fireflies,_4k,_detailed,_trending_in_artstation,_fantasy_vivid_colors.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“A digital illustration of a steampunk flying machine in the sky with cogs and mechanisms, 4k, detailed, trending in artstation, fantasy vivid colors”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;resources/ai-generated-images/dalle_2/DALL_E_2022-08-16_18.54.30_-_A_digital_illustration_of_a_steampunk_flying_machine_in_the_sky_with_cogs_and_mechanisms,_4k,_detailed,_trending_in_artstation,_fantasy_vivid_colors.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;
&lt;img src=&quot;resources/ai-generated-images/dalle_2/DALL_E_2022-08-16_18.54.36_-_A_digital_illustration_of_a_steampunk_flying_machine_in_the_sky_with_cogs_and_mechanisms,_4k,_detailed,_trending_in_artstation,_fantasy_vivid_colors.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Lastly, for another throwback to old times, I tried:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“The white fox in the Arcadian praerie | Breath-taking digital painting with vivid colours amazing art mesmerizing, captivating, artstation 3, japanese style”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;resources/ai-generated-images/dalle_2/DALL_E_2022-08-16_17.51.09_-_The_white_fox_in_the_Arcadian_praerie___Breath-taking_digital_painting_with_vivid_colours_amazing_art_mesmerizing,_captivating,_artstation_3,_japanese.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt; &lt;img src=&quot;resources/ai-generated-images/dalle_2/DALL_E_2022-08-16_17.51.14_-_The_white_fox_in_the_Arcadian_praerie___Breath-taking_digital_painting_with_vivid_colours_amazing_art_mesmerizing,_captivating,_artstation_3,_japanese.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The scene’s composition got so much better, and now it actually looks like an illustration of a fox. This is what the old model did:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/resources/ai-generated-images/1640235251_The_white_fox_in_the_Arcadian_praerie-Breath-taking_digital_painting_with_vivid_colours_amazing_ar.jpg&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It’s hard to believe the jump that only took a single year is so significant. Even if the difference in budgets also obviously plays a role.&lt;/p&gt;

&lt;p&gt;And how could I forget the site’s mascot: a cute anime otter (oil painting).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;resources/ai-generated-images/dalle_2/DALL_E_2022-08-16_19.03.05_-_An_cute_anime_otter,_oil_painting.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt; &lt;img src=&quot;resources/ai-generated-images/dalle_2/DALL_E_2022-08-16_19.03.10_-_An_cute_anime_otter,_oil_painting.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt; &lt;img src=&quot;resources/ai-generated-images/dalle_2/DALL_E_2022-08-16_19.03.12_-_An_cute_anime_otter,_oil_painting.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;conclusions&quot;&gt;Conclusions&lt;/h2&gt;

&lt;p&gt;I don’t have a lot to say besides a) DALL-E 2 can consistently generate images that are on par with professional illustrations and b) that’s both awesome and eery.&lt;/p&gt;

&lt;p&gt;If you’re an artist, I don’t think DALL-E 2 will replace you. But I honestly can’t imagine what DALL-E 3, or 4 will look like.&lt;/p&gt;

&lt;p&gt;Imagine what will happen when in ~5 years we have 30 times more compute due to Moore’s law and suddenly synthesizing an image will take seconds in personal computers. What’s more, who can tell what the next generation of text-to-image models will be like, or how much more resource effective they’ll be? And what happens when generated images are so realistic and plausible (which arguably they already are) that they are as good as the training data, and suddenly we can do self-supervised learning and improve latent representations even more?&lt;/p&gt;

&lt;p&gt;I am deeply interested in seeing where this will go, and which commercial applications people will come up with for these models beside the obvious “monetize art generation and let the dopamine cycle go brrr” or “make stock images obsolete”.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;If you liked this post, please share it on Twitter, Reddit, or whatever you like!&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;All images in this article were made with DALL-E 2, you can use them for free. If you do please link to this post.&lt;/em&gt;&lt;/p&gt;

</content>
		</entry>
	
		<entry>
			<title>Making Notes Work for Me: Reflections after 2 Years of Digital Gardening</title>
			<link href="http://strikingloo.github.io/reflections-digital-gardening"/>
			<updated>2022-08-12T00:00:00-03:00</updated>
			<id>http://strikingloo.github.io/reflections-digital-gardening</id>
			<content type="html">&lt;p&gt;I have been maintaining a Personal Wiki / Digital Garden for two years. Throughout all this time, I’ve reflected a lot on how I keep my notes, why I do it and what I get from doing it. Here is what I have learned.&lt;/p&gt;

&lt;p&gt;To save you time, I will start by stating my conclusion. It could be summarized as: &lt;strong&gt;Keeping notes is great, definitely do it. Maybe use a personal wiki?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Before I started running this site, I basically kept no notes, except the odd random paper scribble, unless it was for college related things. Even then, I just wrote things down on a notebook, never to be seen again.&lt;/p&gt;

&lt;p&gt;I was already using &lt;a href=&quot;/wiki/anki&quot;&gt;Spaced Repetition systems&lt;/a&gt; for language learning, but I wasn’t that deliberate about my memory in other areas of my life like programming.&lt;/p&gt;

&lt;p&gt;In the time since I set up &lt;a href=&quot;/wiki&quot;&gt;this site’s wiki&lt;/a&gt;, I finished my degree in Computer Science, and also aced a few key interviews. I am not saying my successes were completely or even mainly dependent on note-taking, but they definitely helped with my final exams. So after hitting the 80 notes benchmark, I thought I would write this post as a way to look back and do a little retro.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/resources/post_image/pathway.jpg&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I would say the main advantages of note taking, to me at least, have been in improving my search capabilities over my own memory (a file-system definitely beats a chunk of electric meat), and helping me be more organized and procrastinate less.&lt;/p&gt;

&lt;p&gt;It’s a lot easier to prepare for an exam when you have all of the relevant knowledge in a single markdown file, waiting to be reviewed.&lt;/p&gt;

&lt;p&gt;As for search, my criteria for adding things to my wiki is usually “will I want to review this fact in the future eventually, and will I have probably forgotten it if I don’t write it down?”&lt;/p&gt;

&lt;p&gt;That is why most of my notes are either &lt;strong&gt;literature notes or paper summaries&lt;/strong&gt;: I mostly find myself going back to impressing facts I read, or wanting to recall that pesky equation from that one diffusion paper I read and can’t quite put my finger on. In those cases, having all that information in a single place has been a life-saver. The same goes for remembering the exact flags of a bash command, or a few useful bookmarks.&lt;/p&gt;

&lt;p&gt;There is also the benefit of being able to read all the sections and paragraphs I highlighted from a book right after finishing it. Being able to do a ‘flash review’ of all the relevant contents helps me tie the introductory ideas to the more advanced concepts presented later on.&lt;/p&gt;

&lt;p&gt;This first review session also doubles as my polishing phase, where I take a raw note and edit it into something less context-dependent (each section should be understandable even if you haven’t read the book) and ideally succinct, clear and relevant.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Many powerful insights about a topic come not when reading a textbook for the first time, but when I review the main concepts later.&lt;/strong&gt; This shouldn’t be too surprising, I guess, but it’s still a benefit worth mentioning.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;In case you want to do a deeper dive into personal wikis and note-taking, here’s &lt;a href=&quot;/wiki/archiving&quot;&gt;every thought I’ve had about personal knowledge management&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;how-i-set-up-my-personal-wiki&quot;&gt;How I set up my personal wiki&lt;/h2&gt;

&lt;p&gt;First, let me tell you about my digital garden set up. It is actually simple: a &lt;strong&gt;Jekyll static site&lt;/strong&gt;, running on github pages because I wasn’t going to pay for a domain and hosting for something I’m not interested in profiting from.&lt;/p&gt;

&lt;p&gt;Every wiki article is a single markdown, rendered by Jekyll as an HTML, and the most I’ve engineered things on is just adding social media buttons (which, by the way, if reading this is fun, click them!), and putting a lot of thought and effort into interlinking my notes.&lt;/p&gt;

&lt;p&gt;This is what my personal wiki looked like initially:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/resources/post_image/screenshot1.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Notice the monochromatic, rustic design compared to what it is now. And this what it looked after my first round of modifications:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/resources/post_image/screenshot2.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I was married to the idea of a “cloud of ideas”, but I think the file names weren’t descriptive enough and anyone besides me who entered that site would not have known where to click at all. Even now I’m not sure how to present the wiki’s articles to make them more appealing, as design is not my strong point.&lt;/p&gt;

&lt;p&gt;The features I added to the site over time were:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Search&lt;/strong&gt; which I implemented based on tags and also matching queries to titles. This was easy to set up by making a big JSON which has each article’s metadata (title, tags, description, abstract).&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Importance scores&lt;/strong&gt; for the articles in my wiki. This was mostly for other readers, as I am pretty aware of what’s going on with each article, but I like making things more transparent for others. They mostly reflect how polished or relevant an article is.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Category pages&lt;/strong&gt; and &lt;strong&gt;support for mathematical notation&lt;/strong&gt;. These two weren’t that hard, but importing MathJax into my workflow made for much sleeker interfaces when writing down equations from papers or textbooks. I was getting tired of “&amp;lt;sub&amp;gt;” HTML tags and writing greek letters by hand. With MathJax you basically get LaTeX support for HTML.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;starting-your-own-personal-wiki&quot;&gt;Starting Your Own Personal Wiki&lt;/h3&gt;

&lt;p&gt;If you want to start your own personal wiki, I recommend you head over to &lt;a href=&quot;https://tomcritchlow.com/2019/02/17/building-digital-garden/&quot;&gt;Tom Critchlow’s guide&lt;/a&gt; for a step by step on setting up a Jekyll static web site where each markdown file is a note.&lt;/p&gt;

&lt;p&gt;You can alternatively fork &lt;a href=&quot;https://github.com/StrikingLoo/Personal-Wiki-Site-Setup&quot;&gt;my Personal Wiki Setup Github repository&lt;/a&gt; and host it in a free GitHub page (you can customize the CSS to make a completely different design maintaining the same functionalities as this site), following the instructions on the README to build your own wiki. I wrote a different post with a detailed &lt;a href=&quot;/personal-wiki-set-up&quot;&gt;step-by-step guide on how to set up a Digital Garden using my template&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Another easy way to set up a wiki is to use TiddlyWiki, which is free and easy to get started with, but won’t allow you the same level of customization. Here is &lt;a href=&quot;https://nesslabs.com/tiddlywiki-beginner-tutorial&quot;&gt;Nesslab’s guide to start using TiddlyWiki&lt;/a&gt;, but it mostly just amounts to downloading the software.&lt;/p&gt;

&lt;p&gt;Either way, it’s about one afternoon’s work if you’re not too picky with the design (or a potentially infinite time sink otherwise).&lt;/p&gt;

&lt;h2 id=&quot;how-i-take-notes&quot;&gt;How I take notes&lt;/h2&gt;

&lt;p&gt;I have spent more time than I am proud of reading articles about the ZettelKasten Method, posts in the zettelkasten forum and subreddit, and browsing other peoples Digital Gardens from &lt;a href=&quot;https://github.com/MaggieAppleton/digital-gardeners&quot;&gt;Maggie Appleton’s Digital Gardeners Repo&lt;/a&gt; for inspiration (or, calling it like it is, procrastination).&lt;/p&gt;

&lt;p&gt;By now I could say thinking of note-taking systems is a bit of a hobby of mine. And anyone who knows me can tell you I am a bit obsessive about productivity and workflows.&lt;/p&gt;

&lt;p&gt;So the first thing I will say is, my wiki is not a &lt;a href=&quot;/wiki/archiving#zettelkasten&quot;&gt;zettelkasten&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Zettelkastens, briefly, are based around the idea of using many atomic notes (usually short pieces of text), and interlinking them heavily, where a train of thought can be reconstructed by following a trail of multiple notes (zettels). This is supposed to make you a more prolific writer and help you develop essays, and zettels are expected to be mostly written in your own words as opposed to quotes and excerpts. The zettelkasten methodology also assumes you will start your trails from some particular zettel, and not just see the whole wiki as a big thing with search and tags, etc.&lt;/p&gt;

&lt;p&gt;There is also a big emphasis in most of the online digital gardening / note making community in networked thought: link your new notes to as many existing notes as possible, and find patterns and connections between concepts you learn. I don’t do this as much, as I don’t think there is that much value in linking (I especially dislike the idea of automatic backlinks for notes, and &lt;a href=&quot;/wiki/archiving#ideas-from-backlinks-are-bad-links&quot;&gt;some in the zettelkasten community agree&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;Instead, I prefer to take a slightly &lt;strong&gt;monolithic approach&lt;/strong&gt;, where I write big notes of 500~1000 lines or more for a single topic, and make each atomic idea a paragraph or subheading. If necessary, I can then just link to headings through their &lt;em&gt;id&lt;/em&gt; instead of the whole note -like my link to the zettelkasten section of my wiki just now-.&lt;/p&gt;

&lt;p&gt;I’m not saying it’s better, it just works better for me. I’d rather be able to review a whole subject in a few minutes by reading a single page instead of opening a hundred tabs every time. This is the part where I tell you to @ me in Twitter or HackerNews and tell me what you prefer doing and how you think it works better, because I love hearing about other people’s methodologies.&lt;/p&gt;

&lt;p&gt;I think if I was more writing or content-generation oriented instead of writing notes for future search, making more atomic notes and linking them could be better, but then each note itself would be further from being an essay.&lt;/p&gt;

&lt;p&gt;I mostly write the notes as I go, for instance by rephrasing an interesting paragraph from a paper just after I read it, or summarizing a book one chapter at a time. Whenever I copy or restate a point from an essay or blog post, I add a link to it as a source. If an idea relates relevantly to a different note and I think the connection is worth expanding upon, I will link to it, but also if I think I will want to have the other note on hand when reading this one.&lt;/p&gt;

&lt;p&gt;The fact that I do most of my reading in a digital format helps a lot with this workflow, and I usually paste quotes into a telegram chat or similar dumping ground if I’m on mobile before doing the final editing from my PC. For physical books, I guess I would mark pages and then find the PDF online to transcribe highlights, but I don’t really do this.&lt;/p&gt;

&lt;h2 id=&quot;conclusions&quot;&gt;Conclusions&lt;/h2&gt;

&lt;p&gt;Taking notes and keeping them tidy in a single place has helped me a lot when reading new, long textbooks or keeping up to date with the current deep learning literature. It’s also extremely convenient for preparing exams.&lt;/p&gt;

&lt;p&gt;Linking them has helped insofar as it makes the site’s navigation easier, but I don’t think particularly deep insights come from just interlinking notes or sections, and don’t feel my wiki is an insight-generating machine or anything like some people see the Zettelkasten (even though I think 80 big notes should count as ‘critical mass’).&lt;/p&gt;

&lt;p&gt;Being able to reflect on things while writing about them, and review my thoughts on a matter months down the line in an ordered, persistent and friendly way has definitely improved my thinking and retention of technical material, and knowing my source for each thing I jot down has been a godsend when directing other people to it. I am not as bullish as &lt;a href=&quot;http://paulgraham.com/essay.html&quot;&gt;Paul Graham&lt;/a&gt; on this, but I think &lt;a href=&quot;/wiki/idea-space&quot;&gt;sitting down to write ideas helps expand them&lt;/a&gt; and solidify them into something better.&lt;/p&gt;

&lt;p&gt;I also think this relates to the Feynman method of learning: if you can explain an idea you just read, you probably understood it. If you can’t, go reread.&lt;/p&gt;

&lt;p&gt;There’s something rewarding to being asked for advice or how to solve an issue and saying “oh don’t worry, that’s on my wiki”. It’s even more rewarding when you yourself are the one with the issue.&lt;/p&gt;

&lt;p&gt;All in all, I think most knowledge workers and scholars should take notes in the way that best suits them, and that this space deserves &lt;a href=&quot;https://numinous.productions/ttft/&quot;&gt;lots of experimentation&lt;/a&gt;, especially on an individual level to see what works for each person.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;If you liked this post, please let me know by upvoting on HackerNews or sharing on Tweeter/Reddit! Knowing people read me is my biggest driver for writing more.&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;related-reading&quot;&gt;Related Reading&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://numinous.productions/ttft/&quot;&gt;How Can We Develop Transformative Tools for Thought?&lt;/a&gt;: A good long form essay about tools that help us think. Much of the current craze around personal wikis or digital gardens originated here, and it’s also an entertaining read (if you’re the kind of person who finds this sort of content fun).&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://paulgraham.com/essay.html&quot;&gt;The Age of the Essay&lt;/a&gt;: A great essay by Paul Graham on the history of the essay, and general writing tips. It touches on how writing helps us think.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://joelhooks.com/digital-garden&quot;&gt;My blog is a digital garden, not a blog&lt;/a&gt;: One of the original essays that started the digital garden movement. A call to go back to the Old Ways of the web, do away with timelines (the stream) and move back to evergreen pages that are continually being tended.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://maggieappleton.com/garden-history&quot;&gt;A Brief History &amp;amp; Ethos of the Digital Garden - Maggie Appleton&lt;/a&gt;: A great summary of the Digital Garden movement with beautiful illustrations.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.gwern.net/About&quot;&gt;About this Website - Gwern&lt;/a&gt;: Definitely an inspiration for my wiki.&lt;/li&gt;
&lt;/ul&gt;
</content>
		</entry>
	
		<entry>
			<title>Lossy Image Compression with Dithering</title>
			<link href="http://strikingloo.github.io/dithering"/>
			<updated>2022-07-20T00:00:00-03:00</updated>
			<id>http://strikingloo.github.io/dithering</id>
			<content type="html">&lt;p&gt;Last year I took an elective on Computer Graphics (&lt;a href=&quot;/wiki/computer-graphics&quot;&gt;course notes&lt;/a&gt;) where I learned about OpenGL shaders, and image compression algorithms.&lt;/p&gt;

&lt;p&gt;One of the algorithms I learned about was &lt;a href=&quot;https://en.wikipedia.org/wiki/Floyd%E2%80%93Steinberg_dithering&quot;&gt;Floyd–Steinberg dithering&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Sometimes when storing an image, we want to compress it lossily: we create a new version of it that loses a part of the information and quality, but is significantly smaller in filesize.&lt;/p&gt;

&lt;p&gt;One way of achieving this, is using a reduced palette. The GIF format does this, for instance, by limiting the available colors in a single file to 256.&lt;/p&gt;

&lt;p&gt;Mapping colors to the most similar one available in a palette is called &lt;a href=&quot;/wiki/data-compression#lossy-compression&quot;&gt;&lt;em&gt;vector quantization&lt;/em&gt;&lt;/a&gt;, since we are effectively subdividing the whole color space into regions and assigning one color to each.&lt;/p&gt;

&lt;p&gt;Mapping pixels from the entire color space (which has 256\^3 possibilities) to a reduced set of values can generate a problem, though: regions of neighboring pixels tend to have similar colors, and the most straightforward way of mapping them, where &lt;strong&gt;each pixel goes to the most similar color available in the reduced palette&lt;/strong&gt;, will generate &lt;strong&gt;large regions of a single flat color in an image&lt;/strong&gt;, producing artifacts. This looks quite ugly to human eyes.&lt;/p&gt;

&lt;p&gt;Dithering aims to solve the problem of large flat areas by altering the colors of neighboring pixels after mapping one to a color from the palette. What’s novel about this method is that it &lt;strong&gt;carries over the difference between each pixel and its new in-palette assigned color, propagating it to its neighbors.&lt;/strong&gt; This way, if for instance I map a pixel to a darker color from my palette, I will make the next neighboring pixels brighter to compensate in average, since I will end up nudging them closer to different elements from the palette. This system prevents artifacts that look like monochromatic flat areas and preserves nuance better.&lt;/p&gt;

&lt;p&gt;However, since we will effectively be adding noise so the pixels in the surrounding region after changing each pixel, this will tend to increase entropy in the resulting image, making compression less effective. &lt;strong&gt;Generally we will need to evaluate the trade-off between image quality and compression.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The exact way the error is propagated is a bit arbitrary: just a convex distribution with most of it going to the neighbors below or to the right, and some in diagonal. What’s interesting is that this way we can take an image that has lots of different colors, and obtain one that has a reduced palette but keeps most of the information, at least perceptually.&lt;/p&gt;

&lt;h2 id=&quot;algorithm-and-implementation-details&quot;&gt;Algorithm and Implementation Details&lt;/h2&gt;

&lt;p&gt;For this project, I implemented dithering in Python using &lt;em&gt;numpy&lt;/em&gt;. The algorithm itself is simple, and I used the pseudocode from Wikipedia as a starting point.&lt;/p&gt;

&lt;p&gt;All we do is:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Move through all the pixels sequentially.&lt;/li&gt;
  &lt;li&gt;Given the current pixel, map it to the closest color in the palette.&lt;/li&gt;
  &lt;li&gt;Compute the difference between the old color and the new one.&lt;/li&gt;
  &lt;li&gt;Add a percentage of that difference to each of the pixel’s neighbors.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Here is the Python code for it.&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/StrikingLoo/481717106a5c9790d8a8fe2687fb7087.js&quot;&gt;&lt;/script&gt;

&lt;p&gt;I solved the color lookup using scipy’s &lt;em&gt;KDTree&lt;/em&gt;, but if you wanted to code everything from scratch you could just replace this with a linear search for the minimum distance element in the table (or use &lt;em&gt;numpy&lt;/em&gt;’s argmin).&lt;/p&gt;

&lt;h3 id=&quot;choosing-a-palette&quot;&gt;Choosing a Palette&lt;/h3&gt;

&lt;p&gt;We can choose the palette in different ways.&lt;/p&gt;

&lt;p&gt;For these experiments I went for evenly spaced palettes. I built them adding every multiple of 255/k for a given k, in all possible combinations for R, G and B. RGB colors go from (0,0,0) to (255,255,255), so this way given a certain k, we can guarantee no color will have to be mapped to a new one farther away than 128/k from it in any component.&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/StrikingLoo/a4d8417f96369ce69eb1aae73b850a1d.js&quot;&gt;&lt;/script&gt;

&lt;p&gt;A smarter approach to build a palette of k colors may have been running K-means clustering over the image to find k &lt;em&gt;centroids&lt;/em&gt; to use as colors, and then use those for a k-color palette. Or maybe just taking the most common color, removing all colors closer than a certain threshold to it, and then repeating those two steps k times.&lt;/p&gt;

&lt;h3 id=&quot;experiments-and-results&quot;&gt;Experiments and Results&lt;/h3&gt;

&lt;p&gt;As an experiment to see how fast the algorithm was and how much smaller the file could get, I ran dithering compression on this image of a red panda (source: &lt;a href=&quot;http://pixabay.com&quot;&gt;pixabay&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/resources/post_image/red-panda.jpg&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Here are the compressed versions after using palettes of evenly spaced colors (as described above) with k = 2, 4, 8 and 16. Note that for k=2, the palette is simple (only 0, 128 or 255 in each value of the color) and for k=16 we’re closer to representing every color (over 5000 different colors out of 256^3=~16M).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/resources/post_image/red-panda-2.jpg&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Compressed image with extremely small palette (k=2)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;In this case, most of the background was converted to grey. 
The picture went from 580Kb to 264Kb in size, but at what cost.&lt;/p&gt;

&lt;p&gt;We can see how the image loses less information as we increase palette size:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/resources/post_image/red-panda-4.jpg&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Compressed image with k = 4&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/resources/post_image/red-panda-8.jpg&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Compressed image with k = 8&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/resources/post_image/red-panda-16.jpg&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Compressed image with k = 16&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The image with the biggest palette looks pretty similar to the original (except in the background details) but has a size of 344Kb. That’s a 40% reduction.&lt;/p&gt;

&lt;p&gt;Just to make sure, let’s try a different image.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/resources/post_image/avenue.jpg&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;source: &lt;a href=&quot;https://pixabay.com/photos/avenue-trees-path-sunbeams-sunrays-815297/&quot;&gt;pixabay&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Again with k=16 (palette of ~5800 colors), this is what we obtain:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/resources/post_image/avenue-16.jpg&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This time, I’d say the image looks about the same. But now the size went down from 962Kb to 534Kb.&lt;/p&gt;

&lt;p&gt;That’s a reduction of 41% for a difference that we mostly don’t notice.&lt;/p&gt;

&lt;h2 id=&quot;extending-to-png&quot;&gt;Extending to .png&lt;/h2&gt;

&lt;p&gt;User @gakxd on &lt;a href=&quot;https://www.reddit.com/r/programming/comments/w6b8ia/lossy_image_compression_with_dithering/&quot;&gt;Reddit&lt;/a&gt; remarks correctly that, since JPEG is a lossy format in itself, we can’t make such a direct conclusion from the reduction in size seen after applying dithering. In fact dithering may interact poorly with JPEG compression. Due to that, I thought it was worth repeating the experiment with a .png image. In this case I am using this site’s twitter card image, and compressing it with a palette of k=16 again (5800 colors).&lt;/p&gt;

&lt;p&gt;Since .png is a lossless format, it will save the colors exactly the way this program leaves them, and we can compare results more directly this way.
The only modifications I had to add to the program were for dealing with RGBA instead of RGB images, and my solution has simply been to leave the ‘A’ channel unchanged (keeping identical transparency) and only propagate errors forward if the A value is bigger than 0. I am not including the code itself as it would be pretty redundant.&lt;/p&gt;

&lt;p&gt;Here are the results.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/resources/post_image/potted-tree.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; style=&quot;height:30%; width:30%&quot; /&gt;
&lt;img src=&quot;/resources/post_image/potted-tree-16.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; style=&quot;height:30%; width:30%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;original image, image with a reduced palette (k=16)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Again, the images look pretty similar (only the fruits in the tree suffered a little from the compression), and this time the file size went from 160Kb to 92Kb, for a reduction of 42% again. And this time, we can appreciate exactly how the image changed, and know that all the gains were solely from the use of dithering, and no other changes to the image itself.&lt;/p&gt;

&lt;p&gt;I am not sure what compression algorithms the PNG format uses so I’m not sure if reducing the palette and avoiding contiguous areas of the same color, which is what dithering does, will contribute possitively to them or not. I leave that for the community to discuss.&lt;/p&gt;

&lt;p&gt;For fun, I tried the .png version on a bigger file (3.4MB) and it got to 885Kb with our k=16 palette. It’s even less than half the size now.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/resources/post_image/large-tree.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;
&lt;img src=&quot;/resources/post_image/large-tree-16.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;before and after&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;We made a simple image compressor that runs sequentially over an image’s pixels mapping them to their closest color in a reduced palette. This way we can constrain it to a smaller color-space and reduce its size significantly.&lt;/p&gt;

&lt;p&gt;For most of the images tried, using a palette of 5800 colors represented a small perceptual difference in quality, but a 40% reduction in size.&lt;/p&gt;

&lt;p&gt;I thought of maybe speeding up the program with parallel execution, but found the implementation a bit tedious. However I’m linking &lt;a href=&quot;https://hal.archives-ouvertes.fr/hal-03594790/document&quot;&gt;the paper for further reading&lt;/a&gt;, in case anyone is interested.&lt;/p&gt;

&lt;p&gt;After further reading about compression, I wrote my notes in the [data compression wiki article]((/wiki/data-compression). I may do a follow up post explaining Huffman codes and the LZ77 algorithm for lossless compression.&lt;/p&gt;
</content>
		</entry>
	
		<entry>
			<title>Confidence Intervals in Python using Bootstrapping</title>
			<link href="http://strikingloo.github.io/bootstrapping"/>
			<updated>2022-07-13T00:00:00-03:00</updated>
			<id>http://strikingloo.github.io/bootstrapping</id>
			<content type="html">&lt;p&gt;Suppose you have data, and want to report a certain metric. For instance, you’d like to estimate the value of some KPI given a set of measures, or you’re a physicist trying to estimate a constant from the readings of a sensor.&lt;/p&gt;

&lt;p&gt;Since you’re estimating that quantity using empiric data, sampled from the “true” distribution, you will have sample error in your estimation. This means even if your measuring method is free of bias, there is no guarantee that your estimation is “correct”.&lt;/p&gt;

&lt;p&gt;To solve this problem, statisticians came up with confidence intervals. This means instead of reporting a single number, you may report an interval like this:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;I am 95% certain that the real value lies between X and Y.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This way, by regulating our certainty (in that example, 95%) we can indirectly move our precision (here the difference between Y and X). For instance, if I say the average length of a species of fish is between 10 and 12 inches (95% sure), I may also estimate I am 80% sure it’s between 10.5 and 11.5 inches, or 99% sure it’s between 9 and 13.&lt;/p&gt;

&lt;p&gt;In the case where we &lt;strong&gt;assume the data come from a normal distribution&lt;/strong&gt;, the 95% confidence interval for the mean is easy to calculate: take the sample’s mean, and add and subtract its standard deviation times two:
 \( \mu \pm 2\sigma  \) .&lt;/p&gt;

&lt;p&gt;There is however a method that doesn’t assume anything about the data’s underlying distribution. It’s called &lt;strong&gt;bootstrapping&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Bootstrapping is in a way a lot simpler than other methods: it makes less assumptions, and it simply works by taking multiple samples and recomputing our metric, which makes it ideal for a programmatic solution.&lt;/p&gt;

&lt;p&gt;Say we want to give a 95% confidence interval for the mean of a variable, and we have &lt;em&gt;N&lt;/em&gt; instances of it. What we do is:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Take &lt;em&gt;M&lt;/em&gt; random samples of &lt;em&gt;k&lt;/em&gt; elements each from our data.&lt;/li&gt;
  &lt;li&gt;For each sample, estimate the value of our metric. Store this new list of &lt;em&gt;M&lt;/em&gt; estimations.&lt;/li&gt;
  &lt;li&gt;Take the percentiles of that list such that X% of values are bound by them, where X is our desired confidence.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;More concretely, if we wanted the 95% confidence interval for the mean of a variable &lt;em&gt;Y&lt;/em&gt;, and we have N instances of that variable, we would take M smaller samples (of k elements each), compute the mean for each of them, then take the p2.5 and p97.5 for those means. We could then say we are 95% sure the true mean lies between those two percentile values. If we wanted to be 99% certain, we would take the p0.5 and p99.5, and so on.&lt;/p&gt;

&lt;p&gt;Typically, &lt;em&gt;k&lt;/em&gt; is a small number (like 30 or 100), and &lt;em&gt;M&lt;/em&gt; is bigger (like ~N/k).&lt;/p&gt;

&lt;p&gt;So, in case we want to add confidence intervals to our predictions or measurements, or we want to check how stable a prediction is over time / different population segments, bootstrapping may be the solution for us.&lt;/p&gt;

&lt;p&gt;Here is a minimal implementation I made in Python, which I plan to copy and paste from here on out. Feel free to use it too.&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/StrikingLoo/f309573b19d075751ebf010c4c863b9a.js&quot;&gt;&lt;/script&gt;

&lt;h3 id=&quot;additional-reading&quot;&gt;Additional Reading&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://erikbern.com/2018/10/08/the-hackers-guide-to-uncertainty-estimates.html&quot;&gt;The hacker’s guide to uncertainty estimates - Erik Bernhardsson&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

</content>
		</entry>
	
		<entry>
			<title>Text to Image Art: Experiments and Prompt Guide for DALL-E Mini and Other AI Art Models</title>
			<link href="http://strikingloo.github.io/art-prompts"/>
			<updated>2022-07-11T00:00:00-03:00</updated>
			<id>http://strikingloo.github.io/art-prompts</id>
			<content type="html">&lt;p&gt;Ever since &lt;a href=&quot;https://openai.com/blog/dall-e/&quot;&gt;DALL-E&lt;/a&gt; came out in January 2021, or even before that with &lt;a href=&quot;https://arxiv.org/pdf/1601.06759.pdf&quot;&gt;PixelRNN&lt;/a&gt;, I’ve found generative models, especially for images, amazing. But what I was interested in was how could I write better prompts that yielded the most beautiful images?&lt;/p&gt;

&lt;p&gt;The idea of text-to-image generators like &lt;a href=&quot;/wiki/clip&quot;&gt;CLIP&lt;/a&gt; or &lt;a href=&quot;/wiki/glide&quot;&gt;GLIDE&lt;/a&gt; is even more astounding, and I love being able to play with them and trying to get a glimpse of the way they “perceive” textual and image inputs. I think interpretability is a fascinating field of study, and understanding a model’s representations may yield ideas for better models in the future (though usually the simplest way to make a better model is training a bigger one with more compute).&lt;/p&gt;

&lt;p&gt;Giving a model a prompt to generate an image out-of becomes sort of addictive, and I’ve spent longer than I am proud to admit fiddling around with these. Here are some of the images I made with &lt;a href=&quot;https://www.eleuther.ai/&quot;&gt;EleutherAI’s&lt;/a&gt; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.imagine&lt;/code&gt; model (which I think is a VQVAE like DALL-E) and some I made using everyone’s favorite viral text-to-image generator, &lt;a href=&quot;https://huggingface.co/spaces/dalle-mini/dalle-mini&quot;&gt;DALL-E Mini&lt;/a&gt; (a small open source version of DALL-E 2, a guided diffusion model with extra steps).&lt;/p&gt;

&lt;p&gt;What follow are a small guide on prompt engineering for dall-e mini and other text-to-image models. Then a showcase of the prompts I used for generating images, and the images I obtained. These were cherrypicked, as they were the ones I liked the most.&lt;/p&gt;

&lt;p&gt;The first batch is all made with EleutherAI’s imagine model, and the second one was made with DALL-E Mini, which can be told from the screenshot’s visible UI.&lt;/p&gt;

&lt;p&gt;I found the comparisons where I used the same prompt in both models to be particularly interesting in showcasing how much better DALL-E mini is at composition, even though VQVAE, being bigger and taking longer, generates images at a higher resolution and thus renders textures and objects in a more believable way.&lt;/p&gt;

&lt;p&gt;I also hope seeing these prompts can give you a small hint of how to make better prompts for your own ideas (usually, just appending ‘digital painting’ or ‘oil painting’ and ‘artstation’ in the end will do half of the trick).&lt;/p&gt;

&lt;p&gt;My general idea is that DALL-E mini can generate satisfying results when the prompt is well-constructed enough, if asked to generate images of inanimate objects, landscapes, buildings and so on. However, illustrations containing animals, people, humanoids or anything that moves, or prompts that ask for a specific action or use verbs, usually get poor results. Bigger models usually deal better with humanoid or animal shapes, and construct scenes better when they include action.&lt;/p&gt;

&lt;p&gt;Feel free to steal any of these images and use them for anything, or share them on social media. I mostly tried prompts that dealt with Biblical or mythological themes, because for some reason I found most people didn’t do those sorts of prompts as much (or maybe because I’m a big fantasy/D&amp;amp;D geek).&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&lt;strong&gt;Update&lt;/strong&gt;: Besides Craiyon, I’ve found &lt;a href=&quot;https://github.com/jina-ai/dalle-flow&quot;&gt;Dall-E Flow&lt;/a&gt;, a colab notebook that uses &lt;a href=&quot;https://github.com/jina-ai/jina&quot;&gt;Jina-AI&lt;/a&gt; to be the best tool for generating beautiful DALL-E images, and I recommend everyone to give it a spin. It’s free and open source, and I’m loving it.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;I also wrote separate articles after experimenting with &lt;a href=&quot;/dall-e-2-prompt-guide&quot;&gt;OpenAI’s DALL-E 2&lt;/a&gt; and its &lt;a href=&quot;/stable-diffusion-vs-dalle-2&quot;&gt;open source competition, StableDiffusion&lt;/a&gt; both of which blew my mind and made me think of the future differently.&lt;/p&gt;

&lt;h3 id=&quot;how-to-write-prompts-for-dall-e--stablediffusion&quot;&gt;How to write prompts for DALL-E / StableDiffusion&lt;/h3&gt;
&lt;p&gt;Usually, what I do is write what I want (adjectives + nouns usually get better results than verbs or complex scenes), then append&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;“Gorgeous/amazing/beautiful” + “digital painting/oil painting/digital art/canvas/3d render” + any of “unreal engine/unity engine”&lt;/li&gt;
  &lt;li&gt;Then I append “style clues” like “anime/Picasso/Giger -always good results-/Junji Ito”&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Using this simple framework often gets me results close to what I want. If you have any tips on how to do better, tell me on Twitter.&lt;/p&gt;

&lt;p&gt;For example, this is a prompt that gave me great results in Craiyon.&lt;/p&gt;

&lt;p&gt;‘Cluttered house in the woods | anime oil painting high resolution cottagecore ghibli inspired 4k’&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/resources/ai-generated-images/ghibli-house-craiyon.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;As you can see, just appending &lt;em&gt;”| oil painting high resolution 4k”&lt;/em&gt; will improve most of your results. You can then also add a style cue like ‘Ghibli inspired’, ‘Giger’ or ‘Salvador Dali’.&lt;/p&gt;

&lt;p&gt;Other DALL-E/craiyon prompt templates for future use:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Pixar style 3D render of X&lt;/li&gt;
  &lt;li&gt;Subdivision control mesh of X&lt;/li&gt;
  &lt;li&gt;A photograph of X, 4k, detailed&lt;/li&gt;
  &lt;li&gt;Low-poly render of X; high resolution, 4k&lt;/li&gt;
  &lt;li&gt;A digital illustration of X, 4k, detailed, trending in artstation, fantasy vivid colors&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;That last one has worked well for me for almost any prompt that describes a simple static scene (like houses, cities, landscapes or interiors) or a single humanoid/animal/plant. You can append styles to these in the end too (Eldritch, disco, lo-fi, etc.).&lt;/p&gt;

&lt;h2 id=&quot;vqvae-eleutherai&quot;&gt;VQVAE (EleutherAI)&lt;/h2&gt;

&lt;p&gt;&lt;em&gt;Steampunk inventor’s library | Gorgeous digital painting with sober colours amazing art mesmerizing, captivating, artstation 3, cozy&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/resources/ai-generated-images/image-1.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;mechanical clockwork flying machine renaissance | Gorgeous digital painting with sober colours amazing art mesmerizing, captivating, artstation 3, realistic, render materials&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/resources/ai-generated-images/image-2.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;A beautiful painting of waves crashing on a cliff by Thomas Cole&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/resources/ai-generated-images/1640231078_A_beautiful_painting_of_waves_crashing_on_a_cliff_by_Thomas_Cole.jpg&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;A beautiful painting of waves crashing on a cliff by Thomas Cole&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/resources/ai-generated-images/1640231297_A_beautiful_painting_of_an_Austrian_Palace_by_Thomas_Cole-cityscape.jpg&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;A glade under the stars | Gorgeous digital painting with sober colours amazing art mesmerizing, captivating, artstation 3, cottagecore, cozy&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/resources/ai-generated-images/1640234361_A_glade_under_the_stars-Gorgeous_digital_painting_with_sober_colours_amazing_art_mesmerizing_capt.jpg&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;A disco coral reef underwater | Gorgeous digital painting with aggressive colours amazing art mesmerizing, captivating, artstation 3, cozy, lo-fi&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/resources/ai-generated-images/1640234860_A_disco_coral_reef_underwater-Gorgeous_digital_painting_with_aggressive_colours_amazing_art_mesmer.jpg&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The sunken city of R’lyeh lies dormant | Breath-taking digital painting with dark colours amazing art mesmerizing, captivating, artstation 3, Lovecraftian, eldritch&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/resources/ai-generated-images/1640234962_The_sunken_city_of_Rlyeh_lies_dormant-Breath-taking_digital_painting_with_dark_colours_amazing_ar.jpg&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The white fox in the Arcadian praerie | Breath-taking digital painting with vivid colours amazing art mesmerizing, captivating, artstation 3, japanese style&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/resources/ai-generated-images/1640235251_The_white_fox_in_the_Arcadian_praerie-Breath-taking_digital_painting_with_vivid_colours_amazing_ar.jpg&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The green idyllic Arcadian praerie with sheep | Breath-taking digital painting with placid colours amazing art mesmerizing, captivating, artstation 3, cottagecore
A beautiful painting of the Garden of Eden by Thomas Cole&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/resources/ai-generated-images/1640235438_The_green_idyllic_Arcadian_praerie_with_sheep-Breath-taking_digital_painting_with_placid_colours_a.jpg&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;biblical-references&quot;&gt;Biblical References&lt;/h3&gt;

&lt;p&gt;A beautiful painting of the Tower of Babel by Thomas Cole&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/resources/ai-generated-images/1640270061_A_beautiful_painting_of_the_Garden_of_Eden_by_Thomas_Cole.jpg&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;the Tower of Babel by beeple gurney richter | 3D Depth Shader;special effects;production values;movie FX;VFX;sci-fi;4K resolution;high dynamic range;Dolby Vision;hdr10;atmos;3 dimensional;vray; ray tracing;hyperrealistic;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/resources/ai-generated-images/1640270742_the_Tower_of_Babel_by_beeple_gurney_richter-3D_Depth_Shaderspecial_effectsproduction_valuesmovi.jpg&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;the Garden of Eden by beeple gurney richter | 3D Depth Shader;special effects;production values;movie FX;VFX;sci-fi;4K resolution;high dynamic range;Dolby Vision;hdr10;atmos;3 dimensional;vray; ray tracing;hyperrealistic;matte painting&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/resources/ai-generated-images/1640270435_the_Garden_of_Eden_by_beeple_gurney_richter-3D_Depth_Shaderspecial_effectsproduction_valuesmovi.jpg&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;fantasy-prompts&quot;&gt;Fantasy Prompts&lt;/h3&gt;

&lt;p&gt;A breath-taking painting of an eldritch beetle god that should not be by Thomas Cole | old masters, artstation 3&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/resources/ai-generated-images/1640231439_A_breath-taking_painting_of_an_eldritch_beetle_god_that_should_not_be_by_Thomas_Cole-old_masters_.jpg&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;A breath-taking painting of Valkyries riding pegasi on the clouds over cliff by Thomas Cole | old masters, artstation 3&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/resources/ai-generated-images/1640231675_A_breath-taking_painting_of_Valkyries_riding_pegasi_on_the_clouds_over_cliff_by_Thomas_Cole-old_ma.jpg&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;golden fae stands on flower at night | Gorgeous digital painting with rich colours amazing art mesmerizing, captivating, artstation 3, cottagecore aesthetic&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/resources/ai-generated-images/image.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Fae Blessing | Breath-taking digital painting with placid colours amazing art mesmerizing, captivating, artstation 3&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/resources/ai-generated-images/1640276542_Fae_Blessing-Breath-taking_digital_painting_with_placid_colours_amazing_art_mesmerizing_captivati.jpg&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/resources/ai-generated-images/1640232053_A_painting_of_an_Eldrazi_God_with_a_nova_background_by_Thomas_Cole-artstation_3.jpg&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;An etching of a Troll with a grin by Gustave Doré | artstation 3&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/resources/ai-generated-images/1640233029_An_etching_of_a_Troll_with_a_grin_by_Gustave_Dore-artstation_3.jpg&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;A wood engraving of Goblins by Gustave Doré&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/resources/ai-generated-images/1640233240_A_wood_engraving_of_Goblins_by_Gustave_Dore.jpg&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Refreshing composition, thick texture. Grotesque Centaur by Zdzisław Beksiński and Geiger inspired&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/resources/ai-generated-images/1640233456_Refreshing_composition_thick_texture-Grotesque_Centaur_by_Zdzisaw_Beksinski_and_Geiger_inspired.jpg&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Grotesque Centaur by Zdzisław Beksiński and Geiger inspired
A centaur in a glade under the stars | Gorgeous digital painting with sober colours amazing art mesmerizing, captivating, artstation 3, cottagecore, cozy&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/resources/ai-generated-images/1640233735_Grotesque_Centaur_by_Zdzisaw_Beksinski_and_Geiger_inspired.jpg&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;fantasy tavern | Breath-taking digital painting with warm colours amazing art mesmerizing, captivating, artstation 3, cottagecore&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/resources/ai-generated-images/1640277186_fantasy_tavern-Breath-taking_digital_painting_with_warm_colours_amazing_art_mesmerizing_captivati.jpg&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;fantasy tavern interior | Breath-taking digital painting with warm colours amazing art mesmerizing, captivating, artstation 3&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/resources/ai-generated-images/1640277448_fantasy_tavern_interior-Breath-taking_digital_painting_with_warm_colours_amazing_art_mesmerizing_.jpg&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;fantasy tavern interior | Breath-taking digital illustration with warm colours amazing art mesmerizing, captivating, artstation 3, D&amp;amp;D Style&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/resources/ai-generated-images/1640277481_fantasy_tavern_interior-Breath-taking_digital_illustration_with_warm_colours_amazing_art_mesmerizi.jpg&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;purple Fungi from Yuggoth | Gorgeous digital painting with intense colours amazing art mesmerizing, captivating, artstation 3, thought-provoking, dark&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/resources/ai-generated-images/1640281351_purple_Fungi_from_Yuggoth-Gorgeous_digital_painting_with_intense_colours_amazing_art_mesmerizing_.jpg&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;3D hyperrealistic materials and soft warm lighting | 3d render amazing graphics shaders 4k UHD| An amazing digital illustration depicting Archangel Michael spread enormous wings at night | artstation 3, hyperrealist, realistic&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/resources/ai-generated-images/1640232675_3D_hyperrealistic_materials_and_soft_warm_lighting-3d_render_amazing_graphics_shaders_4k_UHD_An_a.jpg&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;dall-e-mini&quot;&gt;DALL-E Mini&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/resources/ai-generated-images/dallemini_2022-7-5_15-38-42.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/resources/ai-generated-images/dallemini_2022-7-5_15-44-51.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/resources/ai-generated-images/dallemini_2022-7-5_15-47-52.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/resources/ai-generated-images/dallemini_2022-7-5_18-14-38.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/resources/ai-generated-images/dallemini_2022-6-28_20-23-36.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/resources/ai-generated-images/dallemini_2022-6-28_20-20-14.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/resources/ai-generated-images/dallemini_2022-6-28_20-21-57.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/resources/ai-generated-images/dallemini_2022-6-29_0-19-26.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/resources/ai-generated-images/dallemini_2022-6-29_0-27-2.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/resources/ai-generated-images/dallemini_2022-6-29_2-56-55.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/resources/ai-generated-images/dallemini_2022-6-28_20-27-30.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/resources/ai-generated-images/dallemini_2022-6-28_20-31-26.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/resources/ai-generated-images/dallemini_2022-6-29_2-59-32.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/resources/ai-generated-images/dallemini_2022-6-29_3-3-45.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/resources/ai-generated-images/dallemini_2022-6-29_3-2-3.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/resources/ai-generated-images/dallemini_2022-6-29_14-30-15.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/resources/ai-generated-images/dallemini_2022-6-29_2-59-23.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/resources/ai-generated-images/dallemini_2022-6-17_15-54-37.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/resources/ai-generated-images/dallemini_2022-6-29_16-14-15.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/resources/ai-generated-images/dallemini_2022-6-17_18-15-1.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/resources/ai-generated-images/dallemini_2022-6-17_15-51-0.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/resources/ai-generated-images/dallemini_2022-6-29_14-38-38.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/resources/ai-generated-images/dallemini_2022-6-18_15-46-26.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/resources/ai-generated-images/dallemini_2022-6-18_16-6-12.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/resources/ai-generated-images/dallemini_2022-6-18_15-58-12.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/resources/ai-generated-images/dallemini_2022-6-18_15-42-38.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/resources/ai-generated-images/dallemini_2022-6-18_16-11-16.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/resources/ai-generated-images/dallemini_2022-6-18_20-49-47.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/resources/ai-generated-images/dallemini_2022-6-18_16-23-50.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/resources/ai-generated-images/dallemini_2022-6-18_16-31-15.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/resources/ai-generated-images/dallemini_2022-6-18_20-41-34.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/resources/ai-generated-images/dallemini_2022-6-18_16-35-53.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/resources/ai-generated-images/dallemini_2022-6-23_0-21-57.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/resources/ai-generated-images/dallemini_2022-6-18_21-3-57.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/resources/ai-generated-images/dallemini_2022-6-22_12-18-38.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/resources/ai-generated-images/dallemini_2022-6-22_11-50-10.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/resources/ai-generated-images/dallemini_2022-6-23_0-36-27.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/resources/ai-generated-images/dallemini_2022-6-23_0-47-9.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/resources/ai-generated-images/dallemini_2022-6-23_0-26-39.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/resources/ai-generated-images/dallemini_2022-6-23_0-38-59.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/resources/ai-generated-images/dallemini_2022-6-23_0-33-40.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/resources/ai-generated-images/dallemini_2022-6-23_0-54-37.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/resources/ai-generated-images/dallemini_2022-6-23_1-57-32.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/resources/ai-generated-images/dallemini_2022-6-23_1-9-58.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/resources/ai-generated-images/dallemini_2022-6-23_1-3-0.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/resources/ai-generated-images/dallemini_2022-7-10_23-40-42.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/resources/ai-generated-images/dallemini_2022-7-10_23-40-34.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/resources/ai-generated-images/dallemini_2022-7-10_23-42-24.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/resources/ai-generated-images/dallemini_2022-7-10_23-42-58.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/resources/ai-generated-images/dallemini_2022-7-10_23-46-8.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/resources/ai-generated-images/dallemini_2022-7-10_23-46-15.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/resources/ai-generated-images/dallemini_2022-7-10_23-46-20.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/resources/ai-generated-images/dallemini_2022-7-10_23-46-29.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/resources/ai-generated-images/dallemini_2022-7-10_23-46-34.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/resources/ai-generated-images/dallemini_2022-7-10_23-46-38.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/resources/ai-generated-images/dallemini_2022-7-10_23-46-46.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/resources/ai-generated-images/dallemini_2022-7-10_23-48-26.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/resources/ai-generated-images/dallemini_2022-7-10_23-48-31.png&quot; alt=&quot;&quot; loading=&quot;lazy&quot; /&gt;&lt;/p&gt;
</content>
		</entry>
	
		<entry>
			<title>Recent changes</title>
			<link href="http://strikingloo.github.io/changes"/>
			<updated>2020-11-19T00:00:00-03:00</updated>
			<id>http://strikingloo.github.io/changes</id>
			<content type="html">&lt;p&gt;As I said previously, this site is just a casual, fun thing I’m maintaining, mostly a repository for my thoughts and notes. That’s why the Digital Garden/ Zettelkassen / Personal wiki part gets all the attention.&lt;/p&gt;

&lt;p&gt;With that said, today I finally sat down to improve this a bit.&lt;/p&gt;

&lt;p&gt;Here’s what I finally did today:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Figured out how to run the site locally. My iteration speed and testing/debugging workflow improved immensely due to much shorter delay. Obviously. It was easy, and it’s now documented under ‘useful bash commands’. I just hadn’t really prioritized it that much, since remote testing worked anyway and the site almost never broke.&lt;/li&gt;
  &lt;li&gt;Improved the styling a bit. There’s a ton of room for improvement, I am no front-end developer or designer, but at least now the font-size isn’t huge and the general style is alright.&lt;/li&gt;
  &lt;li&gt;My favorite change: added search functionality for my Digital Garden. This will come in handy when there are a lot more notes in here. I’m thinking of doing a prettier layout than “all the filenames in a huge table”, something like the beautiful garden in &lt;a href=&quot;https://maggieappleton.com/garden&quot;&gt;Maggie Appleton’s website&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;My next things to do are:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Keep adding notes and zettels to my Garden.&lt;/li&gt;
  &lt;li&gt;Add a “thought trains” note where I just write whatever comes to my mind, with a low bar for quality.&lt;/li&gt;
  &lt;li&gt;Improve the site’s design.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Anyway, this really feels like gardening in a way: it’s relaxing, things grow organically and slowly as needs arise (aesthetic or otherwise).&lt;/p&gt;

&lt;p&gt;I don’t think many people will ever read this, so it’s almost like a journal.&lt;/p&gt;

&lt;p&gt;Maybe I’ll start using this post as a changelog.&lt;/p&gt;

&lt;p&gt;2022-08-16: Move all wiki-articles to wiki. Remove category pages for the time being, but they will be re-added soon.&lt;/p&gt;

&lt;p&gt;2022-08-24: After adding the second Personal Wiki post, I removed fontawesome and replaced it with beautiful icon images. I was using few icons and now most pages have seen a ~50% reduction in size.&lt;/p&gt;

&lt;p&gt;2022-08-30: Make headers Georgia, blog posts have description and visible date and are inline blocks now, also lots of little changes to navigation guide and wiki main page yesterday. Also made bottom buttons lazy-loading, I wonder if someone will notice. The next day, make wiki items similar to blog items and height=auto everywhere on mobile.&lt;/p&gt;

&lt;p&gt;2022-09-06: Unified search through tagged and search page, added default and custom images for general social media. Reduced line-height for H1 to hopefully decrease bounce ratio for mobile.&lt;/p&gt;

&lt;p&gt;2022-09-08: The big site redesign. Made site monochromatic, and layout header single line and flex. The menu became responsive and cuter on mobile. In 09/15 I also added the little ‘#’ after headers on hover using CSS.&lt;/p&gt;

&lt;p&gt;2022-09-25: After the big redesign, traffic fell a bit. Now made images bigger by default and responsive in size. Also added bottom buttons and added links to homepage.&lt;/p&gt;

&lt;p&gt;2022-10-03: Implement grid layouts. Simplify post descriptions.&lt;/p&gt;
</content>
		</entry>
	
		<entry>
			<title>Having a ton of fun</title>
			<link href="http://strikingloo.github.io/test-post"/>
			<updated>2020-06-11T00:00:00-03:00</updated>
			<id>http://strikingloo.github.io/test-post</id>
			<content type="html">&lt;p&gt;Honestly, this is just a fun experiment. I may use this as a laid back blog where I rant about my life, philosophy, Politics, programming and who knows what else. This website is 90% me playing around with Jekyll, 10% content, so don’t be surprised if this is the only blog post for a while.&lt;/p&gt;

&lt;p&gt;I may also use this site as a bit of a wiki/knowledge repository for the times when Evernote doesn’t seem powerful enough. I want this site to become a sort of ‘digital garden’ as described in &lt;a href=&quot;https://hapgood.us/2015/10/17/the-garden-and-the-stream-a-technopastoral/&quot;&gt;Mike Caulfield’s technopastoral&lt;/a&gt;, mostly for my own use but also for anyone who may find it helpful.&lt;/p&gt;

&lt;p&gt;I don’t plan on sharing this site on Social Media too much, however.&lt;/p&gt;
</content>
		</entry>
	
		<entry>
			<title>3 Machine Learning Books that Helped me Level Up as a Data Scientist</title>
			<link href="http://strikingloo.github.io/3-ml-books"/>
			<updated>2019-04-28T00:00:00-03:00</updated>
			<id>http://strikingloo.github.io/3-ml-books</id>
			<content type="html">&lt;p&gt;There is a Japanese word, &lt;em&gt;tsundoku&lt;/em&gt; (積ん読), which means buying and keeping a growing collection of books, even though you don’t really read them all.&lt;/p&gt;

&lt;p&gt;I think we Developers and Data Scientists are particularly prone to falling into this trap. Personally, I even hoard bookmarks: my phone’s Chrome browser has so many open tabs, the counter was replaced with a “:D” emoji.&lt;/p&gt;

&lt;p&gt;In that zeal for reading and learning most of us experience, we usually end up lost , not sure of which book to pick up next. That’s why today I’ll give you a short list: just 3 Machine Learning books, so that you won’t just bookmark it and forget it.&lt;/p&gt;

&lt;p&gt;Each of these books has helped me immensely in different stages of my career as a Data Scientist, particularly in my role as a Machine Learning Engineer.&lt;/p&gt;

&lt;h2 id=&quot;oreilly-data-science-from-scratch-with-python&quot;&gt;O’Reilly: Data Science from Scratch with Python&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;/resources/post_image/data-science-from-scratch.png&quot; alt=&quot;The book cover of Data Science from Scratch with Python&quot; loading=&quot;lazy&quot; style=&quot;width:40%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I’ll review &lt;a href=&quot;https://amzn.to/2DDhIUH&quot;&gt;this book&lt;/a&gt; first, since it’s the most introductory or broad one in this list.&lt;/p&gt;

&lt;p&gt;I have a personal attachment to this book, since it’s the one that got me my job. That’s right. I knew next to nothing about Data Science, even what Data Science was, before picking up this book.&lt;/p&gt;

&lt;p&gt;I did have a pretty strong Probability and Statistics background, and knew enough Python to defend myself. However, I was missing the practical side of it.&lt;/p&gt;

&lt;p&gt;This book did many things for me. It:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Showed me how to process data in Python efficiently and elegantly (following Python’s good practices ).&lt;/li&gt;
  &lt;li&gt;Taught me how to implement most simple Machine Learning algorithms from scratch.&lt;/li&gt;
  &lt;li&gt;Showed me what the day-to-day job of a Data Scientist may look like.&lt;/li&gt;
  &lt;li&gt;Taught me how to communicate my results to others clearly.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I wholeheartedly recommend it if you’re new to the Data Science community. It will give you a clear overview of most topics you’ll need in order to start being a productive Data Scientist.&lt;/p&gt;

&lt;p&gt;It will also showcase Python’s most commonly used libraries and expose you to a lot of idiomatic code , which is always a plus.&lt;/p&gt;

&lt;p&gt;Here’s a link to &lt;em&gt;&lt;a href=&quot;https://amzn.to/2DDhIUH&quot;&gt;Data Science from Scratch on Amazon&lt;/a&gt;&lt;/em&gt;.&lt;/p&gt;

&lt;h2 id=&quot;springer-introduction-to-statistical-learning&quot;&gt;Springer: Introduction to Statistical Learning&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;/resources/post_image/intro-to-statistical-learning.webp&quot; alt=&quot;The book cover of Introduction to Statistical Learning&quot; loading=&quot;lazy&quot; style=&quot;width:40%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://amzn.to/2ZP93YS&quot;&gt;This book&lt;/a&gt; is the most comprehensive Machine Learning book I’ve found so far. I learned a lot from it, from Unsupervised Learning algorithms like &lt;a href=&quot;/wiki/clustering&quot;&gt;K-Means Clustering&lt;/a&gt;, to Supervised Learning ones like XGBoost.&lt;/p&gt;

&lt;p&gt;The first chapters may feel a bit too introductory if you’re already working in this field (at least that was my experience). However, they also sum up many things you may not have learned in such an organized way before.&lt;/p&gt;

&lt;p&gt;The later chapters are, however, where I think this book really shines. Its explanation of random forests, boosted trees and support vector machines are spot on.&lt;/p&gt;

&lt;p&gt;Here are some of the topics you can learn from &lt;em&gt;Introduction to Statistical Learning&lt;/em&gt;:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Regression and Supervised Learning Algorithms: from Linear Regression and SVMs to tree-based methods.&lt;/li&gt;
  &lt;li&gt;Unsupervised Learning techniques: especially Clustering, including the K-Means algorithm.&lt;/li&gt;
  &lt;li&gt;Sampling methods, and other general Machine Learning core concepts.&lt;/li&gt;
  &lt;li&gt;The meaning, advantages and disadvantages of metrics such as accuracy, recall, precision , etc.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I think this book has been my best read so far this year, and it’s made me into a more round up Data Scientist. I recommend it if you have a bit more experience, but want to polish your edges. It is also an excellent reference book to keep on your shelf.&lt;/p&gt;

&lt;p&gt;It also shows everything’s implementation in R, which I didn’t find particularly useful, but it didn’t hurt. You’ll probably import most of this code from SciKit learn anyway.&lt;/p&gt;

&lt;p&gt;As before, here’s a link to &lt;a href=&quot;https://amzn.to/2ZP93YS&quot;&gt;Springer’s Introduction to Statistical Learning on Amazon&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;deep-learning-by-goodfellow-bengio-et-al&quot;&gt;Deep Learning by Goodfellow, Bengio et al.&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;/resources/post_image/deep-learning.jpeg&quot; alt=&quot;&quot; loading=&quot;lazy&quot; style=&quot;width:40%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://amzn.to/2J0WjZe&quot;&gt;This book&lt;/a&gt; blows my mind every time I open it. I’ll be the first to admit I haven’t really read it from start to finish. Yet.
The only reason it’s the last one in the list is because of its narrow scope : Artificial Neural Networks or Deep Learning.&lt;/p&gt;

&lt;p&gt;However its first chapters, with an overview of Deep Learning’s precursors and what makes it different, and then the explanation of how Deep Learning works , are marvelous.&lt;/p&gt;

&lt;p&gt;It even starts off by explaining everything you need to know before studying deep learning , with whole chapters dedicated to linear algebra , probability and information theory , and numerical computation methods.&lt;/p&gt;

&lt;p&gt;The next chapters, which I’ve only partially read, serve as an awesome reference whenever you need to dive deeper into a particular Neural Network architecture.&lt;/p&gt;

&lt;p&gt;They include in-depth explanations of Convolutional Neural Networks and Recurrent Neural Networks, along with many regularization or optimization methods.&lt;/p&gt;

&lt;p&gt;The third and last section, which revolves around cutting-edge technology , explains Generative models , &lt;a href=&quot;https://medium.com/towards-data-science/autoencoders-deep-learning-with-tensorflows-eager-api-data-stuff-378318784ae&quot;&gt;Autoencoders&lt;/a&gt; and many other interesting algorithms. Adding them to your own toolkit will probably give you a great boost.&lt;/p&gt;

&lt;p&gt;The authors of this book are the rock stars of Machine Learning right now. One of them even &lt;a href=&quot;https://venturebeat.com/2019/03/27/geoffrey-hinton-yann-lecun-and-yoshua-bengio-honored-with-the-turing-award/&quot;&gt;won a Turing award&lt;/a&gt; recently, so I can’t think of better people to teach this subject.&lt;/p&gt;

&lt;p&gt;Here’s an &lt;a href=&quot;https://amzn.to/2J0WjZe&quot;&gt;Amazon link&lt;/a&gt; if you’re interested in the &lt;em&gt;Deep Learning&lt;/em&gt; book.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;I went from a broad, introductory book to an advanced, specific one.&lt;/p&gt;

&lt;p&gt;Each of these Machine Learning books has had a profound impact in my career and, to some degree, the way I see the world.&lt;/p&gt;

&lt;p&gt;I really hope at least some of them will have the same positive impact on your life.&lt;/p&gt;

&lt;p&gt;We can also discuss them on &lt;a href=&quot;http://twitter.com/strikingloo&quot;&gt;Twitter&lt;/a&gt;, &lt;a href=&quot;http://medium.com/@strikingloo&quot;&gt;Medium&lt;/a&gt; of &lt;a href=&quot;http://dev.to/strikingloo&quot;&gt;dev.to&lt;/a&gt; if you’re interested. I want to hear your opinions.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;(small disclaimer: all of these links are Amazon affiliate links, which means I get a small commission if you buy the books. However, I’ll only review books I’ve actually read, and have genuinely recommended to people in real life)&lt;/em&gt;&lt;/p&gt;

</content>
		</entry>
	
		<entry>
			<title>FuzzyWuzzy: How to Measure String Distance in Python</title>
			<link href="http://strikingloo.github.io/fuzzywuzzy-python-string-distance"/>
			<updated>2019-04-15T00:00:00-03:00</updated>
			<id>http://strikingloo.github.io/fuzzywuzzy-python-string-distance</id>
			<content type="html">&lt;p&gt;&lt;em&gt;Note: I originally wrote this article on my first blog, so it is not as polished as newer things.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Python’s &lt;em&gt;FuzzyWuzzy&lt;/em&gt; library is used for measuring the similarity between two strings. Here’s how you can start using it too.&lt;/p&gt;

&lt;p&gt;Sometimes, we need to see whether two strings are the same. When comparing an entered password’s hash to the one stored in your login database, ‘similarity’ just won’t cut it.&lt;/p&gt;

&lt;p&gt;Other times, however, things can get a bit… &lt;em&gt;fuzzier&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;If my customer’s name is &lt;em&gt;Albert Thompson&lt;/em&gt;, but he pays with a Credit Card under the name of &lt;em&gt;Albert G. Thompson&lt;/em&gt;, should I be calling the police to report fraud? Should “The Lord of the Rings II: The Two Towers” and “The Lord of the Rings 2: the 2 Towers” be treated as two completely separate books by a website? Are Austria and Australia really two different countries?&lt;/p&gt;

&lt;p&gt;Okay, I may have gotten carried away with that last one, but you get the idea.&lt;/p&gt;

&lt;h2 id=&quot;string-distance-measures&quot;&gt;String distance measures&lt;/h2&gt;

&lt;p&gt;What we want is some function that measures how similar two strings are, but is robust to small changes. This problem is as common as it sounds: scientists have been coming up with solutions to it for a long while.&lt;/p&gt;

&lt;h3 id=&quot;jaccard-distance-a-first-approach&quot;&gt;Jaccard Distance: a first approach&lt;/h3&gt;

&lt;p&gt;One of the most intuitive ones is the Jaccard distance. It can be generalized to a distance measure for any two sets. It has the following formula:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/resources/post_image/fuzzywuzzy1.svg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;That is, how many elements are on either set, but not shared by both, divided by the total count of distinct elements.&lt;/p&gt;

&lt;p&gt;For instance, given the strings “Albert” and “Alberto”, it will report a similarity of 85.7%, since they share 6 letters out of a total of 7.&lt;/p&gt;

&lt;p&gt;However, this is not a measure specifically tailored for strings.&lt;/p&gt;

&lt;p&gt;It will fail in many use-cases, since it doesn’t really take ordering into account. For example two anagrams, like “rail safety” and “fairy tales”, will always have a 100% match, even if those strings are quite different.&lt;/p&gt;

&lt;h3 id=&quot;levenshtein-distance&quot;&gt;Levenshtein Distance&lt;/h3&gt;

&lt;p&gt;Invented by the Russian Scientist Vladimir Levenshtein in the ’60s, this measure is a bit more intuitive: it counts how many substitutions are needed, given a string &lt;em&gt;u&lt;/em&gt;, to transform it into &lt;em&gt;v&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;For this method, a substitution is defined as:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Erasing a character.&lt;/li&gt;
  &lt;li&gt;Adding one.&lt;/li&gt;
  &lt;li&gt;Replacing a character with another one.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The minimum amount of these operations that need to be done to &lt;em&gt;u&lt;/em&gt; in order to turn it into &lt;em&gt;v,&lt;/em&gt; correspond to the Levenshtein distance between those two strings.&lt;/p&gt;

&lt;p&gt;It can be obtained recursively with this formula:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/resources/post_image/fuzzywuzzy2.svg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Where &lt;em&gt;i&lt;/em&gt; and &lt;em&gt;j&lt;/em&gt; are indexes to the last character of the substring we’ll be comparing. The second term in the last expression is equal to 1 if those characters are different, and 0 if they’re the same.&lt;/p&gt;

&lt;p&gt;This is the measure Python’s FuzzyWuzzy library uses.&lt;/p&gt;

&lt;h2 id=&quot;using-fuzzywuzzy-in-python&quot;&gt;Using FuzzyWuzzy in Python&lt;/h2&gt;

&lt;p&gt;To obtain the similarity ratio between two strings, all we have to do is this:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;from fuzzywuzzy import fuzz
similarity = fuzz.ratio(&quot;hello&quot;,&quot;world&quot;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;You probably noticed I said ratio. The ratio method will always return a number between 0 and 100 (yeah, I’d have preferred it to be between 0 and 1, or call it a percentage, but to each their own).&lt;/p&gt;

&lt;p&gt;It can be shown that the Levenshtein distance is at most the length of the longest string: replace all characters in the shorter one with the first part of the longer one, and then add the remaining ones.&lt;/p&gt;

&lt;p&gt;That’s how we can normalize the distance to return a ratio, so that the number won’t fluctuate enormously given inputs with different sizes.&lt;/p&gt;

&lt;p&gt;This solves some of the previously mentioned problems:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;fuzz.ratio(&quot;Albert Thompson&quot;, &quot;Albert G. Thompson&quot;) #91%

fuzz.ratio(&quot;The Lord of the Rings II: The Two Towers&quot;,
 &quot;The Lord of the Rings 2: the 2 Towers&quot;) #88%
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Even if it may bring a few new ones:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;#88% for two different countries
fuzz.ratio(&quot;Austria&quot;,&quot;Australia&quot;)

#57% but it&apos;s the same country
fuzz.ratio(&quot;Czechia&quot;,&quot;Czech Republic&quot;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;other-fuzzywuzzy-methods&quot;&gt;Other FuzzyWuzzy methods&lt;/h2&gt;

&lt;p&gt;Python’s FuzzyWuzzy library provides us not only with the vanilla Levenshtein distance, but also with a few other methods we can make use of.&lt;/p&gt;

&lt;h3 id=&quot;partial_ratio&quot;&gt;partial_ratio&lt;/h3&gt;

&lt;p&gt;The &lt;em&gt;partial_ratio&lt;/em&gt; method calculates the FuzzyWuzzy ratio for all substrings of the longer string with the length of the shorter one, and then returns the highest match.&lt;/p&gt;

&lt;p&gt;For instance,&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;fuzz.partial_ratio(&quot;abc&quot;,&quot;a&quot;) == 
min([fuzz.ratio( char, &quot;a&quot;) for char in &quot;abc&quot;])
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This has some interesting effects:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;fuzz.partial_ratio(&quot;Thomas and His Friends&quot;, &quot;Thomas&quot;) #100%
fuzz.partial_ratio(&quot;Batman vs Superman&quot;, &quot;Batman&quot;) #100%
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Effectively, the partial_ratio method could be a fuzzy replacement to the &lt;em&gt;contains&lt;/em&gt; string method, just as the regular ratio may replace the &lt;em&gt;equals&lt;/em&gt; method.&lt;/p&gt;

&lt;p&gt;However, it will fail for strings that are similar, but whose words appear in a different order. Even a slight order change will break it.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;#72% with basically the same idea
fuzz.partial_ratio(&quot;Peanut Butter and Jelly&quot;,
 &quot;Jelly and Peanut Butter&quot;) 

#86% with a random (carefully selected) string
fuzz.partial_ratio(&quot;Peanut Butter and Jelly&quot;,
 &quot;Otter and Hell&quot;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;token_sort_ratio&quot;&gt;token_sort_ratio&lt;/h3&gt;

&lt;p&gt;The Token Sort Ratio divides both strings into words, then joins those again alphanumerically, before calling the regular ratio on them.&lt;/p&gt;

&lt;p&gt;This means:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;fuzz.partial_ratio(&quot;Batman vs Superman&quot;, 
 &quot;Superman vs Batman&quot;) #100%

fuzz.partial_ratio(&quot;a b c&quot;, &quot;c b a&quot;) #100%
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;token_set_ratio&quot;&gt;token_set_ratio&lt;/h3&gt;

&lt;p&gt;The Token Set Ratio separates each string into words, turns both lists into sets (discarding repeated words) and then sorts those before doing the ratio.&lt;/p&gt;

&lt;p&gt;This way not only do we rule out shared words, we also account for repetitions.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;fuzz.token_set_ratio(&quot;fun&quot;,&quot;fun fun fun&quot;) #100%

fuzz.token_set_ratio(&quot;Lord the Rings of&quot;,
 &quot;Lord of the Rings&quot;) #100%
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;conclusions&quot;&gt;Conclusions&lt;/h2&gt;

&lt;p&gt;Python’s FuzzyWuzzy library can be a useful tool to have under your belt. Both for customer’s names matching, or acting as a poor man’s word embedding, it can save you a lot of trouble or help with your Machine Learning model’s feature engineering.&lt;/p&gt;

&lt;p&gt;However, since it requires preprocessing (like turning both strings to lowercase) and doesn’t take synonyms into account, it may not be the best solution for cases where actual NLP or &lt;a href=&quot;/wiki/clustering&quot;&gt;Clustering methods&lt;/a&gt; may be needed.&lt;/p&gt;

&lt;p&gt;I hope you’ve found this article helpful, and let me know if you find another use for FuzzyWuzzy in your job.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Follow me on&lt;/em&gt; &lt;a href=&quot;http://www.twitter.com/strikingloo&quot;&gt;&lt;em&gt;Twitter&lt;/em&gt;&lt;/a&gt; to stay up to date with more Python tutorials, tips and tricks._&lt;/p&gt;

&lt;p&gt;&lt;em&gt;If you found this article useful, please consider sharing it on social media.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;For other programming notes and articles, visit the &lt;a href=&quot;/tagged?q=programming&quot;&gt;programming tag page&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
</content>
		</entry>
	
		<entry>
			<title>Vim: How to Start Using The Text Editor for Developers</title>
			<link href="http://strikingloo.github.io/vim-basics"/>
			<updated>2019-04-14T00:00:00-03:00</updated>
			<id>http://strikingloo.github.io/vim-basics</id>
			<content type="html">&lt;p&gt;&lt;em&gt;Note: I originally wrote this article on my first blog, so it may not be as polished as newer things.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;To some, Vim is a beautiful relic from the past. To others, it’s that annoying thing you have to escape whenever you need to write a message for a merge commit.&lt;/p&gt;

&lt;p&gt;Let me introduce you to this picturesque text editor and its wonders, and show you why we’re still using it &lt;a href=&quot;https://en.wikipedia.org/wiki/Vim_%28text_editor%29&quot;&gt;26 years after its first release.&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;First a small piece of advice: this text editor shines the most when used for editing structured text files (like JSONs) or code, not so much when writing new files from scratch (though I do use it for that a lot too). Also, many people like adding a vim plugin to their IDE instead of going full vim, so they get the commands and the nice UI. Your mileage may vary.&lt;/p&gt;

&lt;p&gt;Now fire up your terminal on your working directory of choice, and type&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;vim sample.txt
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;so that we can start this tutorial.&lt;/p&gt;

&lt;h2 id=&quot;first-steps-navigating-editing-and-exiting&quot;&gt;First steps: navigating, editing and exiting.&lt;/h2&gt;

&lt;p&gt;Whenever you open Vim on a file, there are three basic things you may want to do:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Reading the file’s contents&lt;/li&gt;
  &lt;li&gt;Writing on the file&lt;/li&gt;
  &lt;li&gt;Quitting the program&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;navigation&quot;&gt;Navigation&lt;/h3&gt;

&lt;p&gt;To navigate a file on Vim, use the letters &lt;em&gt;h,j,k, _and&lt;/em&gt; l.&lt;em&gt;These commands are called&lt;/em&gt; motions_, as they move the cursor.&lt;/p&gt;

&lt;p&gt;The keys &lt;em&gt;h&lt;/em&gt; and &lt;em&gt;l&lt;/em&gt; will move your cursor horizontally (one character at a time), while &lt;em&gt;j&lt;/em&gt; and &lt;em&gt;k&lt;/em&gt; move vertically (one line at a time). If you put your hand on them, the layout sorta makes sense.&lt;/p&gt;

&lt;p&gt;Some people have trouble remembering which key goes up and which goes down. Pro tip: &lt;em&gt;j&lt;/em&gt; sorta looks like a downwards pointing arrow.&lt;/p&gt;

&lt;p&gt;As a general note, it is considered bad practice -even though it’s possible- to use the arrow keys for moving in Vim. Get used to using &lt;em&gt;hjkl&lt;/em&gt;, and I promise you’ll see a significant boost in speed.&lt;/p&gt;

&lt;h3 id=&quot;repetition&quot;&gt;Repetition&lt;/h3&gt;

&lt;p&gt;Once you’re confident moving through a file one character or line at a time, try pressing a number (any number, it could have many digits) before moving. You’ll jump as many times as the number you entered.&lt;/p&gt;

&lt;p&gt;This is a powerful concept in Vim: Repetition. Have you ever found yourself editing a text file and doing the same thing over and over? Especially something mundane, like deleting quotes and replacing them with commas? Vim’s got you covered: Just do the thing once, and press . to repeat it. Enter a number and press . again if you want to repeat it as many times.&lt;/p&gt;

&lt;h3 id=&quot;edition&quot;&gt;Edition&lt;/h3&gt;

&lt;p&gt;Moving around in a text file and reading what’s in it is good and all, but what if we need to change some of its contents? Do not despair, editing a file is as easy as pressing the &lt;em&gt;i&lt;/em&gt; key. That will move you from &lt;em&gt;normal mode&lt;/em&gt; into &lt;em&gt;editing mode&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Most Vim commands will only be available on normal mode, and entering editing mode is usually frowned upon, and should be avoided as much as possible. However, when entering editing mode, Vim will be like any other Text Editor (with Syntax Highlighting on), making its functionalities a superset of those available in your typical notepad.&lt;/p&gt;

&lt;p&gt;To exit editing mode, press the &lt;em&gt;ESC&lt;/em&gt; key.&lt;/p&gt;

&lt;h3 id=&quot;quitting&quot;&gt;Quitting&lt;/h3&gt;

&lt;p&gt;To quit Vim, enter normal mode, and press &lt;em&gt;:wq&lt;/em&gt; if you want to save your changes (write and quit), or &lt;em&gt;q!&lt;/em&gt; if you want to leave without saving.&lt;/p&gt;

&lt;h2 id=&quot;more-commands-some-useful-features&quot;&gt;More commands: Some Useful Features.&lt;/h2&gt;

&lt;p&gt;Editing files from the terminal might make you look like a cool hacker or something, but why should we use this text-based program instead of good old Sublime Text? The answer is &lt;em&gt;commands&lt;/em&gt;.&lt;/p&gt;

&lt;h3 id=&quot;a-thousand-ways-of-deleting-text&quot;&gt;A thousand ways of deleting text&lt;/h3&gt;

&lt;p&gt;Want to delete part of your file? You &lt;em&gt;could&lt;/em&gt; enter editing mode and press backspace once for every character. It doesn’t really beat using Sublime and pressing &lt;em&gt;ctrl+shift+left&lt;/em&gt; to select a whole word before deleting it.&lt;/p&gt;

&lt;p&gt;If you really want to harness the power of Vim, you should try pressing the_d_key. Pressed once, it won’t do anything. But it’s not sitting idle, it is actually expectant, waiting for an order. Pass it a motion like the ones we learned today (&lt;em&gt;l, 5j&lt;/em&gt;, whichever you feel like really) and it will gobble those characters up. For instance, &lt;em&gt;dNl&lt;/em&gt; for any number &lt;em&gt;N&lt;/em&gt;, will delete the following &lt;em&gt;N&lt;/em&gt; letters from the cursor.&lt;/p&gt;

&lt;h3 id=&quot;introducing-new-motions&quot;&gt;Introducing new motions&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;e&lt;/em&gt; : Moves the cursor to the end of the current word (defined as a concatenation of letters, numbers and underscores).&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;w&lt;/em&gt; : Moves it to the beginning of the next word.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;So if I have this text:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Hello there, general.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;And my cursor is standing on the H. When I press &lt;em&gt;de&lt;/em&gt; in normal mode, the line will end up looking like this:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;there, general.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;While using &lt;em&gt;dw&lt;/em&gt; will leave it like this:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;there, general
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Notice how in the second example, there’s no space before ‘there’.&lt;/p&gt;

&lt;p&gt;We could then press &lt;em&gt;i&lt;/em&gt; to insert some replacement word after deleting ‘Hello’. Luckily, there’s an even more fluid way of doing that: the &lt;em&gt;c&lt;/em&gt; command (for &lt;em&gt;change&lt;/em&gt;). Pressing &lt;em&gt;c&lt;/em&gt; and a motion is exactly equivalent to pressing &lt;em&gt;d+motion+i&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;This is starting to look nicer, but it still doesn’t beat pressing shift+home/end and deleting a whole line in a few keystrokes, right? Well I see that, and raise you to the &lt;em&gt;$&lt;/em&gt; and &lt;em&gt;0&lt;/em&gt; motions.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;0:&lt;/strong&gt; moves the cursor to the first character in the current line.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;$:&lt;/strong&gt; moves the cursor to the last character in the line.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;There’s an even faster way of deleting the whole current line though: &lt;em&gt;dd&lt;/em&gt;. And if you want to delete many lines? &lt;em&gt;dxd&lt;/em&gt; deletes the x following lines.&lt;/p&gt;

&lt;h3 id=&quot;generally-useful-vim-commands&quot;&gt;Generally useful Vim commands&lt;/h3&gt;

&lt;p&gt;By now, the usefulness of vim when editing code should start to become apparent.&lt;/p&gt;

&lt;p&gt;A few other commands you may want to check out are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;o&lt;/em&gt; and &lt;em&gt;O&lt;/em&gt;: create a new line above or below the current one, respectively, and enter editing mode.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;v&lt;/em&gt; : enter &lt;em&gt;visual mode&lt;/em&gt; to select text to which you may then apply more commands.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;y&lt;/em&gt; or &lt;em&gt;Y&lt;/em&gt;: &lt;em&gt;yank (copy)&lt;/em&gt; the selected text, or the current line, respectively.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;p&lt;/em&gt; : &lt;em&gt;put&lt;/em&gt; the &lt;em&gt;yanked&lt;/em&gt; content. Notice that yanking will move text to a special Vim reserved buffer, and not to your usual clipboard. This way, you can effectively manage two different clipboards. One you can paste from with &lt;em&gt;ctrl+shift+v&lt;/em&gt; as usual (in editing mode), and the other with &lt;em&gt;p&lt;/em&gt; (in normal mode).&lt;/li&gt;
  &lt;li&gt;* : find the next occurrence of the current word.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;When writing software, I find myself duplicating lines to change a few words quite often, so I think &lt;em&gt;Yp&lt;/em&gt; is an amazing command.&lt;/p&gt;

&lt;h3 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h3&gt;

&lt;p&gt;I’ve barely scratched the surface with this introduction, but I hope I’ll at least have persuaded you into trying Vim out for yourself. It may not replace an IDE if you’re coding in Java or C++, especially if you’re using frameworks and auto-complete is helping you. But when coding in C or Python, I usually pick it as my editor of choice. And sometimes when I need to transform a string quickly, editing it from Vim is faster than coding a script in Bash or Python.&lt;/p&gt;

&lt;p&gt;I also encourage you to try the software on your own, and run the &lt;em&gt;vimtutor&lt;/em&gt; program from your shell (it usually comes preinstalled with Linux and on Macs). If you want to really learn how to optimize your Vim use after going through vimtutor, this geeky, awesome site &lt;a href=&quot;http://vimgolf.com/&quot;&gt;vim golf&lt;/a&gt; may be of interest to you as well.&lt;/p&gt;

&lt;p&gt;I hope you found this article useful or interesting, and as usual any feedback will be welcome, whether anything I said was plain wrong, or you actually liked some part of this tutorial.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Follow me on &lt;a href=&quot;http://www.twitter.com/strikingloo&quot;&gt;Twitter&lt;/a&gt; for more Programming tutorials, tips and tricks, and if you liked it please consider tweeting my article.&lt;/em&gt;&lt;/p&gt;
</content>
		</entry>
	
		<entry>
			<title>K-Means Clustering for Magic: the Gathering Decks - Card Recommendation</title>
			<link href="http://strikingloo.github.io/k-means-clustering-magic-the-gathering"/>
			<updated>2019-04-14T00:00:00-03:00</updated>
			<id>http://strikingloo.github.io/k-means-clustering-magic-the-gathering</id>
			<content type="html">&lt;p&gt;Unsupervised Learning has been called the closest thing we have to “actual” Artificial Intelligence, in the sense of General AI, with K-Means Clustering one of its simplest, but most powerful applications.&lt;/p&gt;

&lt;p&gt;I am not here to discuss whether those claims are true or not. I will however state, that I am often amazed by how well unsupervised learning techniques, even the most rudimentary, capture patterns in the data that I would expect only people to find.&lt;/p&gt;

&lt;p&gt;Today we’ll apply unsupervised learning on a Dataset I gathered myself. It’s a database of professional Magic: The Gathering decks that I crawled from &lt;a href=&quot;https://mtgtop8.com/&quot;&gt;mtgtop8.com,&lt;/a&gt; an awesome website if you’re into Magic: the Gathering.&lt;/p&gt;

&lt;p&gt;I scraped the MtgTop8 data from a few years of tournaments, and they’re all available to be consumed in &lt;a href=&quot;https://github.com/StrikingLoo/mtgProject&quot;&gt;this GitHub repository&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;If you’re not into the game, or even if you’ve never even played it, &lt;strong&gt;don’t worry&lt;/strong&gt;: it won’t get in the way too much, as I will just explain the theoretical side of K-means Clustering and show you how to apply it using Dask, the parallel computing Python framework. If you are into the game, then you’re gonna love the examples.&lt;/p&gt;

&lt;h2 id=&quot;k-means-clustering&quot;&gt;K-Means Clustering&lt;/h2&gt;

&lt;p&gt;The algorithm we will look into today is called ‘K-means clustering’. It provides a way to characterize and categorize data if we don’t really know how to separate it before hand.&lt;/p&gt;

&lt;h3 id=&quot;why-do-we-need-unsupervised-learning&quot;&gt;Why do we need Unsupervised Learning?&lt;/h3&gt;

&lt;p&gt;What do I mean by Unsupervised Learning? Suppose you had a set of pictures of cats and dogs. You could train a supervised Machine Learning model to classify the pictures into either category.&lt;/p&gt;

&lt;p&gt;However, imagine you have a big, complex dataset of things you don’t know a lot about. For instance, you could have data about the light spectrum produced by different planets, and be looking for a way to group them into categories.&lt;/p&gt;

&lt;p&gt;For another example, you could have loads of genetic data from many different organisms, and wish to define which ones belong to the same genus or families in an intuitive way.&lt;/p&gt;

&lt;p&gt;Or, in our case, we could have 777 different Magic: The Gathering decks, using over 600 different cards (yeah, the professional meta is not &lt;em&gt;that&lt;/em&gt; diverse), and want to train a machine learning model so that it understands which cards work well together and which ones don’t.&lt;/p&gt;

&lt;p&gt;Now imagine you had to do this task, and you had no idea how to play this game. Wouldn’t it be nice if someone had invented an algorithm to cluster data together if they look similar, without you having to provide a definition for ‘similar’? That’s what clustering, and k-means clustering in particular, are all about.&lt;/p&gt;

&lt;p&gt;Now that’s done, I hope you’re motivated, because it’s time to get our hands dirty with some theory.&lt;/p&gt;

&lt;h3 id=&quot;how-does-k-means-clustering-work&quot;&gt;How does K-Means Clustering work?&lt;/h3&gt;

&lt;p&gt;K-Means Clustering receives a single hyperparameter: &lt;em&gt;k&lt;/em&gt;, which specifies how many clusters we want to categorize our data into.&lt;/p&gt;

&lt;p&gt;The clusters won’t necessarily all have the same quantity of instances. However, they should each characterize a specific subset of our data. How will we achieve this? Let’s find out.&lt;/p&gt;

&lt;p&gt;First, the input for this algorithm needs to be a set of vectors. That is, all your features should be numerical, and in the same order. If you had any categorical features, my advice would be to use one-hot encode: convert each categorical variable into a vector of n-elements: one for each possible category, all set to 0 except the one for the given category.&lt;/p&gt;

&lt;p&gt;What the algorithm will do is initiate &lt;em&gt;k&lt;/em&gt; random ‘centroids’ -points in the space defined by the dimensions of the dataset’s elements-, and then it will:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Assign each element to the centroid closest to it.&lt;/li&gt;
  &lt;li&gt;Remap the centroid to the point lying on the average of all the elements assigned to it.&lt;/li&gt;
  &lt;li&gt;Repeat steps 1 and 2, until convergence (or a stopping condition, such as doing N iterations for a given N) is met. Convergence is usually defined as no instances changing their cluster label from one iteration to the next.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In the end, each element will have been assigned to one of &lt;em&gt;k&lt;/em&gt; clusters, such that the elements in the same cluster all lie closest to it.&lt;/p&gt;

&lt;p&gt;Here is a gif showing the algorithm in action, courtesy of Wikimedia Commons.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;resources/post_image/k-means-convergence.gif&quot; alt=&quot;K-means convergence, Chire, CC BY-SA 4.0 &amp;lt;https://creativecommons.org/licenses/by-sa/4.0&amp;gt;, via Wikimedia Commons&quot; loading=&quot;lazy&quot; style=&quot;width:60%;height:60%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The crosses represent the position of each cluster’s centroid, and you can see how they start moving until they become stable. Notice also how they move more initially, and less over time.&lt;/p&gt;

&lt;h3 id=&quot;applications-for-k-means-clustering&quot;&gt;Applications for K-means clustering&lt;/h3&gt;

&lt;p&gt;Like many other unsupervised learning algorithms, K-means clustering can work wonders if used as a way to generate inputs for a supervised Machine Learning algorithm (for instance, a classifier).&lt;/p&gt;

&lt;p&gt;The inputs could be a one-hot encode of which cluster a given instance falls into, or the k distances to each cluster’s centroid.&lt;/p&gt;

&lt;p&gt;For this project however, what we’ll be developing will be a (somewhat rudimentary) recommender system which will, given an instance, return elements appearing on the same cluster.&lt;/p&gt;

&lt;h2 id=&quot;using-dasks-k-means-clustering-in-python&quot;&gt;Using Dask’s K-means Clustering in Python&lt;/h2&gt;

&lt;p&gt;Having defined the concepts for this project, let’s now begin the practical part. The code is available in the Jupyter Notebook on &lt;a href=&quot;https://github.com/StrikingLoo/MtGRecommender/&quot;&gt;this repository&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;processing-the-data&quot;&gt;Processing the Data&lt;/h3&gt;

&lt;p&gt;I stored the decks following this format:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;N card name
M another card name
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;in 777 different &lt;em&gt;.txt&lt;/em&gt; files, where each line refers to a card, and the digits before the first space are the number of apparitions for that card in the deck.&lt;/p&gt;

&lt;p&gt;In order to transform them into a more manageable format -I’ll be using a list of tuples (Int, String) for each deck, each tuple a card-, this is what we’ll do:&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/StrikingLoo/45f3c1bd30753876dcb9235118e1eeb1.js&quot;&gt;&lt;/script&gt;

&lt;p&gt;This is what a deck looks like now.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[(4, &apos;Ancient Ziggurat&apos;), (4, &apos;Cavern of Souls&apos;), (4, &apos;Horizon Canopy&apos;),
 (1, &apos;Plains&apos;), (2, &apos;Seachrome Coast&apos;), (4, &apos;Unclaimed Territory&apos;),
  (4, &apos;Champion of the Parish&apos;), (1, &apos;Dark Confidant&apos;) ...]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Where each tuple represents a card (yes, those are real card names), and how many times it appears.&lt;/p&gt;

&lt;p&gt;Since we want to map each deck to a vector, it seems intuitive to just&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Turn them into a list, with one element for each different card in the whole MtgTop8 dataset.&lt;/li&gt;
  &lt;li&gt;Set each component to the number of apparitions for the corresponding card (with all the components corresponding to cards that do not appear in the deck set to 0).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To do that, let’s get all the different cards that appear in all the decks.&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/StrikingLoo/2145782d9c6509f19dc68aa38fc54ed8.js&quot;&gt;&lt;/script&gt;

&lt;p&gt;And now let’s use our newfound knowledge about card names to turn all decks into beautiful, consumable vectors.&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/StrikingLoo/7e1f38cf47fde3c3cedbed6a7eb2c176.js&quot;&gt;&lt;/script&gt;

&lt;p&gt;Now all our decks can be easily fed into Dask’s K-Means Clustering algorithm, and we can play with the output.&lt;/p&gt;

&lt;p&gt;We could have just used ‘binary’ vectors, and set the components to 1 if the card appears in the deck, and 0 if it doesn’t. We could also try that later and see if we get good results too.&lt;/p&gt;

&lt;h3 id=&quot;applying-k-means-clustering&quot;&gt;Applying K-means Clustering&lt;/h3&gt;

&lt;p&gt;Now that our data is all neatly mapped to the vector space, actually using Dask’s K-means Clustering is pretty simple.&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/StrikingLoo/251b55817a4777a37ccbc75b8faa0f18.js&quot;&gt;&lt;/script&gt;

&lt;p&gt;Where the most important part is the &lt;em&gt;n_clusters&lt;/em&gt; argument, which I kind of arbitrarily set to 8.&lt;/p&gt;

&lt;p&gt;In real life, you may want to experiment with different values. For this particular case, I know MtG has 5 different ‘colors’ for cards. To prevent the algorithm from just clustering the cards for their colors (which it didn’t do at all anyway), I chose a number bigger than 5.&lt;/p&gt;

&lt;p&gt;The algorithm returns the labels as a Dask Array. I may do an article on how to use those later, but right now I didn’t want to deal with all that. Also, the MtgTop8 dataset is small enough for that to not matter that much, so I decided to convert it back to a list of integers. Sue me.&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/StrikingLoo/40004ecf4733dcfcfdb1e0e78be966ce.js&quot;&gt;&lt;/script&gt;

&lt;h3 id=&quot;exploratory-analysis-lets-see-what-we-got&quot;&gt;Exploratory Analysis: Let’s see what we got&lt;/h3&gt;

&lt;p&gt;At first I wanted to check if the results made any sense. This was my first time using K-means Clustering on this dataset, and I wanted to make sure it had learned something valuable. So I just checked which cards were most frequent in the decks from each cluster. The results were, at least to me, astounding. Here’s what I did to check.&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/StrikingLoo/f8df3b441fdefb7808474015b54c4753.js&quot;&gt;&lt;/script&gt;

&lt;p&gt;If you’re interested in the results, here’s a &lt;a href=&quot;https://towardsdatascience.com/magic-the-gathering-meets-data-science-2a0367c724fe&quot;&gt;separate article about them&lt;/a&gt;. I just didn’t want to mix my M:tG findings with this tutorial so that readers who are into Data Science but not into the game won’t be bored.
In case you’re interested, I later wrote about a completely different application for K-Means Clustering: &lt;a href=&quot;https://dev.to/strikingloo/k-means-clustering-with-dask-image-filters-for-pictures-of-kittens-ip7&quot;&gt;image filters&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I also strongly encourage you to download the notebook from the GitHub project and play with it, it’s honest fun.&lt;/p&gt;

&lt;h3 id=&quot;card-recommendations-using-k-means-clustering&quot;&gt;Card Recommendations using K-Means Clustering&lt;/h3&gt;

&lt;p&gt;Now we made that sanity check, we can proceed with the actual application for all the labels we generated.&lt;/p&gt;

&lt;p&gt;There are many ways we could have approached the recommendation problem: given a card, suggest other cards that go well with it, without using any data about the cards except which decks they appear in (that is, no cheating and asking for more data about the cards like color, price, or an expert’s opinion).&lt;/p&gt;

&lt;p&gt;Think for a moment, how would you use the clusters data to generate the recommendations? I’m sure you could come up with a few ideas.&lt;/p&gt;

&lt;p&gt;If what you came up with is not what I’m about to do, please tell me. Creativity is more fun if it’s a team effort, and I really want to see what my dear readers come up with.&lt;/p&gt;

&lt;p&gt;Finally, here’s what I did:
&lt;script src=&quot;https://gist.github.com/StrikingLoo/d325dea2347f5f48cc0e4c16c2ef5cec.js&quot;&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;As you can see, I omit how many times a card appears on a given deck for this part, and just look at the relative number of apparitions for a card on a given cluster.&lt;/p&gt;

&lt;p&gt;I then return the cards with the most similar relative apparitions (defined by Euclidian distance).&lt;/p&gt;

&lt;p&gt;If you’re a Magic: The Gathering player, try out this code and see the results. It makes reasonably good (though kind of conservative) recommendations.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;K-Means clustering allowed us to approach a domain without really knowing a whole lot about it, and draw conclusions and even design a useful application around it.&lt;/p&gt;

&lt;p&gt;It let us do that by learning the underlying patterns in the data for us, only asking that we gave it the data in the correct format.&lt;/p&gt;

&lt;p&gt;I encourage you to play with the code here, and try making your own recommendation’s system with a different Dataset, or solving some other problem. If you do, please show me your results. I wanna see what you come up with.&lt;/p&gt;

&lt;p&gt;In the future, I’d like to do this same analysis using non-professional decks. That way, I could make a recommendations engine for casual players (like me). I think it would be cool if it worked with almost any card, and not just 642.&lt;/p&gt;

&lt;p&gt;If you want to delve deeper into Unsupervised Learning, I can’t recommend &lt;a href=&quot;https://amzn.to/3RSmJwT&quot;&gt;Introduction to Statistical Learning&lt;/a&gt; enough. That’s the book I learned about K-Means Clustering from. It’s also the book thanks to which I finally understood Boosted Trees, though that’s a tale for another article.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Liked this article? Then please follow me on&lt;/em&gt; &lt;a href=&quot;https://www.twitter.com/strikingloo&quot;&gt;&lt;em&gt;Twitter&lt;/em&gt;&lt;/a&gt;. Let me know if you found the article helpful, or if any of it sounds wrong or doesn’t work._&lt;/p&gt;

&lt;p&gt;Happy coding.&lt;/p&gt;
</content>
		</entry>
	

</feed>
