<!DOCTYPE html>
	<html lang="en">
		<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0">
		<link rel="icon" type="image/svg+xml" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 width=%22256%22 height=%22256%22 viewBox=%220 0 100 100%22><rect width=%22100%22 height=%22100%22 rx=%2220%22 fill=%22%23d8eaeb%22></rect><text x=%2250%%22 y=%2250%%22 dominant-baseline=%22central%22 text-anchor=%22middle%22 font-size=%2293%22>üå≥</text></svg>" />
            <title>BERT Interpretability</title>
			<link rel="canonical" href="https://strikingloo.github.io/wiki/bert-interpretability">
			  <meta name="description" content="A paper on BERT interpretability from thegradient.pub, and my reflections on the lottery hypothesis and model distilling.">
  			<meta property="og:site_name" content="BERT Interpretability">

        	
        	<link rel="stylesheet" type="text/css" href="/css/post-min.css">
        	

			<script async src="https://www.googletagmanager.com/gtag/js?id=UA-52642322-5"></script>

			<script>
			  window.dataLayer = window.dataLayer || [];
			  function gtag(){dataLayer.push(arguments);}
			  gtag('js', new Date());

			  gtag('config', 'UA-52642322-5');
			</script>
			
			  <meta property="og:description" content="A paper on BERT interpretability from thegradient.pub, and my reflections on the lottery hypothesis and model distilling.">
			
	  		<meta property="og:locale" content="en_US">
	  		
			  <meta property="og:title" content="BERT Interpretability">
			  <meta property="og:type" content="article">
		  	  <meta property="article:published_time" content="2021-01-07T00:00:00-03:00">
		      <meta property="article:author" content="http://localhost:4000/">
		  	
		  	
  				<meta property="og:url" content="https://strikingloo.github.io/wiki/bert-interpretability">
  			
  			<meta content="index,follow" name="robots"><!-- All Search Engines -->
  			<meta content="index,follow" name="googlebot"><!-- Google Specific -->
  			<meta name="robots" content="max-image-preview:large">

		</head>
		<body>
			<div class="head-banner">
			<p>Strikingloo</p>
			<nav>
	    		<ul>
	        		<li><a href="/">Home</a></li>
		        	<li><a href="/about/">About</a></li>
	        		<li><a href="/wiki/">Wiki</a></li>
	        		<li><a href="/blog/">Blog </a></li>
	    		</ul>
			</nav>
			<div style="clear: both;"></div>
		    </div>

			<div class="container">
			
			<h1>BERT Interpretability</h1>
<p class="meta">07 Jan 2021 - importance: 6 </p>




<div class="post">
  <p>What follows are my notes from a <a href="https://thegradient.pub/when-bert-plays-the-lottery-all-tickets-are-winning">paper on BERT interpretability</a> from thegradient.pub.</p>

<p>It tests the lottery ticket hypothesis on BERT by iteratively pruning its ‚Äòleast important‚Äô parts, either structurally (head by head) or on a weight level (by removing those weights closer to 0, irrespective of their position). The pruning continues while the reduced model performs at least 90% as well as the original without extra fine-tuning.</p>

<p>The result is these smaller models can be fine-tuned to perform as well as the original while being a fraction of the size, for almost all tasks (8 out of 9 in their benchmarks).</p>

<p>I am not aware of similar studies run on, for instance, GPT-3 but it would be very interesting to see how much understanding of human language (plus all the reasoning, inference and creative tasks in which GPT achieves excellent zero-shot performance) a large language model retains after iterative pruning, and what the trade off between compression and natural language understanding becomes for a model in that, much bigger regime.</p>

<p>The lottery hypothesis could also help train <a href="/wiki/glide">diffusion models</a> that reach a similar performance to <a href="/DALL-E-2-prompt-guide">DALL-E 2</a> or Stable Diffusion while consuming a smaller amount of VRAM, potentially opening up the gates for mass user adoption (imagine a desktop app that runs DALL-E 2 locally).</p>

<h2 id="on-bert-interpretability">On BERT interpretability:</h2>

<blockquote>
  <p>The <strong>Lottery Ticket Hypothesis</strong> proposes that randomly initialized neural networks contain <strong>subnetworks</strong> that could be re-trained alone to <strong>reach (or exceed) the performance of the full model</strong> (<a href="https://thegradient.pub/when-bert-plays-the-lottery-all-tickets-are-winning/">Frankle and Carbin 2019</a>);</p>
</blockquote>

<p>‚ÄúGiven all that: if BERT is so overparameterized, could we make it more <strong>interpretable by pruning it down to its most essential components</strong>? If they are indeed crucial for the model‚Äôs performance, then their functions should tell us something about how the whole thing actually works. Under this approach, we would use <strong>pruning as a technique for model analysis rather than model compression</strong>.‚Äù</p>

<p>‚ÄúThe classical Lottery Ticket Hypothesis was mostly tested with <strong>unstructured pruning</strong>, specifically <strong>magnitude pruning (m-pruning)</strong> where the <strong>weights with the lowest magnitude are pruned irrespective of their position in the model</strong>. We iteratively prune 10% of the least magnitude weights across the entire fine-tuned model (except the embeddings) and evaluate on dev set, for as long as the performance of the pruned subnetwork is above 90% of the full model.‚Äù</p>

<p>‚ÄúWe also experiment with <strong>structured pruning (s-pruning)</strong> of entire components of BERT architecture based on their importance scores: specifically, we <strong>‚Äòremove‚Äô the least important self-attention heads and MLPs</strong> by applying a mask. In each iteration, we prune 10% of BERT heads and 1 MLP, for as long as the performance of the pruned subnetwork is above 90% of the full model. To determine which heads/MLPs to prune, we use a loss-based approximation: the importance scores proposed by Michel, Levy and Neubig (2019) for self-attention heads, which we extend to MLPs. Please see our paper and the original formulation for more details.‚Äù</p>

<h3 id="results">Results</h3>

<p>‚ÄúFor most GLUE tasks, the <strong>‚Äògood‚Äô subnetworks can be retrained to reach performance close to that of the full model, but so can randomly sampled subnetworks of the same size.</strong> This is good news for BERT compression (it‚Äôs a lottery you can‚Äôt lose), but bad news for interpretability.‚Äù</p>

<p>‚ÄúFor m-pruning, pruned and re-fine-tuned ‚Äúgood‚Äù subnetworks reach the full model performance in 8 out of 9 GLUE tasks (except for WNLI, where the model generally fails to learn). These results are consistent with concurrent work on magnitude-pruned BERT (<a href="https://thegradient.pub/when-bert-plays-the-lottery-all-tickets-are-winning/">Chen et al. 2020</a>). ‚Äú</p>

<p>The ‚Äúrandom‚Äù and ‚Äúbad‚Äù subnetworks also generally perform better when re-fine-tuned, but the ‚Äúbad‚Äù subnetworks are consistently worse than ‚Äúrandom‚Äù.</p>

<p>‚ÄúAll this means that <strong>we still have more questions than answers about how BERT achieves its remarkable performance.</strong> If so many of the important self-attention heads are not even potentially interpretable, should we give up on the idea that some knowledge is encoded in specific architecture components, rather than spread across the whole network? How can we study such distributed representations?‚Äù</p>

<p><a href="https://thegradient.pub/when-bert-plays-the-lottery-all-tickets-are-winning"><em>Source</em></a></p>

<h3 id="see-also">See Also</h3>

<ul>
  <li><a href="/wiki/deep-learning-NLP#transformers">Deep Learning for NLP: Transformers</a>.</li>
  <li><a href="/wiki/unsupervised-learning-berkeley#semi-supervised-learning">Semi-supervised learning</a>.</li>
  <li><a href="/wiki/distilling-knowledge-nn">Distilling the Knowledge in a Neural Network</a>.</li>
</ul>

</div>
<a href='https://ko-fi.com/R6R3F4NIO' target='_blank' rel="noopener noreferrer nofollow">
  <img style='border:0px;height:4em;width:auto;' src='https://cdn.ko-fi.com/cdn/kofi5.png?v=3' border='0' alt='Buy Me a Coffee at ko-fi.com' loading='lazy'/></a>
  <p style='text-align: center;'>
  <a href="https://twitter.com/intent/tweet?text=BERT Interpretability&url=https://strikingloo.github.io/wiki/bert-interpretability%2F%3Futm_source%3Dtwitter%26utm_medium%3Dsocial&via=strikingLoo" title="Share on Twitter!">[<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 310 310" height="1em" width="1em" rel="noopener noreferrer nofollow"><path fill="#1DA1F2" d="M302.973 57.388a117.512 117.512 0 01-14.993 5.463 66.276 66.276 0 0013.494-23.73 5 5 0 00-7.313-5.824 117.994 117.994 0 01-34.878 13.783c-12.381-12.098-29.197-18.983-46.581-18.983-36.695 0-66.549 29.853-66.549 66.547 0 2.89.183 5.764.545 8.598C101.163 99.244 58.83 76.863 29.76 41.204a5.001 5.001 0 00-8.196.642c-5.896 10.117-9.013 21.688-9.013 33.461 0 16.035 5.725 31.249 15.838 43.137a56.37 56.37 0 01-8.907-3.977 5 5 0 00-7.427 4.257c-.007.295-.007.59-.007.889 0 23.935 12.882 45.484 32.577 57.229a57.372 57.372 0 01-5.063-.735 4.998 4.998 0 00-5.699 6.437c7.29 22.76 26.059 39.501 48.749 44.605-18.819 11.787-40.34 17.961-62.932 17.961a120.4 120.4 0 01-14.095-.826 5 5 0 00-3.286 9.174c29.023 18.609 62.582 28.445 97.047 28.445 67.754 0 110.139-31.95 133.764-58.753 29.46-33.421 46.356-77.658 46.356-121.367 0-1.826-.028-3.67-.084-5.508 11.623-8.757 21.63-19.355 29.773-31.536a5 5 0 00-6.182-7.351z"></path></svg>Share on twitter]</a></p>

<template id="post-delayed-content">

<div class="backButton">
<a href="https://twitter.com/intent/tweet?text=BERT Interpretability&url=https://strikingloo.github.io/wiki/bert-interpretability%2F%3Futm_source%3Dtwitter%26utm_medium%3Dsocial&via=strikingLoo" id='tweetThis' title="Share on Twitter!" rel="noopener noreferrer nofollow">[<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 310 310" height="1em" width="1em"><path fill="#1DA1F2" d="M302.973 57.388a117.512 117.512 0 01-14.993 5.463 66.276 66.276 0 0013.494-23.73 5 5 0 00-7.313-5.824 117.994 117.994 0 01-34.878 13.783c-12.381-12.098-29.197-18.983-46.581-18.983-36.695 0-66.549 29.853-66.549 66.547 0 2.89.183 5.764.545 8.598C101.163 99.244 58.83 76.863 29.76 41.204a5.001 5.001 0 00-8.196.642c-5.896 10.117-9.013 21.688-9.013 33.461 0 16.035 5.725 31.249 15.838 43.137a56.37 56.37 0 01-8.907-3.977 5 5 0 00-7.427 4.257c-.007.295-.007.59-.007.889 0 23.935 12.882 45.484 32.577 57.229a57.372 57.372 0 01-5.063-.735 4.998 4.998 0 00-5.699 6.437c7.29 22.76 26.059 39.501 48.749 44.605-18.819 11.787-40.34 17.961-62.932 17.961a120.4 120.4 0 01-14.095-.826 5 5 0 00-3.286 9.174c29.023 18.609 62.582 28.445 97.047 28.445 67.754 0 110.139-31.95 133.764-58.753 29.46-33.421 46.356-77.658 46.356-121.367 0-1.826-.028-3.67-.084-5.508 11.623-8.757 21.63-19.355 29.773-31.536a5 5 0 00-6.182-7.351z"></path></svg>]</a>
<br/>
<a href="/blog/" id='backToBlog' title="Back to blog" rel="noopener noreferrer">[‚Üê]</a>
</div>
</template>
<script>

const headings = document.querySelectorAll('h2[id],h3[id]');
for (var heading of headings) {
    heading.innerHTML = `<a href=#${heading.id}>${heading.innerHTML}</a>`;
}
function externalLinks() { for(var c = document.getElementsByTagName("a"), a = 0;a < c.length;a++) { var b = c[a]; b.getAttribute("href") && b.hostname !== location.hostname && (b.target = "_blank") } }
externalLinks();

function renderBottomButtons(){
  const templateNode = document.getElementById('post-delayed-content')
  const templateContentClone = templateNode.content.cloneNode(true) // perform a deep copy
  document.body.appendChild(templateContentClone)
}

function scrollEventHandler(){
 const scrollOffset = window.pageYOffset
 const browserViewHeight = window.innerHeight
 if (scrollOffset > browserViewHeight/3) {
    console.log('done')
    renderBottomButtons();
    window.removeEventListener('scroll', scrollEventHandler)
 }
}
window.addEventListener('scroll', scrollEventHandler)
</script>

<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:site" content="@StrikingLoo" />
<meta name="twitter:title" content="BERT Interpretability" />
<meta name="twitter:description" content="A paper on BERT interpretability from thegradient.pub, and my reflections on the lottery hypothesis and model distilling." />

<meta name="twitter:image:src" content="https://strikingloo.github.io/resources/book-tw.jpg"/>
<meta property="og:image" content="https://strikingloo.github.io/resources/preview-image-terrarium.png"/>

			
			</div>
			<footer>
	    		<ul>
	        		<li><a href="mailto:lucianostrika44@gmail.com" rel="me" title="email me">‚úâÔ∏è</a></li>
	        		<li><a href="https://github.com/strikingloo" rel="me noopener noreferrer nofollow" title="GitHub"><svg viewBox="0 0 438.549 438.549" xmlns="http://www.w3.org/2000/svg" height="1em" width="1em"><path fill="#0F3D3E" d="M409.132 114.573c-19.608-33.596-46.205-60.194-79.798-79.8-33.598-19.607-70.277-29.408-110.063-29.408-39.781 0-76.472 9.804-110.063 29.408-33.596 19.605-60.192 46.204-79.8 79.8C9.803 148.168 0 184.854 0 224.63c0 47.78 13.94 90.745 41.827 128.906 27.884 38.164 63.906 64.572 108.063 79.227 5.14.954 8.945.283 11.419-1.996 2.475-2.282 3.711-5.14 3.711-8.562 0-.571-.049-5.708-.144-15.417a2549.81 2549.81 0 01-.144-25.406l-6.567 1.136c-4.187.767-9.469 1.092-15.846 1-6.374-.089-12.991-.757-19.842-1.999-6.854-1.231-13.229-4.086-19.13-8.559-5.898-4.473-10.085-10.328-12.56-17.556l-2.855-6.57c-1.903-4.374-4.899-9.233-8.992-14.559-4.093-5.331-8.232-8.945-12.419-10.848l-1.999-1.431c-1.332-.951-2.568-2.098-3.711-3.429-1.142-1.331-1.997-2.663-2.568-3.997-.572-1.335-.098-2.43 1.427-3.289s4.281-1.276 8.28-1.276l5.708.853c3.807.763 8.516 3.042 14.133 6.851 5.614 3.806 10.229 8.754 13.846 14.842 4.38 7.806 9.657 13.754 15.846 17.847 6.184 4.093 12.419 6.136 18.699 6.136s11.704-.476 16.274-1.423c4.565-.952 8.848-2.383 12.847-4.285 1.713-12.758 6.377-22.559 13.988-29.41-10.848-1.14-20.601-2.857-29.264-5.14-8.658-2.286-17.605-5.996-26.835-11.14-9.235-5.137-16.896-11.516-22.985-19.126-6.09-7.614-11.088-17.61-14.987-29.979-3.901-12.374-5.852-26.648-5.852-42.826 0-23.035 7.52-42.637 22.557-58.817-7.044-17.318-6.379-36.732 1.997-58.24 5.52-1.715 13.706-.428 24.554 3.853 10.85 4.283 18.794 7.952 23.84 10.994 5.046 3.041 9.089 5.618 12.135 7.708 17.705-4.947 35.976-7.421 54.818-7.421s37.117 2.474 54.823 7.421l10.849-6.849c7.419-4.57 16.18-8.758 26.262-12.565 10.088-3.805 17.802-4.853 23.134-3.138 8.562 21.509 9.325 40.922 2.279 58.24 15.036 16.18 22.559 35.787 22.559 58.817 0 16.178-1.958 30.497-5.853 42.966-3.9 12.471-8.941 22.457-15.125 29.979-6.191 7.521-13.901 13.85-23.131 18.986-9.232 5.14-18.182 8.85-26.84 11.136-8.662 2.286-18.415 4.004-29.263 5.146 9.894 8.562 14.842 22.077 14.842 40.539v60.237c0 3.422 1.19 6.279 3.572 8.562 2.379 2.279 6.136 2.95 11.276 1.995 44.163-14.653 80.185-41.062 108.068-79.226 27.88-38.161 41.825-81.126 41.825-128.906-.01-39.771-9.818-76.454-29.414-110.049z"></path></svg></a></li>
			        <li><a href="https://twitter.com/intent/follow?screen_name=strikingloo" rel="me noopener noreferrer nofollow" title="twitter"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 310 310" height="1em" width="1em"><path fill="#0F3D3E" d="M302.973 57.388a117.512 117.512 0 01-14.993 5.463 66.276 66.276 0 0013.494-23.73 5 5 0 00-7.313-5.824 117.994 117.994 0 01-34.878 13.783c-12.381-12.098-29.197-18.983-46.581-18.983-36.695 0-66.549 29.853-66.549 66.547 0 2.89.183 5.764.545 8.598C101.163 99.244 58.83 76.863 29.76 41.204a5.001 5.001 0 00-8.196.642c-5.896 10.117-9.013 21.688-9.013 33.461 0 16.035 5.725 31.249 15.838 43.137a56.37 56.37 0 01-8.907-3.977 5 5 0 00-7.427 4.257c-.007.295-.007.59-.007.889 0 23.935 12.882 45.484 32.577 57.229a57.372 57.372 0 01-5.063-.735 4.998 4.998 0 00-5.699 6.437c7.29 22.76 26.059 39.501 48.749 44.605-18.819 11.787-40.34 17.961-62.932 17.961a120.4 120.4 0 01-14.095-.826 5 5 0 00-3.286 9.174c29.023 18.609 62.582 28.445 97.047 28.445 67.754 0 110.139-31.95 133.764-58.753 29.46-33.421 46.356-77.658 46.356-121.367 0-1.826-.028-3.67-.084-5.508 11.623-8.757 21.63-19.355 29.773-31.536a5 5 0 00-6.182-7.351z"></path></svg></a></li>
			        <li><a href="http://www.linkedin.com/in/luciano-strika" rel="me noopener noreferrer nofollow" title="linkedin"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 310 310" height="1em" width="1em"><path fill="#0F3D3E" d="M72.16 99.73H9.927a5 5 0 00-5 5v199.928a5 5 0 005 5H72.16a5 5 0 005-5V104.73a5 5 0 00-5-5zM41.066.341C18.422.341 0 18.743 0 41.362 0 63.991 18.422 82.4 41.066 82.4c22.626 0 41.033-18.41 41.033-41.038C82.1 18.743 63.692.341 41.066.341zM230.454 94.761c-24.995 0-43.472 10.745-54.679 22.954V104.73a5 5 0 00-5-5h-59.599a5 5 0 00-5 5v199.928a5 5 0 005 5h62.097a5 5 0 005-5V205.74c0-33.333 9.054-46.319 32.29-46.319 25.306 0 27.317 20.818 27.317 48.034v97.204a5 5 0 005 5H305a5 5 0 005-5V194.995c0-49.565-9.451-100.234-79.546-100.234z"></path></svg></a></li>
			        <li><a href="/resources/Luciano_Strika.pdf">CV</a></li>
				</ul>
				<p><i>Built with ‚ù§Ô∏è by <a href="https://strikingloo.github.io/">Strikingloo</a>.</i></p>
			</footer>
			

			

			
			<link rel="preload" href="/css/non-critical-post-min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
			<noscript><link rel="stylesheet" href="/css/non-critical-post-min.css"></noscript>
        	
		</body>
	</html>
