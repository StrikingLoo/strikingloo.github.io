<!DOCTYPE html>
	<html lang="en">
		<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0">
		<link rel="icon" type="image/svg+xml" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 width=%22256%22 height=%22256%22 viewBox=%220 0 100 100%22><rect width=%22100%22 height=%22100%22 rx=%2220%22 fill=%22%23d8eaeb%22></rect><text x=%2250%%22 y=%2250%%22 dominant-baseline=%22central%22 text-anchor=%22middle%22 font-size=%2293%22>üå≥</text></svg>" />
            <title>CLIP: Learning Transferable Visual Models From Natural Language Supervision</title>
			<link rel="canonical" href="https://strikingloo.github.io/wiki/clip">
			  <meta name="description" content="Notes on the CLIP paper, a very clever contrastive training regime that maps both textual and image inputs into a shared latent universe, to then apply seman...">
  			<meta property="og:site_name" content="CLIP: Learning Transferable Visual Models From Natural Language Supervision">

        	
        	<link rel="stylesheet" type="text/css" href="/css/post-min.css">
        	

			<script async src="https://www.googletagmanager.com/gtag/js?id=UA-52642322-5"></script>

			<script>
			  window.dataLayer = window.dataLayer || [];
			  function gtag(){dataLayer.push(arguments);}
			  gtag('js', new Date());

			  gtag('config', 'UA-52642322-5');
			</script>
			
			  <meta property="og:description" content="Notes on the CLIP paper, a very clever contrastive training regime that maps both textual and image inputs into a shared latent universe, to then apply seman...">
			
	  		<meta property="og:locale" content="en_US">
	  		
			  <meta property="og:title" content="CLIP: Learning Transferable Visual Models From Natural Language Supervision">
			  <meta property="og:type" content="article">
		  	  <meta property="article:published_time" content="2021-10-25T00:00:00-03:00">
		      <meta property="article:author" content="http://localhost:4000/">
		  	
		  	
  				<meta property="og:url" content="https://strikingloo.github.io/wiki/clip">
  			
  			<meta content="index,follow" name="robots"><!-- All Search Engines -->
  			<meta content="index,follow" name="googlebot"><!-- Google Specific -->
  			<meta name="robots" content="max-image-preview:large">

		</head>
		<body>
			<div class="head-banner">
			<p>Strikingloo</p>
			<nav>
	    		<ul>
	        		<li><a href="/">Home</a></li>
		        	<li><a href="/about/">About</a></li>
	        		<li><a href="/wiki/">Wiki</a></li>
	        		<li><a href="/blog/">Blog </a></li>
	    		</ul>
			</nav>
			<div style="clear: both;"></div>
		    </div>

			<div class="container">
			
			<h1>CLIP: Learning Transferable Visual Models From Natural Language Supervision</h1>
<p class="meta">25 Oct 2021 - importance: 8 </p>




<div class="post">
  <p>These are my notes and explanation on the <a href="https://arxiv.org/abs/2103.00020">CLIP paper</a>, a model that uses a very clever contrastive training regime that <strong>maps both textual and image inputs into a shared latent universe</strong>, to then apply semantically meaningful distance notions and generate images or text.</p>

<p>The basic idea is: we have a model that maps text prompts to a latent space (like a sentence embedding. In this case, a Transformer). We also have a model that maps images into a latent space (here a ResNet or a Transformer) of the same dimension <em>d</em>.</p>

<p>Finally, what we do is take a (massive, crawled from the internet) dataset of images and their captions, and train both models to increase the cosine similarity between a caption and its corresponding image. After backpropagating for a long while, we obtain a new embedding space where a text‚Äôs representation and its corresponding image‚Äôs are ‚Äòclose‚Äô in terms of cosine similarity (angle), and far from other arbitrary captions and pictures.</p>

<p>This is achieved through a similar loss as is used for word embeddings. It is <strong>contrastive</strong>: we take a pair <em>(image, caption)</em> and also make a spurious pair with the image and a random caption, and one with the caption and a random image. Then we use a loss that penalizes the true pair for diverging from a similarity of 1, and the spurious pairs for having a similarity bigger than 0.</p>

<p>Here ends my explanation and begin the quotes and figures from the paper.</p>

<h2 id="paper-quotes-and-summary">Paper quotes and summary</h2>

<p>State-of-the-art computer vision systems are
trained to predict a fixed set of predetermined
object categories. This restricted form of supervision limits their generality and usability since
additional labeled data is needed to specify any
other visual concept. Learning directly from raw
text about images is a promising alternative which
leverages a much broader source of supervision.</p>

<p>We demonstrate that the simple pre-training task
of predicting which caption goes with which image is an efficient and scalable way to learn SOTA
image representations from scratch on a dataset
of 400 million (image, text) pairs collected from
the internet.</p>

<p>we match the accuracy of the original ResNet-50 on ImageNet
zero-shot without needing to use any of the 1.28
million training examples it was trained on.</p>

<ul>
  <li>Model available on <a href="https://github.com/OpenAI/CLIP">GitHub</a></li>
  <li>A nice client I‚Äôve been using: <a href="https://github.com/nerdyrodent/VQGAN-CLIP">here</a>.</li>
</ul>

<p>The line of work of semi-supervised learning using images with a hashtag related to a imagenet label, or pretraining on a very big dataset noisily labelled (JFT-300M) represents the current pragmatic middle ground between learning from a limited amount of supervised ‚Äúgold-labels‚Äù and learning from practically unlimited amounts of raw text.</p>

<p>Both works limit their supervision to 1000 and 18291 classes respectively.
Natural language is able to express, and therefore supervise, a much wider set of visual concepts through its generality. Both approaches also use static softmax classifiers to
perform prediction and lack a mechanism for dynamic outputs. This severely curtails their flexibility and limits their ‚Äúzero-shot‚Äù capabilities.</p>

<p>We create a new dataset of 400 million (image, text) pairs and demonstrate that a simplified version of
ConVIRT trained from scratch, which we call CLIP, for Contrastive Language-Image Pre-training, is an efficient method
of learning from natural language supervision.</p>

<p>CLIP grows capable of competitive zero-shot transfer performance in a battery of benchmarks.</p>

<ul>
  <li>We also confirm these findings with linear-probe representation learning analysis and show that CLIP outperforms the best publicly available ImageNet model while also being more computationally efficient.</li>
  <li>We additionally find that zero-shot CLIP models are much more robust than equivalent accuracy supervised ImageNet models which suggests that zero-shot evaluation of task-agnostic models is much more representative of a model‚Äôs capability.</li>
</ul>

<p>We constructed a new dataset of <strong>400 million (image,
text) pairs collected form a variety of publicly available
sources on the Internet.</strong> To attempt to cover as broad a set
of visual concepts as possible, we search for (image, text)
pairs as part of the construction process whose text includes
one of a set of 500,000 queries (words occurring at least 100 times in
the English version of Wikipedia) and take up to 20k images per query. The resulting dataset has a similar total
word count as the WebText dataset used to train GPT-2.</p>

<blockquote>
  <p>Recent work in contrastive representation learning for images has found that contrastive objectives can learn better representations than their equivalent predictive objective (predicting the embedding instead of the discrete word is better).</p>
</blockquote>

<p>Noting these findings, we explored training
a system to solve the potentially easier proxy task of predicting only which text as a whole is paired with which
image and not the exact words of that text. Starting with
the same bag-of-words encoding baseline, we swapped the
predictive objective for a contrastive objective in Figure 2
and observed a further 4x efficiency improvement in the rate
of zero-shot transfer to ImageNet.</p>

<p>Given a batch of N (image, text) pairs, CLIP is trained to
predict which of the N √ó N possible (image, text) pairings
across a batch actually occurred. To do this, CLIP learns a
multi-modal embedding space by jointly training an image
encoder and text encoder to maximize the cosine similarity of the image and text embeddings of the N real pairs
in the batch while minimizing the cosine similarity of the
embeddings of the N2 ‚àí N incorrect pairings.</p>

<p><strong>We optimize a symmetric cross entropy loss over these similarity
scores.</strong></p>

<h3 id="architecture">Architecture</h3>

<p>Two versions for image encoder:</p>
<ul>
  <li><strong>ResNet-50</strong><a href="https://arxiv.org/pdf/1512.03385.pdf">üå±</a> with a few modifications.</li>
  <li><strong>ViT</strong>: closely following <a href="/wiki/visual-transformer">previous implementation</a>.</li>
</ul>

<p>The text encoder is a Transformer with input capped at 76 characters. The text sequence is bracketed with [SOS] and [EOS] tokens and the activations of the highest layer of the transformer at the [EOS] token are treated as the feature representation of the text which is layer normalized and then linearly projected into the multi-modal embedding space.</p>

<blockquote>
  <p>‚ÄúTo save additional memory, gradient checkpointing (Griewank &amp; Walther, 2000;
Chen et al., 2016), half-precision Adam statistics (Dhariwal
et al., 2020), and half-precision stochastically rounded text
encoder weights were used.‚Äù (gotta look all of these up).</p>
</blockquote>

<p>Every step of CLIP pre-training can
be viewed as optimizing the performance of a randomly
created proxy to a computer vision dataset which contains 1
example per class and has 32,768 total classes defined via
natural language descriptions. For zero-shot evaluation, we
cache the zero-shot classifier once it has been computed by
the text encoder and reuse it for all subsequent predictions.</p>

<p>Another issue we encountered is that it‚Äôs relatively rare in
our pre-training dataset for the text paired with the image
to be just a single word. Usually the text is a full sentence
describing the image in some way. To help bridge this
distribution gap, we found that using the prompt template
<strong>‚ÄúA photo of a {label}.‚Äù</strong> to be a good default that
helps specify the text is about the content of the image.
This
often improves performance over the baseline of using only
the label text. For instance, just using this prompt improves
accuracy on ImageNet by 1.3%.</p>

<p>We‚Äôve observed ensembling -by averaging embedding representations over many different textual prompts and caching them, which quickly gets amortized- across many generated zero-shot classifiers to reliably improve performance.</p>

<p>For fine-grained classification, it works to add, ‚Äúa photo of a X, a type of Y‚Äù.</p>

<p>Using a linear probe, CLIP beats other models in a few-shot context (up to 16 instances), and interestingly its 0-shot approach beats few shots up to 4.
Zero-shot CLIP performs competitively against fully supervised Linear Probe on ResNet50 on a wide array of tasks (wins in 16/27 datasets).</p>

<p>However, models trained with CLIP scale very well
and the largest model we trained (ResNet-50x64) slightly
outperforms the best performing existing model (a Noisy
Student EfficientNet-L2) on both overall score and compute
efficiency -on linear probe accuracy over 27 datasets, a standard benchmark-. We also find that CLIP vision transformers are
about 3x more compute efficient than CLIP ResNets, which
allows us to reach higher overall performance within our
compute budget
and use it for the majority of datasets.</p>

<p>Note that by this point, a CLIP ViT has a transformer for textual representation, a transformer for image representation, and just does dot product between them, so it‚Äôs transformers all the way down. Linear probe CLIP ViT beats other models in 21/27 tasks.</p>

<h3 id="related-reading">Related Reading</h3>

<p>For more reading see:</p>
<ul>
  <li><a href="/wiki/deep-learning-NLP#transformers">Core NLP article</a> for an explanation on transformers.</li>
  <li><a href="/wiki/glide">GLIDE</a> for an application of CLIP in text-to-image generation (was state of the art until DALL-E 2 arrived). GLIDE uses CLIP for guided diffusion downstream.</li>
  <li><a href="/wiki/transformers-see-like-cnn">Do Transformers See like Convolutional Neural Networks?</a> for a more in-depth comparison between ViT and ResNet (and other CNNs) and their ways of modeling data.</li>
  <li>See <a href="https://github.com/mlfoundations/open_clip">Open CLIP</a> for an open-source CLIP model is available to play with.</li>
</ul>


</div>
<a href='https://ko-fi.com/R6R3F4NIO' target='_blank' rel="noopener noreferrer nofollow">
  <img style='border:0px;height:4em;width:auto;' src='https://cdn.ko-fi.com/cdn/kofi5.png?v=3' border='0' alt='Buy Me a Coffee at ko-fi.com' loading='lazy'/></a>
  <p style='text-align: center;'>
  <a href="https://twitter.com/intent/tweet?text=CLIP: Learning Transferable Visual Models From Natural Language Supervision&url=https://strikingloo.github.io/wiki/clip%2F%3Futm_source%3Dtwitter%26utm_medium%3Dsocial&via=strikingLoo" title="Share on Twitter!">[<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 310 310" height="1em" width="1em" rel="noopener noreferrer nofollow"><path fill="#1DA1F2" d="M302.973 57.388a117.512 117.512 0 01-14.993 5.463 66.276 66.276 0 0013.494-23.73 5 5 0 00-7.313-5.824 117.994 117.994 0 01-34.878 13.783c-12.381-12.098-29.197-18.983-46.581-18.983-36.695 0-66.549 29.853-66.549 66.547 0 2.89.183 5.764.545 8.598C101.163 99.244 58.83 76.863 29.76 41.204a5.001 5.001 0 00-8.196.642c-5.896 10.117-9.013 21.688-9.013 33.461 0 16.035 5.725 31.249 15.838 43.137a56.37 56.37 0 01-8.907-3.977 5 5 0 00-7.427 4.257c-.007.295-.007.59-.007.889 0 23.935 12.882 45.484 32.577 57.229a57.372 57.372 0 01-5.063-.735 4.998 4.998 0 00-5.699 6.437c7.29 22.76 26.059 39.501 48.749 44.605-18.819 11.787-40.34 17.961-62.932 17.961a120.4 120.4 0 01-14.095-.826 5 5 0 00-3.286 9.174c29.023 18.609 62.582 28.445 97.047 28.445 67.754 0 110.139-31.95 133.764-58.753 29.46-33.421 46.356-77.658 46.356-121.367 0-1.826-.028-3.67-.084-5.508 11.623-8.757 21.63-19.355 29.773-31.536a5 5 0 00-6.182-7.351z"></path></svg>Share on twitter]</a></p>

<template id="post-delayed-content">

<div class="backButton">
<a href="https://twitter.com/intent/tweet?text=CLIP: Learning Transferable Visual Models From Natural Language Supervision&url=https://strikingloo.github.io/wiki/clip%2F%3Futm_source%3Dtwitter%26utm_medium%3Dsocial&via=strikingLoo" id='tweetThis' title="Share on Twitter!" rel="noopener noreferrer nofollow">[<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 310 310" height="1em" width="1em"><path fill="#1DA1F2" d="M302.973 57.388a117.512 117.512 0 01-14.993 5.463 66.276 66.276 0 0013.494-23.73 5 5 0 00-7.313-5.824 117.994 117.994 0 01-34.878 13.783c-12.381-12.098-29.197-18.983-46.581-18.983-36.695 0-66.549 29.853-66.549 66.547 0 2.89.183 5.764.545 8.598C101.163 99.244 58.83 76.863 29.76 41.204a5.001 5.001 0 00-8.196.642c-5.896 10.117-9.013 21.688-9.013 33.461 0 16.035 5.725 31.249 15.838 43.137a56.37 56.37 0 01-8.907-3.977 5 5 0 00-7.427 4.257c-.007.295-.007.59-.007.889 0 23.935 12.882 45.484 32.577 57.229a57.372 57.372 0 01-5.063-.735 4.998 4.998 0 00-5.699 6.437c7.29 22.76 26.059 39.501 48.749 44.605-18.819 11.787-40.34 17.961-62.932 17.961a120.4 120.4 0 01-14.095-.826 5 5 0 00-3.286 9.174c29.023 18.609 62.582 28.445 97.047 28.445 67.754 0 110.139-31.95 133.764-58.753 29.46-33.421 46.356-77.658 46.356-121.367 0-1.826-.028-3.67-.084-5.508 11.623-8.757 21.63-19.355 29.773-31.536a5 5 0 00-6.182-7.351z"></path></svg>]</a>
<br/>
<a href="/blog/" id='backToBlog' title="Back to blog" rel="noopener noreferrer">[‚Üê]</a>
</div>
</template>
<script>
/*
const headings = document.querySelectorAll('h2[id],h3[id]');
for (var heading of headings) {
    heading.innerHTML = `<a href=#${heading.id}>${heading.innerHTML}</a>`;
}*/
function externalLinks() { for(var c = document.getElementsByTagName("a"), a = 0;a < c.length;a++) { var b = c[a]; b.getAttribute("href") && b.hostname !== location.hostname && (b.target = "_blank") } }
externalLinks();

function renderBottomButtons(){
  const templateNode = document.getElementById('post-delayed-content')
  const templateContentClone = templateNode.content.cloneNode(true) // perform a deep copy
  document.body.appendChild(templateContentClone)
}

function scrollEventHandler(){
 const scrollOffset = window.pageYOffset
 const browserViewHeight = window.innerHeight
 if (scrollOffset > browserViewHeight/3) {
    console.log('done')
    renderBottomButtons();
    window.removeEventListener('scroll', scrollEventHandler)
 }
}
window.addEventListener('scroll', scrollEventHandler)
</script>

<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:site" content="@StrikingLoo" />
<meta name="twitter:title" content="CLIP: Learning Transferable Visual Models From Natural Language Supervision" />
<meta name="twitter:description" content="Notes on the CLIP paper, a very clever contrastive training regime that maps both textual and image inputs into a shared latent universe, to then apply semantically meaningful distance notions and generate images or text." />

<meta name="twitter:image:src" content="https://strikingloo.github.io/resources/book-tw.jpg"/>
<meta property="og:image" content="https://strikingloo.github.io/resources/preview-image-terrarium.png"/>

			
			</div>
			<footer>
	    		<ul>
	        		<li><a href="mailto:lucianostrika44@gmail.com" rel="me" title="email me">‚úâÔ∏è</a></li>
	        		<li><a href="https://github.com/strikingloo" rel="me noopener noreferrer nofollow" title="GitHub"><svg viewBox="0 0 438.549 438.549" xmlns="http://www.w3.org/2000/svg" height="1em" width="1em"><path fill="#0F3D3E" d="M409.132 114.573c-19.608-33.596-46.205-60.194-79.798-79.8-33.598-19.607-70.277-29.408-110.063-29.408-39.781 0-76.472 9.804-110.063 29.408-33.596 19.605-60.192 46.204-79.8 79.8C9.803 148.168 0 184.854 0 224.63c0 47.78 13.94 90.745 41.827 128.906 27.884 38.164 63.906 64.572 108.063 79.227 5.14.954 8.945.283 11.419-1.996 2.475-2.282 3.711-5.14 3.711-8.562 0-.571-.049-5.708-.144-15.417a2549.81 2549.81 0 01-.144-25.406l-6.567 1.136c-4.187.767-9.469 1.092-15.846 1-6.374-.089-12.991-.757-19.842-1.999-6.854-1.231-13.229-4.086-19.13-8.559-5.898-4.473-10.085-10.328-12.56-17.556l-2.855-6.57c-1.903-4.374-4.899-9.233-8.992-14.559-4.093-5.331-8.232-8.945-12.419-10.848l-1.999-1.431c-1.332-.951-2.568-2.098-3.711-3.429-1.142-1.331-1.997-2.663-2.568-3.997-.572-1.335-.098-2.43 1.427-3.289s4.281-1.276 8.28-1.276l5.708.853c3.807.763 8.516 3.042 14.133 6.851 5.614 3.806 10.229 8.754 13.846 14.842 4.38 7.806 9.657 13.754 15.846 17.847 6.184 4.093 12.419 6.136 18.699 6.136s11.704-.476 16.274-1.423c4.565-.952 8.848-2.383 12.847-4.285 1.713-12.758 6.377-22.559 13.988-29.41-10.848-1.14-20.601-2.857-29.264-5.14-8.658-2.286-17.605-5.996-26.835-11.14-9.235-5.137-16.896-11.516-22.985-19.126-6.09-7.614-11.088-17.61-14.987-29.979-3.901-12.374-5.852-26.648-5.852-42.826 0-23.035 7.52-42.637 22.557-58.817-7.044-17.318-6.379-36.732 1.997-58.24 5.52-1.715 13.706-.428 24.554 3.853 10.85 4.283 18.794 7.952 23.84 10.994 5.046 3.041 9.089 5.618 12.135 7.708 17.705-4.947 35.976-7.421 54.818-7.421s37.117 2.474 54.823 7.421l10.849-6.849c7.419-4.57 16.18-8.758 26.262-12.565 10.088-3.805 17.802-4.853 23.134-3.138 8.562 21.509 9.325 40.922 2.279 58.24 15.036 16.18 22.559 35.787 22.559 58.817 0 16.178-1.958 30.497-5.853 42.966-3.9 12.471-8.941 22.457-15.125 29.979-6.191 7.521-13.901 13.85-23.131 18.986-9.232 5.14-18.182 8.85-26.84 11.136-8.662 2.286-18.415 4.004-29.263 5.146 9.894 8.562 14.842 22.077 14.842 40.539v60.237c0 3.422 1.19 6.279 3.572 8.562 2.379 2.279 6.136 2.95 11.276 1.995 44.163-14.653 80.185-41.062 108.068-79.226 27.88-38.161 41.825-81.126 41.825-128.906-.01-39.771-9.818-76.454-29.414-110.049z"></path></svg></a></li>
			        <li><a href="https://twitter.com/intent/follow?screen_name=strikingloo" rel="me noopener noreferrer nofollow" title="twitter"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 310 310" height="1em" width="1em"><path fill="#0F3D3E" d="M302.973 57.388a117.512 117.512 0 01-14.993 5.463 66.276 66.276 0 0013.494-23.73 5 5 0 00-7.313-5.824 117.994 117.994 0 01-34.878 13.783c-12.381-12.098-29.197-18.983-46.581-18.983-36.695 0-66.549 29.853-66.549 66.547 0 2.89.183 5.764.545 8.598C101.163 99.244 58.83 76.863 29.76 41.204a5.001 5.001 0 00-8.196.642c-5.896 10.117-9.013 21.688-9.013 33.461 0 16.035 5.725 31.249 15.838 43.137a56.37 56.37 0 01-8.907-3.977 5 5 0 00-7.427 4.257c-.007.295-.007.59-.007.889 0 23.935 12.882 45.484 32.577 57.229a57.372 57.372 0 01-5.063-.735 4.998 4.998 0 00-5.699 6.437c7.29 22.76 26.059 39.501 48.749 44.605-18.819 11.787-40.34 17.961-62.932 17.961a120.4 120.4 0 01-14.095-.826 5 5 0 00-3.286 9.174c29.023 18.609 62.582 28.445 97.047 28.445 67.754 0 110.139-31.95 133.764-58.753 29.46-33.421 46.356-77.658 46.356-121.367 0-1.826-.028-3.67-.084-5.508 11.623-8.757 21.63-19.355 29.773-31.536a5 5 0 00-6.182-7.351z"></path></svg></a></li>
			        <li><a href="http://www.linkedin.com/in/luciano-strika" rel="me noopener noreferrer nofollow" title="linkedin"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 310 310" height="1em" width="1em"><path fill="#0F3D3E" d="M72.16 99.73H9.927a5 5 0 00-5 5v199.928a5 5 0 005 5H72.16a5 5 0 005-5V104.73a5 5 0 00-5-5zM41.066.341C18.422.341 0 18.743 0 41.362 0 63.991 18.422 82.4 41.066 82.4c22.626 0 41.033-18.41 41.033-41.038C82.1 18.743 63.692.341 41.066.341zM230.454 94.761c-24.995 0-43.472 10.745-54.679 22.954V104.73a5 5 0 00-5-5h-59.599a5 5 0 00-5 5v199.928a5 5 0 005 5h62.097a5 5 0 005-5V205.74c0-33.333 9.054-46.319 32.29-46.319 25.306 0 27.317 20.818 27.317 48.034v97.204a5 5 0 005 5H305a5 5 0 005-5V194.995c0-49.565-9.451-100.234-79.546-100.234z"></path></svg></a></li>
			        <li><a href="/resources/Luciano_Strika.pdf">CV</a></li>
				</ul>
				<p><i>Built with ‚ù§Ô∏è by <a href="https://strikingloo.github.io/">Strikingloo</a>.</i></p>
			</footer>
			

			

			
			<link rel="preload" href="/css/non-critical-post-min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
			<noscript><link rel="stylesheet" href="/css/non-critical-post-min.css"></noscript>
        	
		</body>
	</html>
